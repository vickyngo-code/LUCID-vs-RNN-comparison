{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6su6pEczxEu",
        "outputId": "0f8cd147-2291-46a5-ff1a-59bc47533044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'lucid-ddos'...\n",
            "remote: Enumerating objects: 86, done.\u001b[K\n",
            "remote: Counting objects: 100% (86/86), done.\u001b[K\n",
            "remote: Compressing objects: 100% (63/63), done.\u001b[K\n",
            "remote: Total 86 (delta 51), reused 53 (delta 23), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (86/86), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/doriguzzi/lucid-ddos.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_Ujo4nxAxR0"
      },
      "outputs": [],
      "source": [
        "!pip install ipython-autotime\n",
        "!pip install pyshark\n",
        "!apt-get install tshark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CahMO_aeBJPI"
      },
      "outputs": [],
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "device_lib.list_local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gct6Vw3AReuq",
        "outputId": "a22f4c13-ab42-42a3-87ce-6e4657a913f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D3r98t-X_Di"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "import warnings\n",
        "warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53d9xj3q87lC",
        "outputId": "e91df33d-a180-486f-c749-25c7b21b190e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/lucid-ddos\n"
          ]
        }
      ],
      "source": [
        "cd /content/drive/MyDrive/\"Colab Notebooks\"/lucid-ddos/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MI-LHOrNQL5J",
        "outputId": "0cde3e91-8cd2-478f-bc17-10dfcf8f187b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file:  SAT-03-11-2018_067.pcap\n",
            "Processing file:  SAT-03-11-2018_010.pcap\n",
            "Processing file:  SAT-03-11-2018_0145.pcap\n",
            "Processing file:  SAT-03-11-2018_0_0.pcap\n",
            "SAT-03-11-2018_010.pcap packet # 0\n",
            "SAT-03-11-2018_067.pcap packet # 0\n",
            "SAT-03-11-2018_0145.pcap packet # 0\n",
            "SAT-03-11-2018_0_0.pcap packet # 0\n",
            "SAT-03-11-2018_010.pcap packet # 1000\n",
            "SAT-03-11-2018_067.pcap packet # 1000\n",
            "SAT-03-11-2018_0_0.pcap packet # 1000\n",
            "SAT-03-11-2018_0145.pcap packet # 1000\n",
            "SAT-03-11-2018_010.pcap packet # 2000\n",
            "SAT-03-11-2018_067.pcap packet # 2000\n",
            "SAT-03-11-2018_0_0.pcap packet # 2000\n",
            "SAT-03-11-2018_0145.pcap packet # 2000\n",
            "SAT-03-11-2018_010.pcap packet # 3000\n",
            "SAT-03-11-2018_067.pcap packet # 3000\n",
            "SAT-03-11-2018_010.pcap packet # 4000\n",
            "SAT-03-11-2018_0_0.pcap packet # 3000\n",
            "SAT-03-11-2018_0145.pcap packet # 3000\n",
            "SAT-03-11-2018_067.pcap packet # 4000\n",
            "SAT-03-11-2018_010.pcap packet # 5000\n",
            "SAT-03-11-2018_0_0.pcap packet # 4000\n",
            "SAT-03-11-2018_0145.pcap packet # 4000\n",
            "SAT-03-11-2018_067.pcap packet # 5000\n",
            "SAT-03-11-2018_010.pcap packet # 6000\n",
            "SAT-03-11-2018_0145.pcap packet # 5000\n",
            "SAT-03-11-2018_0_0.pcap packet # 5000\n",
            "SAT-03-11-2018_067.pcap packet # 6000\n",
            "SAT-03-11-2018_010.pcap packet # 7000\n",
            "SAT-03-11-2018_0145.pcap packet # 6000\n",
            "SAT-03-11-2018_010.pcap packet # 8000\n",
            "SAT-03-11-2018_067.pcap packet # 7000\n",
            "SAT-03-11-2018_0_0.pcap packet # 6000\n",
            "SAT-03-11-2018_010.pcap packet # 9000\n",
            "SAT-03-11-2018_067.pcap packet # 8000\n",
            "SAT-03-11-2018_0145.pcap packet # 7000\n",
            "SAT-03-11-2018_0_0.pcap packet # 7000\n",
            "SAT-03-11-2018_010.pcap packet # 10000\n",
            "SAT-03-11-2018_067.pcap packet # 9000\n",
            "SAT-03-11-2018_0_0.pcap packet # 8000\n",
            "SAT-03-11-2018_0145.pcap packet # 8000\n",
            "SAT-03-11-2018_010.pcap packet # 11000\n",
            "SAT-03-11-2018_067.pcap packet # 10000\n",
            "SAT-03-11-2018_0_0.pcap packet # 9000\n",
            "SAT-03-11-2018_010.pcap packet # 12000\n",
            "SAT-03-11-2018_0145.pcap packet # 9000\n",
            "SAT-03-11-2018_067.pcap packet # 11000\n",
            "SAT-03-11-2018_010.pcap packet # 13000\n",
            "SAT-03-11-2018_0_0.pcap packet # 10000\n",
            "SAT-03-11-2018_0145.pcap packet # 10000\n",
            "SAT-03-11-2018_067.pcap packet # 12000\n",
            "SAT-03-11-2018_010.pcap packet # 14000\n",
            "SAT-03-11-2018_0_0.pcap packet # 11000\n",
            "SAT-03-11-2018_010.pcap packet # 15000\n",
            "SAT-03-11-2018_067.pcap packet # 13000\n",
            "SAT-03-11-2018_0145.pcap packet # 11000\n",
            "SAT-03-11-2018_0_0.pcap packet # 12000\n",
            "SAT-03-11-2018_010.pcap packet # 16000\n",
            "SAT-03-11-2018_067.pcap packet # 14000\n",
            "SAT-03-11-2018_0145.pcap packet # 12000\n",
            "SAT-03-11-2018_010.pcap packet # 17000\n",
            "SAT-03-11-2018_0_0.pcap packet # 13000\n",
            "SAT-03-11-2018_067.pcap packet # 15000\n",
            "SAT-03-11-2018_010.pcap packet # 18000\n",
            "SAT-03-11-2018_0145.pcap packet # 13000\n",
            "SAT-03-11-2018_0_0.pcap packet # 14000\n",
            "SAT-03-11-2018_067.pcap packet # 16000\n",
            "SAT-03-11-2018_010.pcap packet # 19000\n",
            "SAT-03-11-2018_0145.pcap packet # 14000\n",
            "SAT-03-11-2018_0_0.pcap packet # 15000\n",
            "SAT-03-11-2018_067.pcap packet # 17000\n",
            "SAT-03-11-2018_010.pcap packet # 20000\n",
            "SAT-03-11-2018_010.pcap packet # 21000\n",
            "SAT-03-11-2018_0145.pcap packet # 15000\n",
            "SAT-03-11-2018_067.pcap packet # 18000\n",
            "SAT-03-11-2018_0_0.pcap packet # 16000\n",
            "SAT-03-11-2018_010.pcap packet # 22000\n",
            "SAT-03-11-2018_067.pcap packet # 19000\n",
            "SAT-03-11-2018_0145.pcap packet # 16000\n",
            "SAT-03-11-2018_0_0.pcap packet # 17000\n",
            "SAT-03-11-2018_010.pcap packet # 23000\n",
            "SAT-03-11-2018_067.pcap packet # 20000\n",
            "SAT-03-11-2018_0_0.pcap packet # 18000\n",
            "SAT-03-11-2018_0145.pcap packet # 17000\n",
            "SAT-03-11-2018_010.pcap packet # 24000\n",
            "SAT-03-11-2018_067.pcap packet # 21000\n",
            "SAT-03-11-2018_0145.pcap packet # 18000\n",
            "SAT-03-11-2018_0_0.pcap packet # 19000\n",
            "SAT-03-11-2018_010.pcap packet # 25000\n",
            "SAT-03-11-2018_067.pcap packet # 22000\n",
            "SAT-03-11-2018_010.pcap packet # 26000\n",
            "SAT-03-11-2018_0145.pcap packet # 19000\n",
            "SAT-03-11-2018_0_0.pcap packet # 20000\n",
            "SAT-03-11-2018_067.pcap packet # 23000\n",
            "SAT-03-11-2018_010.pcap packet # 27000\n",
            "SAT-03-11-2018_0_0.pcap packet # 21000\n",
            "SAT-03-11-2018_0145.pcap packet # 20000\n",
            "SAT-03-11-2018_010.pcap packet # 28000\n",
            "SAT-03-11-2018_067.pcap packet # 24000\n",
            "SAT-03-11-2018_0145.pcap packet # 21000\n",
            "SAT-03-11-2018_010.pcap packet # 29000\n",
            "SAT-03-11-2018_0_0.pcap packet # 22000\n",
            "SAT-03-11-2018_067.pcap packet # 25000\n",
            "SAT-03-11-2018_010.pcap packet # 30000\n",
            "SAT-03-11-2018_0_0.pcap packet # 23000\n",
            "SAT-03-11-2018_0145.pcap packet # 22000\n",
            "SAT-03-11-2018_067.pcap packet # 26000\n",
            "SAT-03-11-2018_010.pcap packet # 31000\n",
            "SAT-03-11-2018_0_0.pcap packet # 24000\n",
            "SAT-03-11-2018_0145.pcap packet # 23000\n",
            "SAT-03-11-2018_067.pcap packet # 27000\n",
            "SAT-03-11-2018_010.pcap packet # 32000\n",
            "SAT-03-11-2018_0145.pcap packet # 24000\n",
            "SAT-03-11-2018_067.pcap packet # 28000\n",
            "SAT-03-11-2018_0_0.pcap packet # 25000\n",
            "SAT-03-11-2018_010.pcap packet # 33000\n",
            "SAT-03-11-2018_010.pcap packet # 34000\n",
            "SAT-03-11-2018_067.pcap packet # 29000\n",
            "SAT-03-11-2018_0145.pcap packet # 25000\n",
            "SAT-03-11-2018_0_0.pcap packet # 26000\n",
            "SAT-03-11-2018_010.pcap packet # 35000\n",
            "SAT-03-11-2018_067.pcap packet # 30000\n",
            "SAT-03-11-2018_0145.pcap packet # 26000\n",
            "SAT-03-11-2018_0_0.pcap packet # 27000\n",
            "SAT-03-11-2018_010.pcap packet # 36000\n",
            "SAT-03-11-2018_067.pcap packet # 31000\n",
            "SAT-03-11-2018_010.pcap packet # 37000\n",
            "SAT-03-11-2018_0_0.pcap packet # 28000\n",
            "SAT-03-11-2018_0145.pcap packet # 27000\n",
            "SAT-03-11-2018_067.pcap packet # 32000\n",
            "SAT-03-11-2018_010.pcap packet # 38000\n",
            "SAT-03-11-2018_0145.pcap packet # 28000\n",
            "SAT-03-11-2018_0_0.pcap packet # 29000\n",
            "SAT-03-11-2018_067.pcap packet # 33000\n",
            "SAT-03-11-2018_010.pcap packet # 39000\n",
            "SAT-03-11-2018_0_0.pcap packet # 30000\n",
            "SAT-03-11-2018_0145.pcap packet # 29000\n",
            "SAT-03-11-2018_010.pcap packet # 40000\n",
            "SAT-03-11-2018_067.pcap packet # 34000\n",
            "SAT-03-11-2018_010.pcap packet # 41000\n",
            "SAT-03-11-2018_0145.pcap packet # 30000\n",
            "SAT-03-11-2018_0_0.pcap packet # 31000\n",
            "SAT-03-11-2018_067.pcap packet # 35000\n",
            "SAT-03-11-2018_010.pcap packet # 42000\n",
            "SAT-03-11-2018_067.pcap packet # 36000\n",
            "SAT-03-11-2018_0145.pcap packet # 31000\n",
            "SAT-03-11-2018_0_0.pcap packet # 32000\n",
            "SAT-03-11-2018_010.pcap packet # 43000\n",
            "SAT-03-11-2018_067.pcap packet # 37000\n",
            "SAT-03-11-2018_0145.pcap packet # 32000\n",
            "SAT-03-11-2018_0_0.pcap packet # 33000\n",
            "SAT-03-11-2018_010.pcap packet # 44000\n",
            "SAT-03-11-2018_067.pcap packet # 38000\n",
            "SAT-03-11-2018_010.pcap packet # 45000\n",
            "SAT-03-11-2018_0145.pcap packet # 33000\n",
            "SAT-03-11-2018_0_0.pcap packet # 34000\n",
            "SAT-03-11-2018_067.pcap packet # 39000\n",
            "SAT-03-11-2018_010.pcap packet # 46000\n",
            "SAT-03-11-2018_0145.pcap packet # 34000\n",
            "SAT-03-11-2018_0_0.pcap packet # 35000\n",
            "SAT-03-11-2018_010.pcap packet # 47000\n",
            "SAT-03-11-2018_067.pcap packet # 40000\n",
            "SAT-03-11-2018_0145.pcap packet # 35000\n",
            "SAT-03-11-2018_010.pcap packet # 48000\n",
            "Completed file SAT-03-11-2018_0_0.pcap in 324.8225622177124 seconds.\n",
            "SAT-03-11-2018_067.pcap packet # 41000\n",
            "SAT-03-11-2018_010.pcap packet # 49000\n",
            "SAT-03-11-2018_0145.pcap packet # 36000\n",
            "SAT-03-11-2018_067.pcap packet # 42000\n",
            "SAT-03-11-2018_010.pcap packet # 50000\n",
            "SAT-03-11-2018_0145.pcap packet # 37000\n",
            "SAT-03-11-2018_067.pcap packet # 43000\n",
            "SAT-03-11-2018_010.pcap packet # 51000\n",
            "SAT-03-11-2018_0145.pcap packet # 38000\n",
            "SAT-03-11-2018_010.pcap packet # 52000\n",
            "SAT-03-11-2018_067.pcap packet # 44000\n",
            "SAT-03-11-2018_0145.pcap packet # 39000\n",
            "SAT-03-11-2018_010.pcap packet # 53000\n",
            "SAT-03-11-2018_067.pcap packet # 45000\n",
            "SAT-03-11-2018_010.pcap packet # 54000\n",
            "SAT-03-11-2018_0145.pcap packet # 40000\n",
            "SAT-03-11-2018_067.pcap packet # 46000\n",
            "SAT-03-11-2018_010.pcap packet # 55000\n",
            "SAT-03-11-2018_0145.pcap packet # 41000\n",
            "SAT-03-11-2018_067.pcap packet # 47000\n",
            "SAT-03-11-2018_010.pcap packet # 56000\n",
            "SAT-03-11-2018_010.pcap packet # 57000\n",
            "SAT-03-11-2018_067.pcap packet # 48000\n",
            "SAT-03-11-2018_0145.pcap packet # 42000\n",
            "SAT-03-11-2018_010.pcap packet # 58000\n",
            "SAT-03-11-2018_067.pcap packet # 49000\n",
            "SAT-03-11-2018_0145.pcap packet # 43000\n",
            "SAT-03-11-2018_010.pcap packet # 59000\n",
            "SAT-03-11-2018_067.pcap packet # 50000\n",
            "SAT-03-11-2018_0145.pcap packet # 44000\n",
            "SAT-03-11-2018_010.pcap packet # 60000\n",
            "SAT-03-11-2018_067.pcap packet # 51000\n",
            "SAT-03-11-2018_0145.pcap packet # 45000\n",
            "SAT-03-11-2018_010.pcap packet # 61000\n",
            "SAT-03-11-2018_067.pcap packet # 52000\n",
            "SAT-03-11-2018_010.pcap packet # 62000\n",
            "SAT-03-11-2018_0145.pcap packet # 46000\n",
            "SAT-03-11-2018_067.pcap packet # 53000\n",
            "SAT-03-11-2018_010.pcap packet # 63000\n",
            "SAT-03-11-2018_0145.pcap packet # 47000\n",
            "SAT-03-11-2018_010.pcap packet # 64000\n",
            "SAT-03-11-2018_067.pcap packet # 54000\n",
            "SAT-03-11-2018_010.pcap packet # 65000\n",
            "SAT-03-11-2018_0145.pcap packet # 48000\n",
            "SAT-03-11-2018_067.pcap packet # 55000\n",
            "SAT-03-11-2018_010.pcap packet # 66000\n",
            "SAT-03-11-2018_0145.pcap packet # 49000\n",
            "SAT-03-11-2018_067.pcap packet # 56000\n",
            "SAT-03-11-2018_010.pcap packet # 67000\n",
            "SAT-03-11-2018_0145.pcap packet # 50000\n",
            "SAT-03-11-2018_067.pcap packet # 57000\n",
            "SAT-03-11-2018_010.pcap packet # 68000\n",
            "SAT-03-11-2018_0145.pcap packet # 51000\n",
            "SAT-03-11-2018_010.pcap packet # 69000\n",
            "SAT-03-11-2018_067.pcap packet # 58000\n",
            "SAT-03-11-2018_010.pcap packet # 70000\n",
            "SAT-03-11-2018_0145.pcap packet # 52000\n",
            "SAT-03-11-2018_067.pcap packet # 59000\n",
            "SAT-03-11-2018_010.pcap packet # 71000\n",
            "SAT-03-11-2018_0145.pcap packet # 53000\n",
            "SAT-03-11-2018_067.pcap packet # 60000\n",
            "SAT-03-11-2018_010.pcap packet # 72000\n",
            "SAT-03-11-2018_067.pcap packet # 61000\n",
            "SAT-03-11-2018_010.pcap packet # 73000\n",
            "SAT-03-11-2018_0145.pcap packet # 54000\n",
            "SAT-03-11-2018_010.pcap packet # 74000\n",
            "SAT-03-11-2018_067.pcap packet # 62000\n",
            "SAT-03-11-2018_0145.pcap packet # 55000\n",
            "SAT-03-11-2018_010.pcap packet # 75000\n",
            "SAT-03-11-2018_067.pcap packet # 63000\n",
            "SAT-03-11-2018_0145.pcap packet # 56000\n",
            "SAT-03-11-2018_010.pcap packet # 76000\n",
            "SAT-03-11-2018_067.pcap packet # 64000\n",
            "SAT-03-11-2018_0145.pcap packet # 57000\n",
            "SAT-03-11-2018_010.pcap packet # 77000\n",
            "SAT-03-11-2018_067.pcap packet # 65000\n",
            "SAT-03-11-2018_010.pcap packet # 78000\n",
            "SAT-03-11-2018_0145.pcap packet # 58000\n",
            "SAT-03-11-2018_067.pcap packet # 66000\n",
            "SAT-03-11-2018_010.pcap packet # 79000\n",
            "SAT-03-11-2018_0145.pcap packet # 59000\n",
            "SAT-03-11-2018_010.pcap packet # 80000\n",
            "SAT-03-11-2018_067.pcap packet # 67000\n",
            "SAT-03-11-2018_010.pcap packet # 81000\n",
            "SAT-03-11-2018_0145.pcap packet # 60000\n",
            "SAT-03-11-2018_067.pcap packet # 68000\n",
            "SAT-03-11-2018_010.pcap packet # 82000\n",
            "SAT-03-11-2018_0145.pcap packet # 61000\n",
            "SAT-03-11-2018_067.pcap packet # 69000\n",
            "SAT-03-11-2018_010.pcap packet # 83000\n",
            "SAT-03-11-2018_0145.pcap packet # 62000\n",
            "SAT-03-11-2018_067.pcap packet # 70000\n",
            "SAT-03-11-2018_010.pcap packet # 84000\n",
            "SAT-03-11-2018_010.pcap packet # 85000\n",
            "SAT-03-11-2018_067.pcap packet # 71000\n",
            "SAT-03-11-2018_0145.pcap packet # 63000\n",
            "SAT-03-11-2018_010.pcap packet # 86000\n",
            "SAT-03-11-2018_067.pcap packet # 72000\n",
            "SAT-03-11-2018_0145.pcap packet # 64000\n",
            "SAT-03-11-2018_010.pcap packet # 87000\n",
            "SAT-03-11-2018_067.pcap packet # 73000\n",
            "SAT-03-11-2018_0145.pcap packet # 65000\n",
            "SAT-03-11-2018_010.pcap packet # 88000\n",
            "SAT-03-11-2018_067.pcap packet # 74000\n",
            "SAT-03-11-2018_010.pcap packet # 89000\n",
            "SAT-03-11-2018_0145.pcap packet # 66000\n",
            "SAT-03-11-2018_067.pcap packet # 75000\n",
            "SAT-03-11-2018_010.pcap packet # 90000\n",
            "SAT-03-11-2018_0145.pcap packet # 67000\n",
            "SAT-03-11-2018_010.pcap packet # 91000\n",
            "SAT-03-11-2018_067.pcap packet # 76000\n",
            "SAT-03-11-2018_0145.pcap packet # 68000\n",
            "SAT-03-11-2018_010.pcap packet # 92000\n",
            "SAT-03-11-2018_067.pcap packet # 77000\n",
            "SAT-03-11-2018_010.pcap packet # 93000\n",
            "SAT-03-11-2018_0145.pcap packet # 69000\n",
            "SAT-03-11-2018_067.pcap packet # 78000\n",
            "SAT-03-11-2018_010.pcap packet # 94000\n",
            "SAT-03-11-2018_0145.pcap packet # 70000\n",
            "SAT-03-11-2018_067.pcap packet # 79000\n",
            "SAT-03-11-2018_010.pcap packet # 95000\n",
            "SAT-03-11-2018_0145.pcap packet # 71000\n",
            "SAT-03-11-2018_010.pcap packet # 96000\n",
            "SAT-03-11-2018_067.pcap packet # 80000\n",
            "SAT-03-11-2018_0145.pcap packet # 72000\n",
            "SAT-03-11-2018_010.pcap packet # 97000\n",
            "SAT-03-11-2018_067.pcap packet # 81000\n",
            "SAT-03-11-2018_010.pcap packet # 98000\n",
            "SAT-03-11-2018_0145.pcap packet # 73000\n",
            "SAT-03-11-2018_067.pcap packet # 82000\n",
            "SAT-03-11-2018_010.pcap packet # 99000\n",
            "SAT-03-11-2018_0145.pcap packet # 74000\n",
            "SAT-03-11-2018_067.pcap packet # 83000\n",
            "SAT-03-11-2018_010.pcap packet # 100000\n",
            "SAT-03-11-2018_0145.pcap packet # 75000\n",
            "SAT-03-11-2018_010.pcap packet # 101000\n",
            "SAT-03-11-2018_067.pcap packet # 84000\n",
            "SAT-03-11-2018_010.pcap packet # 102000\n",
            "SAT-03-11-2018_067.pcap packet # 85000\n",
            "SAT-03-11-2018_0145.pcap packet # 76000\n",
            "SAT-03-11-2018_010.pcap packet # 103000\n",
            "SAT-03-11-2018_067.pcap packet # 86000\n",
            "SAT-03-11-2018_0145.pcap packet # 77000\n",
            "SAT-03-11-2018_010.pcap packet # 104000\n",
            "SAT-03-11-2018_067.pcap packet # 87000\n",
            "SAT-03-11-2018_010.pcap packet # 105000\n",
            "SAT-03-11-2018_0145.pcap packet # 78000\n",
            "SAT-03-11-2018_067.pcap packet # 88000\n",
            "SAT-03-11-2018_010.pcap packet # 106000\n",
            "SAT-03-11-2018_0145.pcap packet # 79000\n",
            "SAT-03-11-2018_010.pcap packet # 107000\n",
            "SAT-03-11-2018_067.pcap packet # 89000\n",
            "SAT-03-11-2018_0145.pcap packet # 80000\n",
            "SAT-03-11-2018_010.pcap packet # 108000\n",
            "SAT-03-11-2018_067.pcap packet # 90000\n",
            "SAT-03-11-2018_010.pcap packet # 109000\n",
            "SAT-03-11-2018_0145.pcap packet # 81000\n",
            "SAT-03-11-2018_067.pcap packet # 91000\n",
            "SAT-03-11-2018_010.pcap packet # 110000\n",
            "SAT-03-11-2018_0145.pcap packet # 82000\n",
            "SAT-03-11-2018_010.pcap packet # 111000\n",
            "SAT-03-11-2018_067.pcap packet # 92000\n",
            "SAT-03-11-2018_0145.pcap packet # 83000\n",
            "SAT-03-11-2018_010.pcap packet # 112000\n",
            "SAT-03-11-2018_067.pcap packet # 93000\n",
            "SAT-03-11-2018_010.pcap packet # 113000\n",
            "SAT-03-11-2018_0145.pcap packet # 84000\n",
            "SAT-03-11-2018_067.pcap packet # 94000\n",
            "SAT-03-11-2018_010.pcap packet # 114000\n",
            "SAT-03-11-2018_0145.pcap packet # 85000\n",
            "SAT-03-11-2018_010.pcap packet # 115000\n",
            "SAT-03-11-2018_067.pcap packet # 95000\n",
            "SAT-03-11-2018_010.pcap packet # 116000\n",
            "SAT-03-11-2018_0145.pcap packet # 86000\n",
            "SAT-03-11-2018_067.pcap packet # 96000\n",
            "SAT-03-11-2018_010.pcap packet # 117000\n",
            "SAT-03-11-2018_067.pcap packet # 97000\n",
            "SAT-03-11-2018_0145.pcap packet # 87000\n",
            "SAT-03-11-2018_010.pcap packet # 118000\n",
            "SAT-03-11-2018_067.pcap packet # 98000\n",
            "SAT-03-11-2018_010.pcap packet # 119000\n",
            "SAT-03-11-2018_0145.pcap packet # 88000\n",
            "SAT-03-11-2018_010.pcap packet # 120000\n",
            "SAT-03-11-2018_067.pcap packet # 99000\n",
            "SAT-03-11-2018_0145.pcap packet # 89000\n",
            "SAT-03-11-2018_010.pcap packet # 121000\n",
            "SAT-03-11-2018_067.pcap packet # 100000\n",
            "SAT-03-11-2018_0145.pcap packet # 90000\n",
            "SAT-03-11-2018_010.pcap packet # 122000\n",
            "SAT-03-11-2018_067.pcap packet # 101000\n",
            "SAT-03-11-2018_010.pcap packet # 123000\n",
            "SAT-03-11-2018_0145.pcap packet # 91000\n",
            "SAT-03-11-2018_067.pcap packet # 102000\n",
            "SAT-03-11-2018_010.pcap packet # 124000\n",
            "SAT-03-11-2018_0145.pcap packet # 92000\n",
            "SAT-03-11-2018_010.pcap packet # 125000\n",
            "SAT-03-11-2018_067.pcap packet # 103000\n",
            "SAT-03-11-2018_0145.pcap packet # 93000\n",
            "SAT-03-11-2018_010.pcap packet # 126000\n",
            "SAT-03-11-2018_067.pcap packet # 104000\n",
            "SAT-03-11-2018_010.pcap packet # 127000\n",
            "SAT-03-11-2018_0145.pcap packet # 94000\n",
            "SAT-03-11-2018_067.pcap packet # 105000\n",
            "SAT-03-11-2018_010.pcap packet # 128000\n",
            "SAT-03-11-2018_0145.pcap packet # 95000\n",
            "SAT-03-11-2018_067.pcap packet # 106000\n",
            "SAT-03-11-2018_010.pcap packet # 129000\n",
            "SAT-03-11-2018_010.pcap packet # 130000\n",
            "SAT-03-11-2018_0145.pcap packet # 96000\n",
            "SAT-03-11-2018_067.pcap packet # 107000\n",
            "SAT-03-11-2018_010.pcap packet # 131000\n",
            "SAT-03-11-2018_067.pcap packet # 108000\n",
            "SAT-03-11-2018_0145.pcap packet # 97000\n",
            "SAT-03-11-2018_010.pcap packet # 132000\n",
            "SAT-03-11-2018_0145.pcap packet # 98000\n",
            "SAT-03-11-2018_067.pcap packet # 109000\n",
            "SAT-03-11-2018_010.pcap packet # 133000\n",
            "SAT-03-11-2018_067.pcap packet # 110000\n",
            "SAT-03-11-2018_0145.pcap packet # 99000\n",
            "SAT-03-11-2018_010.pcap packet # 134000\n",
            "SAT-03-11-2018_010.pcap packet # 135000\n",
            "SAT-03-11-2018_067.pcap packet # 111000\n",
            "SAT-03-11-2018_0145.pcap packet # 100000\n",
            "SAT-03-11-2018_010.pcap packet # 136000\n",
            "SAT-03-11-2018_067.pcap packet # 112000\n",
            "SAT-03-11-2018_0145.pcap packet # 101000\n",
            "SAT-03-11-2018_010.pcap packet # 137000\n",
            "SAT-03-11-2018_067.pcap packet # 113000\n",
            "SAT-03-11-2018_0145.pcap packet # 102000\n",
            "SAT-03-11-2018_010.pcap packet # 138000\n",
            "SAT-03-11-2018_067.pcap packet # 114000\n",
            "SAT-03-11-2018_0145.pcap packet # 103000\n",
            "SAT-03-11-2018_010.pcap packet # 139000\n",
            "SAT-03-11-2018_010.pcap packet # 140000\n",
            "SAT-03-11-2018_067.pcap packet # 115000\n",
            "SAT-03-11-2018_0145.pcap packet # 104000\n",
            "SAT-03-11-2018_010.pcap packet # 141000\n",
            "SAT-03-11-2018_067.pcap packet # 116000\n",
            "SAT-03-11-2018_0145.pcap packet # 105000\n",
            "SAT-03-11-2018_010.pcap packet # 142000\n",
            "SAT-03-11-2018_067.pcap packet # 117000\n",
            "SAT-03-11-2018_010.pcap packet # 143000\n",
            "SAT-03-11-2018_0145.pcap packet # 106000\n",
            "SAT-03-11-2018_067.pcap packet # 118000\n",
            "SAT-03-11-2018_010.pcap packet # 144000\n",
            "SAT-03-11-2018_0145.pcap packet # 107000\n",
            "SAT-03-11-2018_010.pcap packet # 145000\n",
            "SAT-03-11-2018_067.pcap packet # 119000\n",
            "SAT-03-11-2018_0145.pcap packet # 108000\n",
            "SAT-03-11-2018_010.pcap packet # 146000\n",
            "SAT-03-11-2018_067.pcap packet # 120000\n",
            "SAT-03-11-2018_0145.pcap packet # 109000\n",
            "SAT-03-11-2018_010.pcap packet # 147000\n",
            "SAT-03-11-2018_067.pcap packet # 121000\n",
            "SAT-03-11-2018_010.pcap packet # 148000\n",
            "SAT-03-11-2018_0145.pcap packet # 110000\n",
            "SAT-03-11-2018_010.pcap packet # 149000\n",
            "SAT-03-11-2018_067.pcap packet # 122000\n",
            "SAT-03-11-2018_0145.pcap packet # 111000\n",
            "SAT-03-11-2018_010.pcap packet # 150000\n",
            "SAT-03-11-2018_067.pcap packet # 123000\n",
            "SAT-03-11-2018_010.pcap packet # 151000\n",
            "SAT-03-11-2018_0145.pcap packet # 112000\n",
            "SAT-03-11-2018_067.pcap packet # 124000\n",
            "SAT-03-11-2018_010.pcap packet # 152000\n",
            "SAT-03-11-2018_0145.pcap packet # 113000\n",
            "SAT-03-11-2018_010.pcap packet # 153000\n",
            "SAT-03-11-2018_067.pcap packet # 125000\n",
            "SAT-03-11-2018_010.pcap packet # 154000\n",
            "SAT-03-11-2018_0145.pcap packet # 114000\n",
            "SAT-03-11-2018_067.pcap packet # 126000\n",
            "SAT-03-11-2018_010.pcap packet # 155000\n",
            "SAT-03-11-2018_0145.pcap packet # 115000\n",
            "SAT-03-11-2018_067.pcap packet # 127000\n",
            "SAT-03-11-2018_010.pcap packet # 156000\n",
            "SAT-03-11-2018_0145.pcap packet # 116000\n",
            "SAT-03-11-2018_010.pcap packet # 157000\n",
            "SAT-03-11-2018_067.pcap packet # 128000\n",
            "SAT-03-11-2018_010.pcap packet # 158000\n",
            "SAT-03-11-2018_0145.pcap packet # 117000\n",
            "SAT-03-11-2018_067.pcap packet # 129000\n",
            "SAT-03-11-2018_010.pcap packet # 159000\n",
            "SAT-03-11-2018_0145.pcap packet # 118000\n",
            "SAT-03-11-2018_067.pcap packet # 130000\n",
            "SAT-03-11-2018_010.pcap packet # 160000\n",
            "SAT-03-11-2018_010.pcap packet # 161000\n",
            "SAT-03-11-2018_067.pcap packet # 131000\n",
            "SAT-03-11-2018_0145.pcap packet # 119000\n",
            "SAT-03-11-2018_010.pcap packet # 162000\n",
            "SAT-03-11-2018_067.pcap packet # 132000\n",
            "SAT-03-11-2018_0145.pcap packet # 120000\n",
            "SAT-03-11-2018_010.pcap packet # 163000\n",
            "SAT-03-11-2018_067.pcap packet # 133000\n",
            "SAT-03-11-2018_0145.pcap packet # 121000\n",
            "SAT-03-11-2018_010.pcap packet # 164000\n",
            "SAT-03-11-2018_067.pcap packet # 134000\n",
            "SAT-03-11-2018_010.pcap packet # 165000\n",
            "SAT-03-11-2018_0145.pcap packet # 122000\n",
            "SAT-03-11-2018_010.pcap packet # 166000\n",
            "SAT-03-11-2018_067.pcap packet # 135000\n",
            "SAT-03-11-2018_0145.pcap packet # 123000\n",
            "SAT-03-11-2018_010.pcap packet # 167000\n",
            "SAT-03-11-2018_067.pcap packet # 136000\n",
            "SAT-03-11-2018_0145.pcap packet # 124000\n",
            "SAT-03-11-2018_010.pcap packet # 168000\n",
            "SAT-03-11-2018_067.pcap packet # 137000\n",
            "SAT-03-11-2018_010.pcap packet # 169000\n",
            "SAT-03-11-2018_0145.pcap packet # 125000\n",
            "SAT-03-11-2018_067.pcap packet # 138000\n",
            "SAT-03-11-2018_010.pcap packet # 170000\n",
            "SAT-03-11-2018_0145.pcap packet # 126000\n",
            "SAT-03-11-2018_010.pcap packet # 171000\n",
            "SAT-03-11-2018_067.pcap packet # 139000\n",
            "SAT-03-11-2018_010.pcap packet # 172000\n",
            "SAT-03-11-2018_0145.pcap packet # 127000\n",
            "SAT-03-11-2018_067.pcap packet # 140000\n",
            "SAT-03-11-2018_010.pcap packet # 173000\n",
            "SAT-03-11-2018_0145.pcap packet # 128000\n",
            "SAT-03-11-2018_067.pcap packet # 141000\n",
            "SAT-03-11-2018_010.pcap packet # 174000\n",
            "SAT-03-11-2018_0145.pcap packet # 129000\n",
            "SAT-03-11-2018_010.pcap packet # 175000\n",
            "SAT-03-11-2018_067.pcap packet # 142000\n",
            "SAT-03-11-2018_010.pcap packet # 176000\n",
            "SAT-03-11-2018_0145.pcap packet # 130000\n",
            "SAT-03-11-2018_067.pcap packet # 143000\n",
            "SAT-03-11-2018_010.pcap packet # 177000\n",
            "SAT-03-11-2018_0145.pcap packet # 131000\n",
            "SAT-03-11-2018_067.pcap packet # 144000\n",
            "SAT-03-11-2018_010.pcap packet # 178000\n",
            "SAT-03-11-2018_0145.pcap packet # 132000\n",
            "SAT-03-11-2018_010.pcap packet # 179000\n",
            "SAT-03-11-2018_067.pcap packet # 145000\n",
            "SAT-03-11-2018_010.pcap packet # 180000\n",
            "SAT-03-11-2018_0145.pcap packet # 133000\n",
            "SAT-03-11-2018_067.pcap packet # 146000\n",
            "SAT-03-11-2018_010.pcap packet # 181000\n",
            "SAT-03-11-2018_0145.pcap packet # 134000\n",
            "SAT-03-11-2018_067.pcap packet # 147000\n",
            "SAT-03-11-2018_010.pcap packet # 182000\n",
            "SAT-03-11-2018_0145.pcap packet # 135000\n",
            "SAT-03-11-2018_067.pcap packet # 148000\n",
            "SAT-03-11-2018_010.pcap packet # 183000\n",
            "SAT-03-11-2018_010.pcap packet # 184000\n",
            "SAT-03-11-2018_0145.pcap packet # 136000\n",
            "SAT-03-11-2018_067.pcap packet # 149000\n",
            "SAT-03-11-2018_010.pcap packet # 185000\n",
            "SAT-03-11-2018_0145.pcap packet # 137000\n",
            "SAT-03-11-2018_067.pcap packet # 150000\n",
            "SAT-03-11-2018_010.pcap packet # 186000\n",
            "SAT-03-11-2018_0145.pcap packet # 138000\n",
            "SAT-03-11-2018_010.pcap packet # 187000\n",
            "SAT-03-11-2018_067.pcap packet # 151000\n",
            "SAT-03-11-2018_010.pcap packet # 188000\n",
            "SAT-03-11-2018_0145.pcap packet # 139000\n",
            "SAT-03-11-2018_067.pcap packet # 152000\n",
            "SAT-03-11-2018_010.pcap packet # 189000\n",
            "SAT-03-11-2018_067.pcap packet # 153000\n",
            "SAT-03-11-2018_0145.pcap packet # 140000\n",
            "SAT-03-11-2018_010.pcap packet # 190000\n",
            "SAT-03-11-2018_067.pcap packet # 154000\n",
            "SAT-03-11-2018_010.pcap packet # 191000\n",
            "SAT-03-11-2018_0145.pcap packet # 141000\n",
            "SAT-03-11-2018_010.pcap packet # 192000\n",
            "SAT-03-11-2018_0145.pcap packet # 142000\n",
            "SAT-03-11-2018_010.pcap packet # 193000\n",
            "Completed file SAT-03-11-2018_067.pcap in 1043.551602602005 seconds.\n",
            "SAT-03-11-2018_0145.pcap packet # 143000\n",
            "SAT-03-11-2018_010.pcap packet # 194000\n",
            "SAT-03-11-2018_010.pcap packet # 195000\n",
            "SAT-03-11-2018_0145.pcap packet # 144000\n",
            "SAT-03-11-2018_010.pcap packet # 196000\n",
            "SAT-03-11-2018_0145.pcap packet # 145000\n",
            "SAT-03-11-2018_010.pcap packet # 197000\n",
            "SAT-03-11-2018_0145.pcap packet # 146000\n",
            "SAT-03-11-2018_010.pcap packet # 198000\n",
            "SAT-03-11-2018_010.pcap packet # 199000\n",
            "SAT-03-11-2018_0145.pcap packet # 147000\n",
            "SAT-03-11-2018_010.pcap packet # 200000\n",
            "SAT-03-11-2018_0145.pcap packet # 148000\n",
            "SAT-03-11-2018_010.pcap packet # 201000\n",
            "SAT-03-11-2018_010.pcap packet # 202000\n",
            "SAT-03-11-2018_0145.pcap packet # 149000\n",
            "SAT-03-11-2018_010.pcap packet # 203000\n",
            "SAT-03-11-2018_0145.pcap packet # 150000\n",
            "SAT-03-11-2018_010.pcap packet # 204000\n",
            "SAT-03-11-2018_0145.pcap packet # 151000\n",
            "SAT-03-11-2018_010.pcap packet # 205000\n",
            "SAT-03-11-2018_010.pcap packet # 206000\n",
            "SAT-03-11-2018_0145.pcap packet # 152000\n",
            "SAT-03-11-2018_010.pcap packet # 207000\n",
            "SAT-03-11-2018_0145.pcap packet # 153000\n",
            "SAT-03-11-2018_010.pcap packet # 208000\n",
            "SAT-03-11-2018_010.pcap packet # 209000\n",
            "SAT-03-11-2018_0145.pcap packet # 154000\n",
            "SAT-03-11-2018_010.pcap packet # 210000\n",
            "SAT-03-11-2018_0145.pcap packet # 155000\n",
            "SAT-03-11-2018_010.pcap packet # 211000\n",
            "SAT-03-11-2018_0145.pcap packet # 156000\n",
            "SAT-03-11-2018_010.pcap packet # 212000\n",
            "SAT-03-11-2018_010.pcap packet # 213000\n",
            "SAT-03-11-2018_0145.pcap packet # 157000\n",
            "SAT-03-11-2018_010.pcap packet # 214000\n",
            "SAT-03-11-2018_0145.pcap packet # 158000\n",
            "SAT-03-11-2018_010.pcap packet # 215000\n",
            "SAT-03-11-2018_0145.pcap packet # 159000\n",
            "SAT-03-11-2018_010.pcap packet # 216000\n",
            "SAT-03-11-2018_010.pcap packet # 217000\n",
            "SAT-03-11-2018_0145.pcap packet # 160000\n",
            "SAT-03-11-2018_010.pcap packet # 218000\n",
            "SAT-03-11-2018_0145.pcap packet # 161000\n",
            "SAT-03-11-2018_010.pcap packet # 219000\n",
            "SAT-03-11-2018_010.pcap packet # 220000\n",
            "SAT-03-11-2018_0145.pcap packet # 162000\n",
            "SAT-03-11-2018_010.pcap packet # 221000\n",
            "SAT-03-11-2018_0145.pcap packet # 163000\n",
            "SAT-03-11-2018_010.pcap packet # 222000\n",
            "SAT-03-11-2018_010.pcap packet # 223000\n",
            "SAT-03-11-2018_0145.pcap packet # 164000\n",
            "SAT-03-11-2018_010.pcap packet # 224000\n",
            "SAT-03-11-2018_0145.pcap packet # 165000\n",
            "SAT-03-11-2018_010.pcap packet # 225000\n",
            "SAT-03-11-2018_0145.pcap packet # 166000\n",
            "SAT-03-11-2018_010.pcap packet # 226000\n",
            "SAT-03-11-2018_0145.pcap packet # 167000\n",
            "SAT-03-11-2018_010.pcap packet # 227000\n",
            "SAT-03-11-2018_010.pcap packet # 228000\n",
            "SAT-03-11-2018_0145.pcap packet # 168000\n",
            "SAT-03-11-2018_010.pcap packet # 229000\n",
            "SAT-03-11-2018_0145.pcap packet # 169000\n",
            "SAT-03-11-2018_010.pcap packet # 230000\n",
            "SAT-03-11-2018_0145.pcap packet # 170000\n",
            "SAT-03-11-2018_010.pcap packet # 231000\n",
            "SAT-03-11-2018_0145.pcap packet # 171000\n",
            "SAT-03-11-2018_010.pcap packet # 232000\n",
            "SAT-03-11-2018_0145.pcap packet # 172000\n",
            "SAT-03-11-2018_010.pcap packet # 233000\n",
            "SAT-03-11-2018_010.pcap packet # 234000\n",
            "SAT-03-11-2018_0145.pcap packet # 173000\n",
            "SAT-03-11-2018_010.pcap packet # 235000\n",
            "SAT-03-11-2018_0145.pcap packet # 174000\n",
            "SAT-03-11-2018_010.pcap packet # 236000\n",
            "SAT-03-11-2018_0145.pcap packet # 175000\n",
            "SAT-03-11-2018_010.pcap packet # 237000\n",
            "SAT-03-11-2018_010.pcap packet # 238000\n",
            "SAT-03-11-2018_0145.pcap packet # 176000\n",
            "SAT-03-11-2018_010.pcap packet # 239000\n",
            "SAT-03-11-2018_0145.pcap packet # 177000\n",
            "SAT-03-11-2018_010.pcap packet # 240000\n",
            "SAT-03-11-2018_0145.pcap packet # 178000\n",
            "SAT-03-11-2018_010.pcap packet # 241000\n",
            "SAT-03-11-2018_0145.pcap packet # 179000\n",
            "SAT-03-11-2018_010.pcap packet # 242000\n",
            "SAT-03-11-2018_010.pcap packet # 243000\n",
            "SAT-03-11-2018_0145.pcap packet # 180000\n",
            "SAT-03-11-2018_010.pcap packet # 244000\n",
            "SAT-03-11-2018_0145.pcap packet # 181000\n",
            "SAT-03-11-2018_010.pcap packet # 245000\n",
            "SAT-03-11-2018_0145.pcap packet # 182000\n",
            "SAT-03-11-2018_010.pcap packet # 246000\n",
            "SAT-03-11-2018_010.pcap packet # 247000\n",
            "SAT-03-11-2018_0145.pcap packet # 183000\n",
            "SAT-03-11-2018_010.pcap packet # 248000\n",
            "SAT-03-11-2018_0145.pcap packet # 184000\n",
            "SAT-03-11-2018_010.pcap packet # 249000\n",
            "SAT-03-11-2018_0145.pcap packet # 185000\n",
            "SAT-03-11-2018_010.pcap packet # 250000\n",
            "SAT-03-11-2018_010.pcap packet # 251000\n",
            "SAT-03-11-2018_0145.pcap packet # 186000\n",
            "SAT-03-11-2018_010.pcap packet # 252000\n",
            "SAT-03-11-2018_0145.pcap packet # 187000\n",
            "SAT-03-11-2018_010.pcap packet # 253000\n",
            "SAT-03-11-2018_0145.pcap packet # 188000\n",
            "SAT-03-11-2018_010.pcap packet # 254000\n",
            "SAT-03-11-2018_010.pcap packet # 255000\n",
            "SAT-03-11-2018_0145.pcap packet # 189000\n",
            "SAT-03-11-2018_010.pcap packet # 256000\n",
            "SAT-03-11-2018_0145.pcap packet # 190000\n",
            "SAT-03-11-2018_010.pcap packet # 257000\n",
            "SAT-03-11-2018_0145.pcap packet # 191000\n",
            "SAT-03-11-2018_010.pcap packet # 258000\n",
            "SAT-03-11-2018_010.pcap packet # 259000\n",
            "SAT-03-11-2018_0145.pcap packet # 192000\n",
            "SAT-03-11-2018_010.pcap packet # 260000\n",
            "SAT-03-11-2018_0145.pcap packet # 193000\n",
            "SAT-03-11-2018_010.pcap packet # 261000\n",
            "SAT-03-11-2018_0145.pcap packet # 194000\n",
            "SAT-03-11-2018_010.pcap packet # 262000\n",
            "SAT-03-11-2018_010.pcap packet # 263000\n",
            "SAT-03-11-2018_0145.pcap packet # 195000\n",
            "SAT-03-11-2018_010.pcap packet # 264000\n",
            "SAT-03-11-2018_0145.pcap packet # 196000\n",
            "SAT-03-11-2018_010.pcap packet # 265000\n",
            "SAT-03-11-2018_0145.pcap packet # 197000\n",
            "SAT-03-11-2018_010.pcap packet # 266000\n",
            "SAT-03-11-2018_0145.pcap packet # 198000\n",
            "SAT-03-11-2018_010.pcap packet # 267000\n",
            "SAT-03-11-2018_010.pcap packet # 268000\n",
            "SAT-03-11-2018_0145.pcap packet # 199000\n",
            "SAT-03-11-2018_010.pcap packet # 269000\n",
            "SAT-03-11-2018_0145.pcap packet # 200000\n",
            "SAT-03-11-2018_010.pcap packet # 270000\n",
            "SAT-03-11-2018_0145.pcap packet # 201000\n",
            "SAT-03-11-2018_010.pcap packet # 271000\n",
            "SAT-03-11-2018_010.pcap packet # 272000\n",
            "SAT-03-11-2018_0145.pcap packet # 202000\n",
            "SAT-03-11-2018_010.pcap packet # 273000\n",
            "SAT-03-11-2018_0145.pcap packet # 203000\n",
            "SAT-03-11-2018_010.pcap packet # 274000\n",
            "SAT-03-11-2018_010.pcap packet # 275000\n",
            "SAT-03-11-2018_0145.pcap packet # 204000\n",
            "SAT-03-11-2018_010.pcap packet # 276000\n",
            "SAT-03-11-2018_0145.pcap packet # 205000\n",
            "SAT-03-11-2018_010.pcap packet # 277000\n",
            "SAT-03-11-2018_010.pcap packet # 278000\n",
            "SAT-03-11-2018_0145.pcap packet # 206000\n",
            "SAT-03-11-2018_010.pcap packet # 279000\n",
            "SAT-03-11-2018_0145.pcap packet # 207000\n",
            "SAT-03-11-2018_010.pcap packet # 280000\n",
            "SAT-03-11-2018_0145.pcap packet # 208000\n",
            "SAT-03-11-2018_010.pcap packet # 281000\n",
            "SAT-03-11-2018_010.pcap packet # 282000\n",
            "SAT-03-11-2018_0145.pcap packet # 209000\n",
            "SAT-03-11-2018_010.pcap packet # 283000\n",
            "SAT-03-11-2018_0145.pcap packet # 210000\n",
            "SAT-03-11-2018_010.pcap packet # 284000\n",
            "SAT-03-11-2018_0145.pcap packet # 211000\n",
            "SAT-03-11-2018_010.pcap packet # 285000\n",
            "SAT-03-11-2018_0145.pcap packet # 212000\n",
            "SAT-03-11-2018_010.pcap packet # 286000\n",
            "SAT-03-11-2018_0145.pcap packet # 213000\n",
            "SAT-03-11-2018_010.pcap packet # 287000\n",
            "SAT-03-11-2018_0145.pcap packet # 214000\n",
            "SAT-03-11-2018_010.pcap packet # 288000\n",
            "SAT-03-11-2018_0145.pcap packet # 215000\n",
            "SAT-03-11-2018_010.pcap packet # 289000\n",
            "SAT-03-11-2018_010.pcap packet # 290000\n",
            "SAT-03-11-2018_0145.pcap packet # 216000\n",
            "SAT-03-11-2018_010.pcap packet # 291000\n",
            "SAT-03-11-2018_0145.pcap packet # 217000\n",
            "SAT-03-11-2018_010.pcap packet # 292000\n",
            "SAT-03-11-2018_0145.pcap packet # 218000\n",
            "SAT-03-11-2018_010.pcap packet # 293000\n",
            "SAT-03-11-2018_010.pcap packet # 294000\n",
            "SAT-03-11-2018_0145.pcap packet # 219000\n",
            "SAT-03-11-2018_010.pcap packet # 295000\n",
            "SAT-03-11-2018_0145.pcap packet # 220000\n",
            "SAT-03-11-2018_010.pcap packet # 296000\n",
            "SAT-03-11-2018_0145.pcap packet # 221000\n",
            "SAT-03-11-2018_010.pcap packet # 297000\n",
            "SAT-03-11-2018_010.pcap packet # 298000\n",
            "SAT-03-11-2018_0145.pcap packet # 222000\n",
            "SAT-03-11-2018_010.pcap packet # 299000\n",
            "SAT-03-11-2018_0145.pcap packet # 223000\n",
            "SAT-03-11-2018_010.pcap packet # 300000\n",
            "SAT-03-11-2018_0145.pcap packet # 224000\n",
            "SAT-03-11-2018_010.pcap packet # 301000\n",
            "SAT-03-11-2018_010.pcap packet # 302000\n",
            "SAT-03-11-2018_0145.pcap packet # 225000\n",
            "SAT-03-11-2018_010.pcap packet # 303000\n",
            "SAT-03-11-2018_0145.pcap packet # 226000\n",
            "SAT-03-11-2018_010.pcap packet # 304000\n",
            "SAT-03-11-2018_0145.pcap packet # 227000\n",
            "SAT-03-11-2018_010.pcap packet # 305000\n",
            "SAT-03-11-2018_010.pcap packet # 306000\n",
            "SAT-03-11-2018_0145.pcap packet # 228000\n",
            "SAT-03-11-2018_010.pcap packet # 307000\n",
            "SAT-03-11-2018_0145.pcap packet # 229000\n",
            "SAT-03-11-2018_010.pcap packet # 308000\n",
            "SAT-03-11-2018_0145.pcap packet # 230000\n",
            "SAT-03-11-2018_010.pcap packet # 309000\n",
            "SAT-03-11-2018_0145.pcap packet # 231000\n",
            "SAT-03-11-2018_010.pcap packet # 310000\n",
            "SAT-03-11-2018_0145.pcap packet # 232000\n",
            "SAT-03-11-2018_010.pcap packet # 311000\n",
            "SAT-03-11-2018_010.pcap packet # 312000\n",
            "Completed file SAT-03-11-2018_0145.pcap in 1409.983453989029 seconds.\n",
            "SAT-03-11-2018_010.pcap packet # 313000\n",
            "SAT-03-11-2018_010.pcap packet # 314000\n",
            "SAT-03-11-2018_010.pcap packet # 315000\n",
            "SAT-03-11-2018_010.pcap packet # 316000\n",
            "SAT-03-11-2018_010.pcap packet # 317000\n",
            "SAT-03-11-2018_010.pcap packet # 318000\n",
            "SAT-03-11-2018_010.pcap packet # 319000\n",
            "SAT-03-11-2018_010.pcap packet # 320000\n",
            "SAT-03-11-2018_010.pcap packet # 321000\n",
            "SAT-03-11-2018_010.pcap packet # 322000\n",
            "SAT-03-11-2018_010.pcap packet # 323000\n",
            "SAT-03-11-2018_010.pcap packet # 324000\n",
            "SAT-03-11-2018_010.pcap packet # 325000\n",
            "SAT-03-11-2018_010.pcap packet # 326000\n",
            "SAT-03-11-2018_010.pcap packet # 327000\n",
            "SAT-03-11-2018_010.pcap packet # 328000\n",
            "SAT-03-11-2018_010.pcap packet # 329000\n",
            "SAT-03-11-2018_010.pcap packet # 330000\n",
            "SAT-03-11-2018_010.pcap packet # 331000\n",
            "SAT-03-11-2018_010.pcap packet # 332000\n",
            "SAT-03-11-2018_010.pcap packet # 333000\n",
            "SAT-03-11-2018_010.pcap packet # 334000\n",
            "SAT-03-11-2018_010.pcap packet # 335000\n",
            "SAT-03-11-2018_010.pcap packet # 336000\n",
            "SAT-03-11-2018_010.pcap packet # 337000\n",
            "SAT-03-11-2018_010.pcap packet # 338000\n",
            "SAT-03-11-2018_010.pcap packet # 339000\n",
            "SAT-03-11-2018_010.pcap packet # 340000\n",
            "SAT-03-11-2018_010.pcap packet # 341000\n",
            "SAT-03-11-2018_010.pcap packet # 342000\n",
            "SAT-03-11-2018_010.pcap packet # 343000\n",
            "SAT-03-11-2018_010.pcap packet # 344000\n",
            "SAT-03-11-2018_010.pcap packet # 345000\n",
            "SAT-03-11-2018_010.pcap packet # 346000\n",
            "SAT-03-11-2018_010.pcap packet # 347000\n",
            "SAT-03-11-2018_010.pcap packet # 348000\n",
            "SAT-03-11-2018_010.pcap packet # 349000\n",
            "SAT-03-11-2018_010.pcap packet # 350000\n",
            "SAT-03-11-2018_010.pcap packet # 351000\n",
            "SAT-03-11-2018_010.pcap packet # 352000\n",
            "SAT-03-11-2018_010.pcap packet # 353000\n",
            "SAT-03-11-2018_010.pcap packet # 354000\n",
            "SAT-03-11-2018_010.pcap packet # 355000\n",
            "SAT-03-11-2018_010.pcap packet # 356000\n",
            "SAT-03-11-2018_010.pcap packet # 357000\n",
            "SAT-03-11-2018_010.pcap packet # 358000\n",
            "SAT-03-11-2018_010.pcap packet # 359000\n",
            "SAT-03-11-2018_010.pcap packet # 360000\n",
            "SAT-03-11-2018_010.pcap packet # 361000\n",
            "SAT-03-11-2018_010.pcap packet # 362000\n",
            "SAT-03-11-2018_010.pcap packet # 363000\n",
            "SAT-03-11-2018_010.pcap packet # 364000\n",
            "SAT-03-11-2018_010.pcap packet # 365000\n",
            "SAT-03-11-2018_010.pcap packet # 366000\n",
            "SAT-03-11-2018_010.pcap packet # 367000\n",
            "SAT-03-11-2018_010.pcap packet # 368000\n",
            "SAT-03-11-2018_010.pcap packet # 369000\n",
            "SAT-03-11-2018_010.pcap packet # 370000\n",
            "SAT-03-11-2018_010.pcap packet # 371000\n",
            "SAT-03-11-2018_010.pcap packet # 372000\n",
            "SAT-03-11-2018_010.pcap packet # 373000\n",
            "SAT-03-11-2018_010.pcap packet # 374000\n",
            "SAT-03-11-2018_010.pcap packet # 375000\n",
            "SAT-03-11-2018_010.pcap packet # 376000\n",
            "SAT-03-11-2018_010.pcap packet # 377000\n",
            "SAT-03-11-2018_010.pcap packet # 378000\n",
            "SAT-03-11-2018_010.pcap packet # 379000\n",
            "SAT-03-11-2018_010.pcap packet # 380000\n",
            "SAT-03-11-2018_010.pcap packet # 381000\n",
            "SAT-03-11-2018_010.pcap packet # 382000\n",
            "SAT-03-11-2018_010.pcap packet # 383000\n",
            "SAT-03-11-2018_010.pcap packet # 384000\n",
            "SAT-03-11-2018_010.pcap packet # 385000\n",
            "SAT-03-11-2018_010.pcap packet # 386000\n",
            "SAT-03-11-2018_010.pcap packet # 387000\n",
            "SAT-03-11-2018_010.pcap packet # 388000\n",
            "SAT-03-11-2018_010.pcap packet # 389000\n",
            "SAT-03-11-2018_010.pcap packet # 390000\n",
            "SAT-03-11-2018_010.pcap packet # 391000\n",
            "SAT-03-11-2018_010.pcap packet # 392000\n",
            "SAT-03-11-2018_010.pcap packet # 393000\n",
            "SAT-03-11-2018_010.pcap packet # 394000\n",
            "SAT-03-11-2018_010.pcap packet # 395000\n",
            "SAT-03-11-2018_010.pcap packet # 396000\n",
            "SAT-03-11-2018_010.pcap packet # 397000\n",
            "SAT-03-11-2018_010.pcap packet # 398000\n",
            "SAT-03-11-2018_010.pcap packet # 399000\n",
            "SAT-03-11-2018_010.pcap packet # 400000\n",
            "SAT-03-11-2018_010.pcap packet # 401000\n",
            "SAT-03-11-2018_010.pcap packet # 402000\n",
            "SAT-03-11-2018_010.pcap packet # 403000\n",
            "SAT-03-11-2018_010.pcap packet # 404000\n",
            "SAT-03-11-2018_010.pcap packet # 405000\n",
            "SAT-03-11-2018_010.pcap packet # 406000\n",
            "SAT-03-11-2018_010.pcap packet # 407000\n",
            "SAT-03-11-2018_010.pcap packet # 408000\n",
            "SAT-03-11-2018_010.pcap packet # 409000\n",
            "SAT-03-11-2018_010.pcap packet # 410000\n",
            "SAT-03-11-2018_010.pcap packet # 411000\n",
            "SAT-03-11-2018_010.pcap packet # 412000\n",
            "SAT-03-11-2018_010.pcap packet # 413000\n",
            "SAT-03-11-2018_010.pcap packet # 414000\n",
            "SAT-03-11-2018_010.pcap packet # 415000\n",
            "SAT-03-11-2018_010.pcap packet # 416000\n",
            "SAT-03-11-2018_010.pcap packet # 417000\n",
            "SAT-03-11-2018_010.pcap packet # 418000\n",
            "SAT-03-11-2018_010.pcap packet # 419000\n",
            "SAT-03-11-2018_010.pcap packet # 420000\n",
            "SAT-03-11-2018_010.pcap packet # 421000\n",
            "SAT-03-11-2018_010.pcap packet # 422000\n",
            "SAT-03-11-2018_010.pcap packet # 423000\n",
            "SAT-03-11-2018_010.pcap packet # 424000\n",
            "SAT-03-11-2018_010.pcap packet # 425000\n",
            "SAT-03-11-2018_010.pcap packet # 426000\n",
            "SAT-03-11-2018_010.pcap packet # 427000\n",
            "SAT-03-11-2018_010.pcap packet # 428000\n",
            "SAT-03-11-2018_010.pcap packet # 429000\n",
            "SAT-03-11-2018_010.pcap packet # 430000\n",
            "SAT-03-11-2018_010.pcap packet # 431000\n",
            "SAT-03-11-2018_010.pcap packet # 432000\n",
            "SAT-03-11-2018_010.pcap packet # 433000\n",
            "SAT-03-11-2018_010.pcap packet # 434000\n",
            "SAT-03-11-2018_010.pcap packet # 435000\n",
            "SAT-03-11-2018_010.pcap packet # 436000\n",
            "SAT-03-11-2018_010.pcap packet # 437000\n",
            "SAT-03-11-2018_010.pcap packet # 438000\n",
            "SAT-03-11-2018_010.pcap packet # 439000\n",
            "SAT-03-11-2018_010.pcap packet # 440000\n",
            "SAT-03-11-2018_010.pcap packet # 441000\n",
            "SAT-03-11-2018_010.pcap packet # 442000\n",
            "SAT-03-11-2018_010.pcap packet # 443000\n",
            "SAT-03-11-2018_010.pcap packet # 444000\n",
            "SAT-03-11-2018_010.pcap packet # 445000\n",
            "SAT-03-11-2018_010.pcap packet # 446000\n",
            "SAT-03-11-2018_010.pcap packet # 447000\n",
            "SAT-03-11-2018_010.pcap packet # 448000\n",
            "SAT-03-11-2018_010.pcap packet # 449000\n",
            "SAT-03-11-2018_010.pcap packet # 450000\n",
            "SAT-03-11-2018_010.pcap packet # 451000\n",
            "SAT-03-11-2018_010.pcap packet # 452000\n",
            "SAT-03-11-2018_010.pcap packet # 453000\n",
            "SAT-03-11-2018_010.pcap packet # 454000\n",
            "SAT-03-11-2018_010.pcap packet # 455000\n",
            "SAT-03-11-2018_010.pcap packet # 456000\n",
            "SAT-03-11-2018_010.pcap packet # 457000\n",
            "SAT-03-11-2018_010.pcap packet # 458000\n",
            "SAT-03-11-2018_010.pcap packet # 459000\n",
            "SAT-03-11-2018_010.pcap packet # 460000\n",
            "SAT-03-11-2018_010.pcap packet # 461000\n",
            "SAT-03-11-2018_010.pcap packet # 462000\n",
            "SAT-03-11-2018_010.pcap packet # 463000\n",
            "SAT-03-11-2018_010.pcap packet # 464000\n",
            "SAT-03-11-2018_010.pcap packet # 465000\n",
            "SAT-03-11-2018_010.pcap packet # 466000\n",
            "SAT-03-11-2018_010.pcap packet # 467000\n",
            "SAT-03-11-2018_010.pcap packet # 468000\n",
            "SAT-03-11-2018_010.pcap packet # 469000\n",
            "SAT-03-11-2018_010.pcap packet # 470000\n",
            "SAT-03-11-2018_010.pcap packet # 471000\n",
            "SAT-03-11-2018_010.pcap packet # 472000\n",
            "SAT-03-11-2018_010.pcap packet # 473000\n",
            "SAT-03-11-2018_010.pcap packet # 474000\n",
            "SAT-03-11-2018_010.pcap packet # 475000\n",
            "SAT-03-11-2018_010.pcap packet # 476000\n",
            "SAT-03-11-2018_010.pcap packet # 477000\n",
            "SAT-03-11-2018_010.pcap packet # 478000\n",
            "SAT-03-11-2018_010.pcap packet # 479000\n",
            "SAT-03-11-2018_010.pcap packet # 480000\n",
            "SAT-03-11-2018_010.pcap packet # 481000\n",
            "SAT-03-11-2018_010.pcap packet # 482000\n",
            "SAT-03-11-2018_010.pcap packet # 483000\n",
            "SAT-03-11-2018_010.pcap packet # 484000\n",
            "SAT-03-11-2018_010.pcap packet # 485000\n",
            "SAT-03-11-2018_010.pcap packet # 486000\n",
            "SAT-03-11-2018_010.pcap packet # 487000\n",
            "SAT-03-11-2018_010.pcap packet # 488000\n",
            "SAT-03-11-2018_010.pcap packet # 489000\n",
            "SAT-03-11-2018_010.pcap packet # 490000\n",
            "SAT-03-11-2018_010.pcap packet # 491000\n",
            "SAT-03-11-2018_010.pcap packet # 492000\n",
            "SAT-03-11-2018_010.pcap packet # 493000\n",
            "SAT-03-11-2018_010.pcap packet # 494000\n",
            "SAT-03-11-2018_010.pcap packet # 495000\n",
            "SAT-03-11-2018_010.pcap packet # 496000\n",
            "SAT-03-11-2018_010.pcap packet # 497000\n",
            "SAT-03-11-2018_010.pcap packet # 498000\n",
            "SAT-03-11-2018_010.pcap packet # 499000\n",
            "SAT-03-11-2018_010.pcap packet # 500000\n",
            "SAT-03-11-2018_010.pcap packet # 501000\n",
            "SAT-03-11-2018_010.pcap packet # 502000\n",
            "SAT-03-11-2018_010.pcap packet # 503000\n",
            "SAT-03-11-2018_010.pcap packet # 504000\n",
            "SAT-03-11-2018_010.pcap packet # 505000\n",
            "SAT-03-11-2018_010.pcap packet # 506000\n",
            "SAT-03-11-2018_010.pcap packet # 507000\n",
            "SAT-03-11-2018_010.pcap packet # 508000\n",
            "SAT-03-11-2018_010.pcap packet # 509000\n",
            "SAT-03-11-2018_010.pcap packet # 510000\n",
            "SAT-03-11-2018_010.pcap packet # 511000\n",
            "SAT-03-11-2018_010.pcap packet # 512000\n",
            "SAT-03-11-2018_010.pcap packet # 513000\n",
            "SAT-03-11-2018_010.pcap packet # 514000\n",
            "SAT-03-11-2018_010.pcap packet # 515000\n",
            "SAT-03-11-2018_010.pcap packet # 516000\n",
            "SAT-03-11-2018_010.pcap packet # 517000\n",
            "SAT-03-11-2018_010.pcap packet # 518000\n",
            "SAT-03-11-2018_010.pcap packet # 519000\n",
            "SAT-03-11-2018_010.pcap packet # 520000\n",
            "SAT-03-11-2018_010.pcap packet # 521000\n",
            "SAT-03-11-2018_010.pcap packet # 522000\n",
            "SAT-03-11-2018_010.pcap packet # 523000\n",
            "SAT-03-11-2018_010.pcap packet # 524000\n",
            "SAT-03-11-2018_010.pcap packet # 525000\n",
            "SAT-03-11-2018_010.pcap packet # 526000\n",
            "SAT-03-11-2018_010.pcap packet # 527000\n",
            "SAT-03-11-2018_010.pcap packet # 528000\n",
            "SAT-03-11-2018_010.pcap packet # 529000\n",
            "SAT-03-11-2018_010.pcap packet # 530000\n",
            "SAT-03-11-2018_010.pcap packet # 531000\n",
            "SAT-03-11-2018_010.pcap packet # 532000\n",
            "SAT-03-11-2018_010.pcap packet # 533000\n",
            "SAT-03-11-2018_010.pcap packet # 534000\n",
            "SAT-03-11-2018_010.pcap packet # 535000\n",
            "SAT-03-11-2018_010.pcap packet # 536000\n",
            "SAT-03-11-2018_010.pcap packet # 537000\n",
            "SAT-03-11-2018_010.pcap packet # 538000\n",
            "SAT-03-11-2018_010.pcap packet # 539000\n",
            "SAT-03-11-2018_010.pcap packet # 540000\n",
            "SAT-03-11-2018_010.pcap packet # 541000\n",
            "SAT-03-11-2018_010.pcap packet # 542000\n",
            "SAT-03-11-2018_010.pcap packet # 543000\n",
            "SAT-03-11-2018_010.pcap packet # 544000\n",
            "SAT-03-11-2018_010.pcap packet # 545000\n",
            "SAT-03-11-2018_010.pcap packet # 546000\n",
            "SAT-03-11-2018_010.pcap packet # 547000\n",
            "SAT-03-11-2018_010.pcap packet # 548000\n",
            "SAT-03-11-2018_010.pcap packet # 549000\n",
            "SAT-03-11-2018_010.pcap packet # 550000\n",
            "SAT-03-11-2018_010.pcap packet # 551000\n",
            "SAT-03-11-2018_010.pcap packet # 552000\n",
            "SAT-03-11-2018_010.pcap packet # 553000\n",
            "SAT-03-11-2018_010.pcap packet # 554000\n",
            "SAT-03-11-2018_010.pcap packet # 555000\n",
            "SAT-03-11-2018_010.pcap packet # 556000\n",
            "SAT-03-11-2018_010.pcap packet # 557000\n",
            "SAT-03-11-2018_010.pcap packet # 558000\n",
            "SAT-03-11-2018_010.pcap packet # 559000\n",
            "SAT-03-11-2018_010.pcap packet # 560000\n",
            "SAT-03-11-2018_010.pcap packet # 561000\n",
            "SAT-03-11-2018_010.pcap packet # 562000\n",
            "SAT-03-11-2018_010.pcap packet # 563000\n",
            "SAT-03-11-2018_010.pcap packet # 564000\n",
            "SAT-03-11-2018_010.pcap packet # 565000\n",
            "SAT-03-11-2018_010.pcap packet # 566000\n",
            "SAT-03-11-2018_010.pcap packet # 567000\n",
            "SAT-03-11-2018_010.pcap packet # 568000\n",
            "SAT-03-11-2018_010.pcap packet # 569000\n",
            "SAT-03-11-2018_010.pcap packet # 570000\n",
            "SAT-03-11-2018_010.pcap packet # 571000\n",
            "SAT-03-11-2018_010.pcap packet # 572000\n",
            "SAT-03-11-2018_010.pcap packet # 573000\n",
            "SAT-03-11-2018_010.pcap packet # 574000\n",
            "SAT-03-11-2018_010.pcap packet # 575000\n",
            "SAT-03-11-2018_010.pcap packet # 576000\n",
            "SAT-03-11-2018_010.pcap packet # 577000\n",
            "SAT-03-11-2018_010.pcap packet # 578000\n",
            "SAT-03-11-2018_010.pcap packet # 579000\n",
            "SAT-03-11-2018_010.pcap packet # 580000\n",
            "SAT-03-11-2018_010.pcap packet # 581000\n",
            "SAT-03-11-2018_010.pcap packet # 582000\n",
            "SAT-03-11-2018_010.pcap packet # 583000\n",
            "SAT-03-11-2018_010.pcap packet # 584000\n",
            "SAT-03-11-2018_010.pcap packet # 585000\n",
            "SAT-03-11-2018_010.pcap packet # 586000\n",
            "SAT-03-11-2018_010.pcap packet # 587000\n",
            "SAT-03-11-2018_010.pcap packet # 588000\n",
            "SAT-03-11-2018_010.pcap packet # 589000\n",
            "SAT-03-11-2018_010.pcap packet # 590000\n",
            "SAT-03-11-2018_010.pcap packet # 591000\n",
            "SAT-03-11-2018_010.pcap packet # 592000\n",
            "SAT-03-11-2018_010.pcap packet # 593000\n",
            "SAT-03-11-2018_010.pcap packet # 594000\n",
            "SAT-03-11-2018_010.pcap packet # 595000\n",
            "SAT-03-11-2018_010.pcap packet # 596000\n",
            "SAT-03-11-2018_010.pcap packet # 597000\n",
            "SAT-03-11-2018_010.pcap packet # 598000\n",
            "SAT-03-11-2018_010.pcap packet # 599000\n",
            "SAT-03-11-2018_010.pcap packet # 600000\n",
            "SAT-03-11-2018_010.pcap packet # 601000\n",
            "SAT-03-11-2018_010.pcap packet # 602000\n",
            "SAT-03-11-2018_010.pcap packet # 603000\n",
            "SAT-03-11-2018_010.pcap packet # 604000\n",
            "SAT-03-11-2018_010.pcap packet # 605000\n",
            "SAT-03-11-2018_010.pcap packet # 606000\n",
            "SAT-03-11-2018_010.pcap packet # 607000\n",
            "SAT-03-11-2018_010.pcap packet # 608000\n",
            "SAT-03-11-2018_010.pcap packet # 609000\n",
            "SAT-03-11-2018_010.pcap packet # 610000\n",
            "SAT-03-11-2018_010.pcap packet # 611000\n",
            "SAT-03-11-2018_010.pcap packet # 612000\n",
            "SAT-03-11-2018_010.pcap packet # 613000\n",
            "SAT-03-11-2018_010.pcap packet # 614000\n",
            "SAT-03-11-2018_010.pcap packet # 615000\n",
            "SAT-03-11-2018_010.pcap packet # 616000\n",
            "SAT-03-11-2018_010.pcap packet # 617000\n",
            "SAT-03-11-2018_010.pcap packet # 618000\n",
            "SAT-03-11-2018_010.pcap packet # 619000\n",
            "SAT-03-11-2018_010.pcap packet # 620000\n",
            "SAT-03-11-2018_010.pcap packet # 621000\n",
            "SAT-03-11-2018_010.pcap packet # 622000\n",
            "SAT-03-11-2018_010.pcap packet # 623000\n",
            "SAT-03-11-2018_010.pcap packet # 624000\n",
            "SAT-03-11-2018_010.pcap packet # 625000\n",
            "SAT-03-11-2018_010.pcap packet # 626000\n",
            "SAT-03-11-2018_010.pcap packet # 627000\n",
            "SAT-03-11-2018_010.pcap packet # 628000\n",
            "SAT-03-11-2018_010.pcap packet # 629000\n",
            "SAT-03-11-2018_010.pcap packet # 630000\n",
            "SAT-03-11-2018_010.pcap packet # 631000\n",
            "SAT-03-11-2018_010.pcap packet # 632000\n",
            "SAT-03-11-2018_010.pcap packet # 633000\n",
            "SAT-03-11-2018_010.pcap packet # 634000\n",
            "SAT-03-11-2018_010.pcap packet # 635000\n",
            "SAT-03-11-2018_010.pcap packet # 636000\n",
            "SAT-03-11-2018_010.pcap packet # 637000\n",
            "SAT-03-11-2018_010.pcap packet # 638000\n",
            "SAT-03-11-2018_010.pcap packet # 639000\n",
            "SAT-03-11-2018_010.pcap packet # 640000\n",
            "SAT-03-11-2018_010.pcap packet # 641000\n",
            "SAT-03-11-2018_010.pcap packet # 642000\n",
            "SAT-03-11-2018_010.pcap packet # 643000\n",
            "SAT-03-11-2018_010.pcap packet # 644000\n",
            "SAT-03-11-2018_010.pcap packet # 645000\n",
            "SAT-03-11-2018_010.pcap packet # 646000\n",
            "SAT-03-11-2018_010.pcap packet # 647000\n",
            "SAT-03-11-2018_010.pcap packet # 648000\n",
            "SAT-03-11-2018_010.pcap packet # 649000\n",
            "SAT-03-11-2018_010.pcap packet # 650000\n",
            "SAT-03-11-2018_010.pcap packet # 651000\n",
            "SAT-03-11-2018_010.pcap packet # 652000\n",
            "SAT-03-11-2018_010.pcap packet # 653000\n",
            "SAT-03-11-2018_010.pcap packet # 654000\n",
            "SAT-03-11-2018_010.pcap packet # 655000\n",
            "SAT-03-11-2018_010.pcap packet # 656000\n",
            "SAT-03-11-2018_010.pcap packet # 657000\n",
            "SAT-03-11-2018_010.pcap packet # 658000\n",
            "SAT-03-11-2018_010.pcap packet # 659000\n",
            "SAT-03-11-2018_010.pcap packet # 660000\n",
            "SAT-03-11-2018_010.pcap packet # 661000\n",
            "SAT-03-11-2018_010.pcap packet # 662000\n",
            "SAT-03-11-2018_010.pcap packet # 663000\n",
            "SAT-03-11-2018_010.pcap packet # 664000\n",
            "SAT-03-11-2018_010.pcap packet # 665000\n",
            "SAT-03-11-2018_010.pcap packet # 666000\n",
            "SAT-03-11-2018_010.pcap packet # 667000\n",
            "SAT-03-11-2018_010.pcap packet # 668000\n",
            "SAT-03-11-2018_010.pcap packet # 669000\n",
            "SAT-03-11-2018_010.pcap packet # 670000\n",
            "SAT-03-11-2018_010.pcap packet # 671000\n",
            "SAT-03-11-2018_010.pcap packet # 672000\n",
            "SAT-03-11-2018_010.pcap packet # 673000\n",
            "SAT-03-11-2018_010.pcap packet # 674000\n",
            "SAT-03-11-2018_010.pcap packet # 675000\n",
            "SAT-03-11-2018_010.pcap packet # 676000\n",
            "SAT-03-11-2018_010.pcap packet # 677000\n",
            "SAT-03-11-2018_010.pcap packet # 678000\n",
            "SAT-03-11-2018_010.pcap packet # 679000\n",
            "SAT-03-11-2018_010.pcap packet # 680000\n",
            "SAT-03-11-2018_010.pcap packet # 681000\n",
            "SAT-03-11-2018_010.pcap packet # 682000\n",
            "SAT-03-11-2018_010.pcap packet # 683000\n",
            "SAT-03-11-2018_010.pcap packet # 684000\n",
            "SAT-03-11-2018_010.pcap packet # 685000\n",
            "SAT-03-11-2018_010.pcap packet # 686000\n",
            "SAT-03-11-2018_010.pcap packet # 687000\n",
            "SAT-03-11-2018_010.pcap packet # 688000\n",
            "SAT-03-11-2018_010.pcap packet # 689000\n",
            "SAT-03-11-2018_010.pcap packet # 690000\n",
            "SAT-03-11-2018_010.pcap packet # 691000\n",
            "SAT-03-11-2018_010.pcap packet # 692000\n",
            "SAT-03-11-2018_010.pcap packet # 693000\n",
            "SAT-03-11-2018_010.pcap packet # 694000\n",
            "SAT-03-11-2018_010.pcap packet # 695000\n",
            "SAT-03-11-2018_010.pcap packet # 696000\n",
            "SAT-03-11-2018_010.pcap packet # 697000\n",
            "Completed file SAT-03-11-2018_010.pcap in 2110.616047143936 seconds.\n",
            "2022-06-14 07:19:09 | dataset_type:DOS2019 | flows (tot,ben,ddos):(389309,8797,380512) | fragments (tot,ben,ddos):(402561,20188,382373) | options:--dataset_type DOS2019 --dataset_folder ./CIC2019_PCAP/ --packets_per_flow 10 --dataset_id DOS2019 --traffic_type all --time_window 10 | process_time:2142.141020298004 |\n",
            "\n",
            "2022-06-14 07:19:24 | examples (tot,ben,ddos):(40376,20188,20188) | Train/Val/Test sizes: (32694,3644,4038) | Packets (train,val,test):(122385,13352,15791) | options:--preprocess_folder ./CIC2019_PCAP/ |\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Preprocess with new datasets\n",
        "!python lucid_dataset_parser.py --dataset_type DOS2019 --dataset_folder ./CIC2019_PCAP/ --packets_per_flow 10 --dataset_id DOS2019 --traffic_type all --time_window 10\n",
        "!python lucid_dataset_parser.py --preprocess_folder ./CIC2019_PCAP/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aZZauk3SQL8_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7bd62688-c9e5-49fc-8eb7-9cfe23c9cd72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.4827 - accuracy: 0.8325 - val_loss: 0.4636 - val_accuracy: 0.9580 - 93ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.4844 - accuracy: 0.8283 - val_loss: 0.4606 - val_accuracy: 0.9580 - 93ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.4795 - accuracy: 0.8318 - val_loss: 0.4575 - val_accuracy: 0.9580 - 112ms/epoch - 7ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.4787 - accuracy: 0.8310 - val_loss: 0.4546 - val_accuracy: 0.9635 - 105ms/epoch - 7ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.4763 - accuracy: 0.8320 - val_loss: 0.4516 - val_accuracy: 0.9635 - 98ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.4749 - accuracy: 0.8308 - val_loss: 0.4487 - val_accuracy: 0.9649 - 99ms/epoch - 6ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.4701 - accuracy: 0.8346 - val_loss: 0.4458 - val_accuracy: 0.9649 - 94ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.4712 - accuracy: 0.8312 - val_loss: 0.4430 - val_accuracy: 0.9649 - 98ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.4708 - accuracy: 0.8298 - val_loss: 0.4403 - val_accuracy: 0.9690 - 101ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.4692 - accuracy: 0.8309 - val_loss: 0.4376 - val_accuracy: 0.9690 - 104ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.4686 - accuracy: 0.8310 - val_loss: 0.4350 - val_accuracy: 0.9690 - 114ms/epoch - 7ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.4684 - accuracy: 0.8286 - val_loss: 0.4325 - val_accuracy: 0.9690 - 93ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.4659 - accuracy: 0.8297 - val_loss: 0.4300 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.4637 - accuracy: 0.8306 - val_loss: 0.4277 - val_accuracy: 0.9693 - 92ms/epoch - 6ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.4640 - accuracy: 0.8302 - val_loss: 0.4252 - val_accuracy: 0.9693 - 106ms/epoch - 7ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.4625 - accuracy: 0.8298 - val_loss: 0.4229 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.4642 - accuracy: 0.8278 - val_loss: 0.4207 - val_accuracy: 0.9693 - 96ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.4603 - accuracy: 0.8306 - val_loss: 0.4186 - val_accuracy: 0.9693 - 104ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.4572 - accuracy: 0.8325 - val_loss: 0.4162 - val_accuracy: 0.9693 - 93ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.4582 - accuracy: 0.8310 - val_loss: 0.4140 - val_accuracy: 0.9693 - 110ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.4566 - accuracy: 0.8319 - val_loss: 0.4118 - val_accuracy: 0.9693 - 101ms/epoch - 6ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.4566 - accuracy: 0.8302 - val_loss: 0.4097 - val_accuracy: 0.9693 - 94ms/epoch - 6ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.4567 - accuracy: 0.8299 - val_loss: 0.4077 - val_accuracy: 0.9693 - 94ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.4516 - accuracy: 0.8328 - val_loss: 0.4057 - val_accuracy: 0.9693 - 100ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.4524 - accuracy: 0.8315 - val_loss: 0.4038 - val_accuracy: 0.9693 - 102ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.4507 - accuracy: 0.8324 - val_loss: 0.4017 - val_accuracy: 0.9693 - 103ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.4509 - accuracy: 0.8311 - val_loss: 0.3998 - val_accuracy: 0.9693 - 105ms/epoch - 7ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.4525 - accuracy: 0.8298 - val_loss: 0.3980 - val_accuracy: 0.9693 - 102ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.4473 - accuracy: 0.8335 - val_loss: 0.3963 - val_accuracy: 0.9693 - 104ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.4477 - accuracy: 0.8324 - val_loss: 0.3944 - val_accuracy: 0.9693 - 95ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.4501 - accuracy: 0.8298 - val_loss: 0.3926 - val_accuracy: 0.9693 - 108ms/epoch - 7ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.4480 - accuracy: 0.8308 - val_loss: 0.3909 - val_accuracy: 0.9693 - 93ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.4468 - accuracy: 0.8323 - val_loss: 0.3892 - val_accuracy: 0.9693 - 90ms/epoch - 6ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.4474 - accuracy: 0.8305 - val_loss: 0.3876 - val_accuracy: 0.9693 - 91ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.4454 - accuracy: 0.8317 - val_loss: 0.3860 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.4433 - accuracy: 0.8332 - val_loss: 0.3843 - val_accuracy: 0.9693 - 91ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 2)           68        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 2)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 2)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 2)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71\n",
            "Trainable params: 71\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.7448 - accuracy: 0.6140 - val_loss: 0.7422 - val_accuracy: 0.8847 - 614ms/epoch - 19ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7163 - accuracy: 0.6692 - val_loss: 0.7173 - val_accuracy: 0.8845 - 145ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6888 - accuracy: 0.7331 - val_loss: 0.6946 - val_accuracy: 0.9281 - 151ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6630 - accuracy: 0.7702 - val_loss: 0.6727 - val_accuracy: 0.9701 - 135ms/epoch - 4ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6377 - accuracy: 0.8164 - val_loss: 0.6512 - val_accuracy: 0.9712 - 135ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.6151 - accuracy: 0.8444 - val_loss: 0.6316 - val_accuracy: 0.9934 - 136ms/epoch - 4ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5944 - accuracy: 0.8473 - val_loss: 0.6177 - val_accuracy: 0.9934 - 139ms/epoch - 4ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.5803 - accuracy: 0.8518 - val_loss: 0.6067 - val_accuracy: 0.9934 - 154ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.5697 - accuracy: 0.8542 - val_loss: 0.5969 - val_accuracy: 0.9934 - 159ms/epoch - 5ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.5597 - accuracy: 0.8540 - val_loss: 0.5872 - val_accuracy: 0.9934 - 161ms/epoch - 5ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.5490 - accuracy: 0.8556 - val_loss: 0.5776 - val_accuracy: 0.9934 - 132ms/epoch - 4ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.5401 - accuracy: 0.8552 - val_loss: 0.5682 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.5299 - accuracy: 0.8580 - val_loss: 0.5588 - val_accuracy: 0.9934 - 145ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.5207 - accuracy: 0.8393 - val_loss: 0.5497 - val_accuracy: 0.9934 - 129ms/epoch - 4ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.5136 - accuracy: 0.8139 - val_loss: 0.5408 - val_accuracy: 0.9934 - 137ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.5070 - accuracy: 0.8124 - val_loss: 0.5322 - val_accuracy: 0.9934 - 139ms/epoch - 4ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.4992 - accuracy: 0.8170 - val_loss: 0.5241 - val_accuracy: 0.9934 - 146ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.4887 - accuracy: 0.8241 - val_loss: 0.5154 - val_accuracy: 0.9934 - 148ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.4834 - accuracy: 0.8221 - val_loss: 0.5080 - val_accuracy: 0.9934 - 151ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.4797 - accuracy: 0.8217 - val_loss: 0.5004 - val_accuracy: 0.9934 - 134ms/epoch - 4ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.4739 - accuracy: 0.8210 - val_loss: 0.4930 - val_accuracy: 0.9934 - 145ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.4652 - accuracy: 0.8309 - val_loss: 0.4854 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.4609 - accuracy: 0.8563 - val_loss: 0.4790 - val_accuracy: 0.9934 - 155ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.4531 - accuracy: 0.8701 - val_loss: 0.4715 - val_accuracy: 0.9934 - 147ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.4516 - accuracy: 0.8747 - val_loss: 0.4652 - val_accuracy: 0.9934 - 137ms/epoch - 4ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.4442 - accuracy: 0.8722 - val_loss: 0.4586 - val_accuracy: 0.9934 - 134ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.4438 - accuracy: 0.8771 - val_loss: 0.4530 - val_accuracy: 0.9934 - 139ms/epoch - 4ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.4388 - accuracy: 0.8915 - val_loss: 0.4470 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.4350 - accuracy: 0.8511 - val_loss: 0.4414 - val_accuracy: 0.9934 - 134ms/epoch - 4ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.4278 - accuracy: 0.8578 - val_loss: 0.4350 - val_accuracy: 0.9934 - 151ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.4277 - accuracy: 0.8970 - val_loss: 0.4296 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.4276 - accuracy: 0.8346 - val_loss: 0.4267 - val_accuracy: 0.9934 - 152ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.4194 - accuracy: 0.8817 - val_loss: 0.4183 - val_accuracy: 0.9934 - 146ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.4156 - accuracy: 0.8965 - val_loss: 0.4113 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.4132 - accuracy: 0.8961 - val_loss: 0.4046 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.4065 - accuracy: 0.8791 - val_loss: 0.3984 - val_accuracy: 0.9934 - 149ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.4037 - accuracy: 0.8578 - val_loss: 0.3924 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.3994 - accuracy: 0.8611 - val_loss: 0.3862 - val_accuracy: 0.9934 - 139ms/epoch - 4ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.3967 - accuracy: 0.8588 - val_loss: 0.3814 - val_accuracy: 0.9934 - 150ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.3944 - accuracy: 0.8595 - val_loss: 0.3769 - val_accuracy: 0.9934 - 150ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.3922 - accuracy: 0.8599 - val_loss: 0.3730 - val_accuracy: 0.9934 - 144ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.3908 - accuracy: 0.8589 - val_loss: 0.3685 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.3888 - accuracy: 0.8587 - val_loss: 0.3650 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.3872 - accuracy: 0.8733 - val_loss: 0.3616 - val_accuracy: 0.9934 - 151ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.3875 - accuracy: 0.8719 - val_loss: 0.3588 - val_accuracy: 0.9934 - 146ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.3853 - accuracy: 0.8974 - val_loss: 0.3553 - val_accuracy: 0.9934 - 138ms/epoch - 4ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.3828 - accuracy: 0.8823 - val_loss: 0.3521 - val_accuracy: 0.9934 - 135ms/epoch - 4ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.3814 - accuracy: 0.8987 - val_loss: 0.3485 - val_accuracy: 0.9934 - 150ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.3799 - accuracy: 0.8981 - val_loss: 0.3458 - val_accuracy: 0.9934 - 129ms/epoch - 4ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.3795 - accuracy: 0.8973 - val_loss: 0.3426 - val_accuracy: 0.9934 - 149ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.3775 - accuracy: 0.8974 - val_loss: 0.3399 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.3756 - accuracy: 0.8967 - val_loss: 0.3371 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.3750 - accuracy: 0.8977 - val_loss: 0.3345 - val_accuracy: 0.9934 - 147ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.3759 - accuracy: 0.8968 - val_loss: 0.3325 - val_accuracy: 0.9934 - 233ms/epoch - 7ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.3765 - accuracy: 0.8974 - val_loss: 0.3301 - val_accuracy: 0.9934 - 207ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.3703 - accuracy: 0.8993 - val_loss: 0.3276 - val_accuracy: 0.9934 - 218ms/epoch - 7ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.3719 - accuracy: 0.8972 - val_loss: 0.3254 - val_accuracy: 0.9934 - 223ms/epoch - 7ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.3727 - accuracy: 0.8975 - val_loss: 0.3243 - val_accuracy: 0.9934 - 232ms/epoch - 7ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.3689 - accuracy: 0.8961 - val_loss: 0.3226 - val_accuracy: 0.9934 - 213ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.3706 - accuracy: 0.8981 - val_loss: 0.3202 - val_accuracy: 0.9934 - 225ms/epoch - 7ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.3695 - accuracy: 0.8962 - val_loss: 0.3194 - val_accuracy: 0.9934 - 222ms/epoch - 7ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.3686 - accuracy: 0.8974 - val_loss: 0.3170 - val_accuracy: 0.9934 - 219ms/epoch - 7ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.3736 - accuracy: 0.8948 - val_loss: 0.3165 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.3743 - accuracy: 0.8934 - val_loss: 0.3155 - val_accuracy: 0.9934 - 146ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.3671 - accuracy: 0.8972 - val_loss: 0.3142 - val_accuracy: 0.9934 - 155ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.3716 - accuracy: 0.8943 - val_loss: 0.3134 - val_accuracy: 0.9934 - 138ms/epoch - 4ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.3672 - accuracy: 0.8972 - val_loss: 0.3111 - val_accuracy: 0.9934 - 146ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.3684 - accuracy: 0.8968 - val_loss: 0.3099 - val_accuracy: 0.9934 - 138ms/epoch - 4ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.3667 - accuracy: 0.8966 - val_loss: 0.3087 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.3665 - accuracy: 0.8966 - val_loss: 0.3078 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.3586 - accuracy: 0.9005 - val_loss: 0.3054 - val_accuracy: 0.9934 - 132ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.3645 - accuracy: 0.8967 - val_loss: 0.3045 - val_accuracy: 0.9934 - 155ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.3652 - accuracy: 0.8970 - val_loss: 0.3037 - val_accuracy: 0.9934 - 147ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.3633 - accuracy: 0.8981 - val_loss: 0.3029 - val_accuracy: 0.9934 - 155ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.3635 - accuracy: 0.8974 - val_loss: 0.3024 - val_accuracy: 0.9934 - 149ms/epoch - 5ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.3657 - accuracy: 0.8962 - val_loss: 0.3007 - val_accuracy: 0.9934 - 154ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.3616 - accuracy: 0.8964 - val_loss: 0.3002 - val_accuracy: 0.9934 - 131ms/epoch - 4ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.3597 - accuracy: 0.8984 - val_loss: 0.2985 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.3604 - accuracy: 0.8980 - val_loss: 0.2977 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.3614 - accuracy: 0.8962 - val_loss: 0.2969 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.3616 - accuracy: 0.8959 - val_loss: 0.2965 - val_accuracy: 0.9934 - 155ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.3586 - accuracy: 0.8975 - val_loss: 0.2953 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.3558 - accuracy: 0.8992 - val_loss: 0.2940 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.3580 - accuracy: 0.8977 - val_loss: 0.2932 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.3548 - accuracy: 0.8992 - val_loss: 0.2915 - val_accuracy: 0.9934 - 129ms/epoch - 4ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.3580 - accuracy: 0.8957 - val_loss: 0.2913 - val_accuracy: 0.9934 - 136ms/epoch - 4ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.3567 - accuracy: 0.8986 - val_loss: 0.2902 - val_accuracy: 0.9934 - 135ms/epoch - 4ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.3581 - accuracy: 0.8987 - val_loss: 0.2895 - val_accuracy: 0.9934 - 137ms/epoch - 4ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.3556 - accuracy: 0.8979 - val_loss: 0.2883 - val_accuracy: 0.9934 - 148ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.3593 - accuracy: 0.8959 - val_loss: 0.2885 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.3589 - accuracy: 0.8959 - val_loss: 0.2880 - val_accuracy: 0.9934 - 129ms/epoch - 4ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.3562 - accuracy: 0.8970 - val_loss: 0.2878 - val_accuracy: 0.9934 - 131ms/epoch - 4ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.3525 - accuracy: 0.8997 - val_loss: 0.2865 - val_accuracy: 0.9934 - 144ms/epoch - 4ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.3505 - accuracy: 0.9007 - val_loss: 0.2851 - val_accuracy: 0.9934 - 133ms/epoch - 4ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.3581 - accuracy: 0.8968 - val_loss: 0.2849 - val_accuracy: 0.9934 - 137ms/epoch - 4ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.3557 - accuracy: 0.8965 - val_loss: 0.2847 - val_accuracy: 0.9934 - 131ms/epoch - 4ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.3560 - accuracy: 0.8972 - val_loss: 0.2833 - val_accuracy: 0.9934 - 131ms/epoch - 4ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.3578 - accuracy: 0.8955 - val_loss: 0.2839 - val_accuracy: 0.9934 - 142ms/epoch - 4ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.3507 - accuracy: 0.8986 - val_loss: 0.2818 - val_accuracy: 0.9934 - 147ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.3516 - accuracy: 0.8989 - val_loss: 0.2807 - val_accuracy: 0.9934 - 136ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 2)           68        \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 2)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 2)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 2)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 2)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 3         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 71\n",
            "Trainable params: 71\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.7943 - accuracy: 0.3879 - val_loss: 0.7815 - val_accuracy: 0.5022 - 587ms/epoch - 37ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.7795 - accuracy: 0.4312 - val_loss: 0.7709 - val_accuracy: 0.5563 - 91ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7667 - accuracy: 0.4683 - val_loss: 0.7600 - val_accuracy: 0.2884 - 109ms/epoch - 7ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7549 - accuracy: 0.5215 - val_loss: 0.7500 - val_accuracy: 0.3971 - 91ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.7442 - accuracy: 0.5598 - val_loss: 0.7398 - val_accuracy: 0.4188 - 101ms/epoch - 6ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.7346 - accuracy: 0.5362 - val_loss: 0.7302 - val_accuracy: 0.7538 - 96ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.7264 - accuracy: 0.5278 - val_loss: 0.7223 - val_accuracy: 0.7541 - 99ms/epoch - 6ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.7190 - accuracy: 0.5472 - val_loss: 0.7147 - val_accuracy: 0.7541 - 87ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.7120 - accuracy: 0.5559 - val_loss: 0.7071 - val_accuracy: 0.7344 - 110ms/epoch - 7ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.7047 - accuracy: 0.5818 - val_loss: 0.6985 - val_accuracy: 0.7086 - 100ms/epoch - 6ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.6982 - accuracy: 0.6186 - val_loss: 0.6914 - val_accuracy: 0.7226 - 96ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.6929 - accuracy: 0.6163 - val_loss: 0.6854 - val_accuracy: 0.7226 - 92ms/epoch - 6ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.6870 - accuracy: 0.6197 - val_loss: 0.6803 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.6830 - accuracy: 0.6175 - val_loss: 0.6759 - val_accuracy: 0.8052 - 96ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.6799 - accuracy: 0.6167 - val_loss: 0.6722 - val_accuracy: 0.8052 - 102ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.6776 - accuracy: 0.6196 - val_loss: 0.6703 - val_accuracy: 0.8052 - 96ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.6763 - accuracy: 0.6187 - val_loss: 0.6687 - val_accuracy: 0.8052 - 98ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.6756 - accuracy: 0.6169 - val_loss: 0.6672 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.6743 - accuracy: 0.6161 - val_loss: 0.6656 - val_accuracy: 0.8052 - 105ms/epoch - 7ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.6731 - accuracy: 0.6199 - val_loss: 0.6641 - val_accuracy: 0.8052 - 95ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.6716 - accuracy: 0.6449 - val_loss: 0.6627 - val_accuracy: 0.8052 - 101ms/epoch - 6ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.6706 - accuracy: 0.6439 - val_loss: 0.6614 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.6705 - accuracy: 0.6409 - val_loss: 0.6601 - val_accuracy: 0.8052 - 112ms/epoch - 7ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.6685 - accuracy: 0.6443 - val_loss: 0.6589 - val_accuracy: 0.8052 - 102ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.6674 - accuracy: 0.6437 - val_loss: 0.6578 - val_accuracy: 0.8052 - 102ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.6665 - accuracy: 0.6443 - val_loss: 0.6566 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.6654 - accuracy: 0.6435 - val_loss: 0.6556 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.6642 - accuracy: 0.6445 - val_loss: 0.6545 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.6639 - accuracy: 0.6431 - val_loss: 0.6532 - val_accuracy: 0.8052 - 108ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.6628 - accuracy: 0.6433 - val_loss: 0.6514 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.6602 - accuracy: 0.6481 - val_loss: 0.6497 - val_accuracy: 0.8052 - 99ms/epoch - 6ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.6599 - accuracy: 0.6469 - val_loss: 0.6482 - val_accuracy: 0.8052 - 96ms/epoch - 6ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.6606 - accuracy: 0.6421 - val_loss: 0.6470 - val_accuracy: 0.8052 - 106ms/epoch - 7ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.6592 - accuracy: 0.6450 - val_loss: 0.6453 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.6591 - accuracy: 0.6432 - val_loss: 0.6439 - val_accuracy: 0.8052 - 100ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.6562 - accuracy: 0.6484 - val_loss: 0.6426 - val_accuracy: 0.8052 - 99ms/epoch - 6ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.6572 - accuracy: 0.6435 - val_loss: 0.6412 - val_accuracy: 0.8052 - 105ms/epoch - 7ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.6567 - accuracy: 0.6557 - val_loss: 0.6402 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.6559 - accuracy: 0.6604 - val_loss: 0.6390 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.6560 - accuracy: 0.6597 - val_loss: 0.6380 - val_accuracy: 0.8052 - 90ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.6547 - accuracy: 0.6610 - val_loss: 0.6368 - val_accuracy: 0.8052 - 99ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.6555 - accuracy: 0.6587 - val_loss: 0.6359 - val_accuracy: 0.8052 - 95ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.6545 - accuracy: 0.6601 - val_loss: 0.6350 - val_accuracy: 0.8052 - 103ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.6533 - accuracy: 0.6614 - val_loss: 0.6341 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.6541 - accuracy: 0.6594 - val_loss: 0.6334 - val_accuracy: 0.8052 - 98ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.6530 - accuracy: 0.6609 - val_loss: 0.6326 - val_accuracy: 0.8052 - 99ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.6540 - accuracy: 0.6587 - val_loss: 0.6318 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.6515 - accuracy: 0.6630 - val_loss: 0.6311 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.6516 - accuracy: 0.6620 - val_loss: 0.6303 - val_accuracy: 0.8052 - 90ms/epoch - 6ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.6527 - accuracy: 0.6592 - val_loss: 0.6297 - val_accuracy: 0.8052 - 94ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.6538 - accuracy: 0.6571 - val_loss: 0.6293 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.6497 - accuracy: 0.6636 - val_loss: 0.6286 - val_accuracy: 0.8052 - 105ms/epoch - 7ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.6493 - accuracy: 0.6639 - val_loss: 0.6278 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.6534 - accuracy: 0.6564 - val_loss: 0.6274 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.6479 - accuracy: 0.6655 - val_loss: 0.6268 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.6486 - accuracy: 0.6633 - val_loss: 0.6261 - val_accuracy: 0.8052 - 125ms/epoch - 8ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.6499 - accuracy: 0.6616 - val_loss: 0.6257 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.6480 - accuracy: 0.6634 - val_loss: 0.6252 - val_accuracy: 0.8052 - 100ms/epoch - 6ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.6502 - accuracy: 0.6597 - val_loss: 0.6248 - val_accuracy: 0.8052 - 97ms/epoch - 6ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.6479 - accuracy: 0.6631 - val_loss: 0.6242 - val_accuracy: 0.8052 - 108ms/epoch - 7ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.6476 - accuracy: 0.6628 - val_loss: 0.6239 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.6482 - accuracy: 0.6617 - val_loss: 0.6232 - val_accuracy: 0.8052 - 106ms/epoch - 7ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.6498 - accuracy: 0.6589 - val_loss: 0.6233 - val_accuracy: 0.8052 - 98ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.6478 - accuracy: 0.6617 - val_loss: 0.6230 - val_accuracy: 0.8052 - 101ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.6496 - accuracy: 0.6589 - val_loss: 0.6227 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.6475 - accuracy: 0.6617 - val_loss: 0.6222 - val_accuracy: 0.8052 - 103ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.6501 - accuracy: 0.6569 - val_loss: 0.6221 - val_accuracy: 0.8052 - 90ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.6480 - accuracy: 0.6600 - val_loss: 0.6221 - val_accuracy: 0.8052 - 100ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.6484 - accuracy: 0.6588 - val_loss: 0.6217 - val_accuracy: 0.8052 - 99ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.6460 - accuracy: 0.6627 - val_loss: 0.6211 - val_accuracy: 0.8052 - 109ms/epoch - 7ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.6470 - accuracy: 0.6611 - val_loss: 0.6205 - val_accuracy: 0.8052 - 96ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.6471 - accuracy: 0.6604 - val_loss: 0.6198 - val_accuracy: 0.8052 - 100ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.6482 - accuracy: 0.6582 - val_loss: 0.6195 - val_accuracy: 0.8052 - 90ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.6458 - accuracy: 0.6625 - val_loss: 0.6189 - val_accuracy: 0.8052 - 109ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.6468 - accuracy: 0.6604 - val_loss: 0.6186 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.6474 - accuracy: 0.6596 - val_loss: 0.6185 - val_accuracy: 0.8052 - 101ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.6467 - accuracy: 0.6606 - val_loss: 0.6180 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.6475 - accuracy: 0.6587 - val_loss: 0.6176 - val_accuracy: 0.8052 - 95ms/epoch - 6ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.6460 - accuracy: 0.6610 - val_loss: 0.6171 - val_accuracy: 0.8052 - 96ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.6472 - accuracy: 0.6590 - val_loss: 0.6170 - val_accuracy: 0.8052 - 97ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.6458 - accuracy: 0.6610 - val_loss: 0.6165 - val_accuracy: 0.8052 - 93ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.6465 - accuracy: 0.6601 - val_loss: 0.6165 - val_accuracy: 0.8052 - 103ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.6452 - accuracy: 0.6624 - val_loss: 0.6164 - val_accuracy: 0.8052 - 106ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.6457 - accuracy: 0.6614 - val_loss: 0.6160 - val_accuracy: 0.8052 - 105ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.6464 - accuracy: 0.6599 - val_loss: 0.6160 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.6448 - accuracy: 0.6623 - val_loss: 0.6153 - val_accuracy: 0.8052 - 122ms/epoch - 8ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.6441 - accuracy: 0.6633 - val_loss: 0.6152 - val_accuracy: 0.8052 - 90ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.6460 - accuracy: 0.6607 - val_loss: 0.6151 - val_accuracy: 0.8052 - 129ms/epoch - 8ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.6455 - accuracy: 0.6613 - val_loss: 0.6148 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.6463 - accuracy: 0.6602 - val_loss: 0.6150 - val_accuracy: 0.8052 - 104ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.6463 - accuracy: 0.6602 - val_loss: 0.6148 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.6454 - accuracy: 0.6611 - val_loss: 0.6147 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.6455 - accuracy: 0.6606 - val_loss: 0.6147 - val_accuracy: 0.8052 - 89ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.6452 - accuracy: 0.6608 - val_loss: 0.6143 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.6430 - accuracy: 0.6643 - val_loss: 0.6140 - val_accuracy: 0.8052 - 100ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.6444 - accuracy: 0.6619 - val_loss: 0.6140 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.6483 - accuracy: 0.6563 - val_loss: 0.6141 - val_accuracy: 0.8052 - 115ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.6471 - accuracy: 0.6581 - val_loss: 0.6139 - val_accuracy: 0.8052 - 92ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.6440 - accuracy: 0.6618 - val_loss: 0.6139 - val_accuracy: 0.8052 - 104ms/epoch - 7ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.6472 - accuracy: 0.6579 - val_loss: 0.6141 - val_accuracy: 0.8052 - 91ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 4)           136       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 4)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 145\n",
            "Trainable params: 145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8396 - accuracy: 0.2645 - val_loss: 0.8050 - val_accuracy: 0.2289 - 829ms/epoch - 26ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7811 - accuracy: 0.3929 - val_loss: 0.7622 - val_accuracy: 0.4125 - 133ms/epoch - 4ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.7398 - accuracy: 0.4896 - val_loss: 0.7286 - val_accuracy: 0.4580 - 142ms/epoch - 4ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.7077 - accuracy: 0.7686 - val_loss: 0.7014 - val_accuracy: 0.7939 - 145ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6812 - accuracy: 0.7564 - val_loss: 0.6800 - val_accuracy: 0.9273 - 139ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.6601 - accuracy: 0.8186 - val_loss: 0.6631 - val_accuracy: 0.9407 - 144ms/epoch - 4ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.6429 - accuracy: 0.8409 - val_loss: 0.6485 - val_accuracy: 0.9701 - 141ms/epoch - 4ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.6270 - accuracy: 0.8461 - val_loss: 0.6345 - val_accuracy: 0.9701 - 133ms/epoch - 4ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.6123 - accuracy: 0.8466 - val_loss: 0.6207 - val_accuracy: 0.9701 - 145ms/epoch - 5ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.5965 - accuracy: 0.8494 - val_loss: 0.6063 - val_accuracy: 0.9701 - 142ms/epoch - 4ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.5809 - accuracy: 0.8520 - val_loss: 0.5917 - val_accuracy: 0.9701 - 145ms/epoch - 5ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.5672 - accuracy: 0.8520 - val_loss: 0.5774 - val_accuracy: 0.9701 - 143ms/epoch - 4ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.5534 - accuracy: 0.8534 - val_loss: 0.5632 - val_accuracy: 0.9923 - 145ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.5375 - accuracy: 0.8596 - val_loss: 0.5489 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.5263 - accuracy: 0.8567 - val_loss: 0.5355 - val_accuracy: 0.9923 - 137ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.5142 - accuracy: 0.8609 - val_loss: 0.5223 - val_accuracy: 0.9923 - 142ms/epoch - 4ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.5008 - accuracy: 0.8628 - val_loss: 0.5096 - val_accuracy: 0.9923 - 149ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.4887 - accuracy: 0.8658 - val_loss: 0.4974 - val_accuracy: 0.9923 - 133ms/epoch - 4ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.4801 - accuracy: 0.8679 - val_loss: 0.4846 - val_accuracy: 0.9923 - 154ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.4680 - accuracy: 0.8758 - val_loss: 0.4719 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.4597 - accuracy: 0.8820 - val_loss: 0.4592 - val_accuracy: 0.9923 - 137ms/epoch - 4ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.4497 - accuracy: 0.8859 - val_loss: 0.4470 - val_accuracy: 0.9923 - 131ms/epoch - 4ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.4413 - accuracy: 0.8880 - val_loss: 0.4355 - val_accuracy: 0.9923 - 148ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.4381 - accuracy: 0.8880 - val_loss: 0.4248 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.4283 - accuracy: 0.8959 - val_loss: 0.4143 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.4203 - accuracy: 0.8982 - val_loss: 0.4043 - val_accuracy: 0.9923 - 148ms/epoch - 5ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.4128 - accuracy: 0.9047 - val_loss: 0.3949 - val_accuracy: 0.9923 - 165ms/epoch - 5ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.4102 - accuracy: 0.9042 - val_loss: 0.3862 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.4063 - accuracy: 0.9032 - val_loss: 0.3780 - val_accuracy: 0.9923 - 149ms/epoch - 5ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.4021 - accuracy: 0.9043 - val_loss: 0.3699 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.3952 - accuracy: 0.9073 - val_loss: 0.3620 - val_accuracy: 0.9923 - 133ms/epoch - 4ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.3921 - accuracy: 0.9105 - val_loss: 0.3551 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.3862 - accuracy: 0.9124 - val_loss: 0.3491 - val_accuracy: 0.9923 - 147ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.3839 - accuracy: 0.9167 - val_loss: 0.3430 - val_accuracy: 0.9923 - 139ms/epoch - 4ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.3814 - accuracy: 0.9150 - val_loss: 0.3368 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.3768 - accuracy: 0.9186 - val_loss: 0.3312 - val_accuracy: 0.9923 - 152ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.3728 - accuracy: 0.9196 - val_loss: 0.3256 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.3692 - accuracy: 0.9226 - val_loss: 0.3206 - val_accuracy: 0.9923 - 133ms/epoch - 4ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.3654 - accuracy: 0.9246 - val_loss: 0.3154 - val_accuracy: 0.9923 - 137ms/epoch - 4ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.3634 - accuracy: 0.9252 - val_loss: 0.3110 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.3630 - accuracy: 0.9233 - val_loss: 0.3068 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.3609 - accuracy: 0.9256 - val_loss: 0.3024 - val_accuracy: 0.9923 - 142ms/epoch - 4ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.3573 - accuracy: 0.9244 - val_loss: 0.2992 - val_accuracy: 0.9923 - 140ms/epoch - 4ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.3560 - accuracy: 0.9255 - val_loss: 0.2951 - val_accuracy: 0.9923 - 135ms/epoch - 4ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.3551 - accuracy: 0.9226 - val_loss: 0.2908 - val_accuracy: 0.9923 - 144ms/epoch - 4ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.3530 - accuracy: 0.9256 - val_loss: 0.2869 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.3506 - accuracy: 0.9274 - val_loss: 0.2833 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.3460 - accuracy: 0.9304 - val_loss: 0.2799 - val_accuracy: 0.9923 - 130ms/epoch - 4ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.3488 - accuracy: 0.9294 - val_loss: 0.2774 - val_accuracy: 0.9923 - 155ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.3510 - accuracy: 0.9276 - val_loss: 0.2752 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.3485 - accuracy: 0.9273 - val_loss: 0.2721 - val_accuracy: 0.9923 - 156ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.3449 - accuracy: 0.9293 - val_loss: 0.2694 - val_accuracy: 0.9923 - 135ms/epoch - 4ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.3449 - accuracy: 0.9248 - val_loss: 0.2677 - val_accuracy: 0.9923 - 139ms/epoch - 4ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.3447 - accuracy: 0.9253 - val_loss: 0.2656 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.3411 - accuracy: 0.9274 - val_loss: 0.2630 - val_accuracy: 0.9923 - 144ms/epoch - 4ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.3416 - accuracy: 0.9263 - val_loss: 0.2610 - val_accuracy: 0.9923 - 142ms/epoch - 4ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.3382 - accuracy: 0.9268 - val_loss: 0.2590 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.3392 - accuracy: 0.9272 - val_loss: 0.2573 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.3390 - accuracy: 0.9264 - val_loss: 0.2558 - val_accuracy: 0.9923 - 152ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.3395 - accuracy: 0.9253 - val_loss: 0.2547 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.3375 - accuracy: 0.9240 - val_loss: 0.2530 - val_accuracy: 0.9923 - 132ms/epoch - 4ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.3353 - accuracy: 0.9256 - val_loss: 0.2514 - val_accuracy: 0.9923 - 145ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.3332 - accuracy: 0.9251 - val_loss: 0.2502 - val_accuracy: 0.9923 - 151ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.3350 - accuracy: 0.9247 - val_loss: 0.2488 - val_accuracy: 0.9923 - 148ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.3377 - accuracy: 0.9226 - val_loss: 0.2479 - val_accuracy: 0.9923 - 154ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.3352 - accuracy: 0.9262 - val_loss: 0.2472 - val_accuracy: 0.9923 - 155ms/epoch - 5ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.3282 - accuracy: 0.9262 - val_loss: 0.2440 - val_accuracy: 0.9923 - 148ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.3357 - accuracy: 0.9221 - val_loss: 0.2445 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.3299 - accuracy: 0.9242 - val_loss: 0.2425 - val_accuracy: 0.9923 - 145ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.3324 - accuracy: 0.9240 - val_loss: 0.2414 - val_accuracy: 0.9923 - 140ms/epoch - 4ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.3305 - accuracy: 0.9259 - val_loss: 0.2405 - val_accuracy: 0.9923 - 169ms/epoch - 5ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.3307 - accuracy: 0.9241 - val_loss: 0.2393 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.3318 - accuracy: 0.9231 - val_loss: 0.2383 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.3274 - accuracy: 0.9240 - val_loss: 0.2374 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.3267 - accuracy: 0.9259 - val_loss: 0.2353 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.3278 - accuracy: 0.9242 - val_loss: 0.2344 - val_accuracy: 0.9923 - 154ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.3271 - accuracy: 0.9246 - val_loss: 0.2335 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.3264 - accuracy: 0.9238 - val_loss: 0.2330 - val_accuracy: 0.9923 - 135ms/epoch - 4ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.3240 - accuracy: 0.9251 - val_loss: 0.2322 - val_accuracy: 0.9923 - 136ms/epoch - 4ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.3264 - accuracy: 0.9262 - val_loss: 0.2306 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.3215 - accuracy: 0.9262 - val_loss: 0.2292 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.3239 - accuracy: 0.9256 - val_loss: 0.2277 - val_accuracy: 0.9923 - 158ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.3250 - accuracy: 0.9241 - val_loss: 0.2273 - val_accuracy: 0.9923 - 154ms/epoch - 5ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.3252 - accuracy: 0.9225 - val_loss: 0.2274 - val_accuracy: 0.9923 - 156ms/epoch - 5ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.3225 - accuracy: 0.9253 - val_loss: 0.2257 - val_accuracy: 0.9923 - 149ms/epoch - 5ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.3223 - accuracy: 0.9235 - val_loss: 0.2246 - val_accuracy: 0.9923 - 158ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.3220 - accuracy: 0.9239 - val_loss: 0.2239 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.3193 - accuracy: 0.9271 - val_loss: 0.2222 - val_accuracy: 0.9923 - 150ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.3221 - accuracy: 0.9237 - val_loss: 0.2224 - val_accuracy: 0.9923 - 147ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.3211 - accuracy: 0.9234 - val_loss: 0.2216 - val_accuracy: 0.9923 - 145ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.3230 - accuracy: 0.9227 - val_loss: 0.2202 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.3185 - accuracy: 0.9247 - val_loss: 0.2185 - val_accuracy: 0.9923 - 143ms/epoch - 4ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.3197 - accuracy: 0.9241 - val_loss: 0.2184 - val_accuracy: 0.9923 - 131ms/epoch - 4ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.3211 - accuracy: 0.9228 - val_loss: 0.2184 - val_accuracy: 0.9923 - 139ms/epoch - 4ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.3192 - accuracy: 0.9247 - val_loss: 0.2171 - val_accuracy: 0.9923 - 136ms/epoch - 4ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.3187 - accuracy: 0.9233 - val_loss: 0.2164 - val_accuracy: 0.9923 - 134ms/epoch - 4ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.3161 - accuracy: 0.9266 - val_loss: 0.2150 - val_accuracy: 0.9923 - 155ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.3186 - accuracy: 0.9252 - val_loss: 0.2149 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.3186 - accuracy: 0.9245 - val_loss: 0.2142 - val_accuracy: 0.9923 - 157ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.3160 - accuracy: 0.9237 - val_loss: 0.2133 - val_accuracy: 0.9923 - 143ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 4)           136       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 4)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 145\n",
            "Trainable params: 145\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.8176 - accuracy: 0.4175 - val_loss: 0.8018 - val_accuracy: 0.4218 - 588ms/epoch - 37ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.7882 - accuracy: 0.4256 - val_loss: 0.7755 - val_accuracy: 0.3982 - 91ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7649 - accuracy: 0.4691 - val_loss: 0.7562 - val_accuracy: 0.8419 - 96ms/epoch - 6ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7462 - accuracy: 0.5790 - val_loss: 0.7391 - val_accuracy: 0.8428 - 92ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.7285 - accuracy: 0.6336 - val_loss: 0.7234 - val_accuracy: 0.8460 - 92ms/epoch - 6ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.7125 - accuracy: 0.7723 - val_loss: 0.7089 - val_accuracy: 0.8510 - 91ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6978 - accuracy: 0.7889 - val_loss: 0.6964 - val_accuracy: 0.8518 - 97ms/epoch - 6ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6858 - accuracy: 0.7946 - val_loss: 0.6862 - val_accuracy: 0.8658 - 104ms/epoch - 6ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6760 - accuracy: 0.7893 - val_loss: 0.6770 - val_accuracy: 0.9481 - 90ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.6668 - accuracy: 0.7892 - val_loss: 0.6687 - val_accuracy: 0.9484 - 99ms/epoch - 6ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.6588 - accuracy: 0.7874 - val_loss: 0.6614 - val_accuracy: 0.9089 - 97ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.6520 - accuracy: 0.7885 - val_loss: 0.6551 - val_accuracy: 0.9092 - 89ms/epoch - 6ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.6452 - accuracy: 0.7989 - val_loss: 0.6483 - val_accuracy: 0.9092 - 101ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.6376 - accuracy: 0.8225 - val_loss: 0.6404 - val_accuracy: 0.9429 - 92ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.6290 - accuracy: 0.8299 - val_loss: 0.6322 - val_accuracy: 0.9374 - 109ms/epoch - 7ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.6213 - accuracy: 0.8315 - val_loss: 0.6236 - val_accuracy: 0.9374 - 92ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.6126 - accuracy: 0.8393 - val_loss: 0.6147 - val_accuracy: 0.9374 - 94ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.6037 - accuracy: 0.8428 - val_loss: 0.6049 - val_accuracy: 0.9484 - 92ms/epoch - 6ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.5929 - accuracy: 0.8451 - val_loss: 0.5947 - val_accuracy: 0.9484 - 107ms/epoch - 7ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.5827 - accuracy: 0.8468 - val_loss: 0.5844 - val_accuracy: 0.9484 - 91ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.5716 - accuracy: 0.8511 - val_loss: 0.5744 - val_accuracy: 0.9484 - 99ms/epoch - 6ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.5619 - accuracy: 0.8541 - val_loss: 0.5641 - val_accuracy: 0.9484 - 88ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.5527 - accuracy: 0.8576 - val_loss: 0.5538 - val_accuracy: 0.9484 - 116ms/epoch - 7ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.5429 - accuracy: 0.8567 - val_loss: 0.5437 - val_accuracy: 0.9484 - 100ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.5336 - accuracy: 0.8584 - val_loss: 0.5338 - val_accuracy: 0.9484 - 101ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.5246 - accuracy: 0.8595 - val_loss: 0.5238 - val_accuracy: 0.9487 - 94ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.5171 - accuracy: 0.8581 - val_loss: 0.5143 - val_accuracy: 0.9580 - 106ms/epoch - 7ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.5083 - accuracy: 0.8582 - val_loss: 0.5051 - val_accuracy: 0.9627 - 102ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.5016 - accuracy: 0.8547 - val_loss: 0.4963 - val_accuracy: 0.9630 - 91ms/epoch - 6ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.4941 - accuracy: 0.8566 - val_loss: 0.4880 - val_accuracy: 0.9646 - 99ms/epoch - 6ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.4870 - accuracy: 0.8579 - val_loss: 0.4799 - val_accuracy: 0.9684 - 96ms/epoch - 6ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.4810 - accuracy: 0.8605 - val_loss: 0.4720 - val_accuracy: 0.9684 - 94ms/epoch - 6ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.4750 - accuracy: 0.8625 - val_loss: 0.4644 - val_accuracy: 0.9684 - 92ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.4685 - accuracy: 0.8641 - val_loss: 0.4571 - val_accuracy: 0.9690 - 104ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.4628 - accuracy: 0.8650 - val_loss: 0.4500 - val_accuracy: 0.9690 - 95ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.4567 - accuracy: 0.8652 - val_loss: 0.4432 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.4517 - accuracy: 0.8679 - val_loss: 0.4367 - val_accuracy: 0.9693 - 96ms/epoch - 6ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.4494 - accuracy: 0.8659 - val_loss: 0.4304 - val_accuracy: 0.9693 - 109ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.4419 - accuracy: 0.8699 - val_loss: 0.4243 - val_accuracy: 0.9693 - 102ms/epoch - 6ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.4379 - accuracy: 0.8697 - val_loss: 0.4184 - val_accuracy: 0.9693 - 95ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.4338 - accuracy: 0.8709 - val_loss: 0.4130 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.4292 - accuracy: 0.8708 - val_loss: 0.4074 - val_accuracy: 0.9693 - 93ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.4249 - accuracy: 0.8711 - val_loss: 0.4021 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.4205 - accuracy: 0.8713 - val_loss: 0.3973 - val_accuracy: 0.9693 - 90ms/epoch - 6ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.4196 - accuracy: 0.8715 - val_loss: 0.3928 - val_accuracy: 0.9693 - 106ms/epoch - 7ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.4136 - accuracy: 0.8736 - val_loss: 0.3884 - val_accuracy: 0.9693 - 90ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.4116 - accuracy: 0.8714 - val_loss: 0.3842 - val_accuracy: 0.9698 - 102ms/epoch - 6ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.4067 - accuracy: 0.8730 - val_loss: 0.3801 - val_accuracy: 0.9698 - 94ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.4037 - accuracy: 0.8744 - val_loss: 0.3760 - val_accuracy: 0.9698 - 94ms/epoch - 6ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.4007 - accuracy: 0.8748 - val_loss: 0.3721 - val_accuracy: 0.9698 - 93ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.4005 - accuracy: 0.8755 - val_loss: 0.3687 - val_accuracy: 0.9698 - 94ms/epoch - 6ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.3963 - accuracy: 0.8766 - val_loss: 0.3653 - val_accuracy: 0.9698 - 102ms/epoch - 6ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.3918 - accuracy: 0.8794 - val_loss: 0.3618 - val_accuracy: 0.9698 - 110ms/epoch - 7ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.3915 - accuracy: 0.8775 - val_loss: 0.3585 - val_accuracy: 0.9698 - 97ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.3869 - accuracy: 0.8803 - val_loss: 0.3554 - val_accuracy: 0.9698 - 117ms/epoch - 7ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.3852 - accuracy: 0.8796 - val_loss: 0.3520 - val_accuracy: 0.9698 - 94ms/epoch - 6ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.3830 - accuracy: 0.8806 - val_loss: 0.3492 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.3792 - accuracy: 0.8805 - val_loss: 0.3460 - val_accuracy: 0.9920 - 104ms/epoch - 6ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.3767 - accuracy: 0.8816 - val_loss: 0.3432 - val_accuracy: 0.9920 - 113ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.3770 - accuracy: 0.8810 - val_loss: 0.3404 - val_accuracy: 0.9920 - 95ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.3755 - accuracy: 0.8810 - val_loss: 0.3375 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.3712 - accuracy: 0.8817 - val_loss: 0.3347 - val_accuracy: 0.9920 - 93ms/epoch - 6ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.3713 - accuracy: 0.8808 - val_loss: 0.3318 - val_accuracy: 0.9920 - 98ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.3682 - accuracy: 0.8812 - val_loss: 0.3295 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.3664 - accuracy: 0.8809 - val_loss: 0.3265 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.3648 - accuracy: 0.8844 - val_loss: 0.3241 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.3635 - accuracy: 0.8903 - val_loss: 0.3215 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.3607 - accuracy: 0.8922 - val_loss: 0.3193 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.3613 - accuracy: 0.8936 - val_loss: 0.3169 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.3585 - accuracy: 0.8936 - val_loss: 0.3148 - val_accuracy: 0.9920 - 95ms/epoch - 6ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.3564 - accuracy: 0.8944 - val_loss: 0.3127 - val_accuracy: 0.9920 - 92ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.3571 - accuracy: 0.8944 - val_loss: 0.3106 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.3533 - accuracy: 0.8965 - val_loss: 0.3087 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.3518 - accuracy: 0.8969 - val_loss: 0.3065 - val_accuracy: 0.9920 - 94ms/epoch - 6ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.3514 - accuracy: 0.8963 - val_loss: 0.3046 - val_accuracy: 0.9920 - 97ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.3485 - accuracy: 0.8974 - val_loss: 0.3027 - val_accuracy: 0.9920 - 92ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.3482 - accuracy: 0.8975 - val_loss: 0.3009 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.3471 - accuracy: 0.8980 - val_loss: 0.2990 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.3457 - accuracy: 0.8981 - val_loss: 0.2973 - val_accuracy: 0.9920 - 92ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.3448 - accuracy: 0.8985 - val_loss: 0.2960 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.3432 - accuracy: 0.8990 - val_loss: 0.2948 - val_accuracy: 0.9920 - 93ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.3437 - accuracy: 0.8981 - val_loss: 0.2935 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.3420 - accuracy: 0.8992 - val_loss: 0.2925 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.3396 - accuracy: 0.8983 - val_loss: 0.2907 - val_accuracy: 0.9920 - 110ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.3366 - accuracy: 0.8995 - val_loss: 0.2892 - val_accuracy: 0.9920 - 93ms/epoch - 6ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.3393 - accuracy: 0.8987 - val_loss: 0.2876 - val_accuracy: 0.9920 - 110ms/epoch - 7ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.3356 - accuracy: 0.9011 - val_loss: 0.2858 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.3366 - accuracy: 0.9011 - val_loss: 0.2841 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.3327 - accuracy: 0.9028 - val_loss: 0.2825 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.3335 - accuracy: 0.9022 - val_loss: 0.2811 - val_accuracy: 0.9920 - 98ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.3354 - accuracy: 0.9009 - val_loss: 0.2794 - val_accuracy: 0.9920 - 98ms/epoch - 6ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.3302 - accuracy: 0.9030 - val_loss: 0.2780 - val_accuracy: 0.9920 - 93ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.3297 - accuracy: 0.9028 - val_loss: 0.2766 - val_accuracy: 0.9920 - 98ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.3296 - accuracy: 0.9039 - val_loss: 0.2748 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.3277 - accuracy: 0.9040 - val_loss: 0.2734 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.3263 - accuracy: 0.9027 - val_loss: 0.2723 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.3252 - accuracy: 0.9027 - val_loss: 0.2708 - val_accuracy: 0.9920 - 117ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.3265 - accuracy: 0.9023 - val_loss: 0.2692 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.3235 - accuracy: 0.9026 - val_loss: 0.2680 - val_accuracy: 0.9920 - 101ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.3229 - accuracy: 0.9042 - val_loss: 0.2666 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 4)           136       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 4)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 141\n",
            "Trainable params: 141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8011 - accuracy: 0.5037 - val_loss: 0.7781 - val_accuracy: 0.3839 - 632ms/epoch - 20ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7588 - accuracy: 0.6173 - val_loss: 0.7413 - val_accuracy: 0.7626 - 150ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.7280 - accuracy: 0.5804 - val_loss: 0.7171 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.7061 - accuracy: 0.6769 - val_loss: 0.6989 - val_accuracy: 0.8425 - 132ms/epoch - 4ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6894 - accuracy: 0.7218 - val_loss: 0.6864 - val_accuracy: 0.8425 - 144ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.6793 - accuracy: 0.7716 - val_loss: 0.6796 - val_accuracy: 0.8425 - 142ms/epoch - 4ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.6742 - accuracy: 0.7797 - val_loss: 0.6750 - val_accuracy: 0.8425 - 160ms/epoch - 5ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.6670 - accuracy: 0.7825 - val_loss: 0.6698 - val_accuracy: 0.8425 - 138ms/epoch - 4ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.6572 - accuracy: 0.7455 - val_loss: 0.6651 - val_accuracy: 0.8425 - 140ms/epoch - 4ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.6497 - accuracy: 0.7434 - val_loss: 0.6602 - val_accuracy: 0.8425 - 151ms/epoch - 5ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.6422 - accuracy: 0.7451 - val_loss: 0.6561 - val_accuracy: 0.8425 - 154ms/epoch - 5ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.6350 - accuracy: 0.7369 - val_loss: 0.6510 - val_accuracy: 0.8425 - 137ms/epoch - 4ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.6275 - accuracy: 0.7354 - val_loss: 0.6454 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.6203 - accuracy: 0.7341 - val_loss: 0.6395 - val_accuracy: 0.8425 - 153ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.6139 - accuracy: 0.7311 - val_loss: 0.6349 - val_accuracy: 0.8425 - 141ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.6087 - accuracy: 0.7506 - val_loss: 0.6303 - val_accuracy: 0.8425 - 148ms/epoch - 5ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.6041 - accuracy: 0.7850 - val_loss: 0.6258 - val_accuracy: 0.8425 - 130ms/epoch - 4ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.5986 - accuracy: 0.7931 - val_loss: 0.6211 - val_accuracy: 0.8425 - 155ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.5931 - accuracy: 0.8149 - val_loss: 0.6168 - val_accuracy: 0.8425 - 142ms/epoch - 4ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.5886 - accuracy: 0.8153 - val_loss: 0.6132 - val_accuracy: 0.8425 - 158ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.5878 - accuracy: 0.8129 - val_loss: 0.6107 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.5825 - accuracy: 0.8139 - val_loss: 0.6076 - val_accuracy: 0.8425 - 150ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.5788 - accuracy: 0.8145 - val_loss: 0.6053 - val_accuracy: 0.8425 - 153ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.5763 - accuracy: 0.8146 - val_loss: 0.6033 - val_accuracy: 0.8425 - 153ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.5720 - accuracy: 0.8151 - val_loss: 0.6007 - val_accuracy: 0.8425 - 134ms/epoch - 4ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.5709 - accuracy: 0.8144 - val_loss: 0.5991 - val_accuracy: 0.8425 - 137ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.5667 - accuracy: 0.8156 - val_loss: 0.5966 - val_accuracy: 0.8425 - 131ms/epoch - 4ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.5653 - accuracy: 0.8156 - val_loss: 0.5947 - val_accuracy: 0.8425 - 144ms/epoch - 5ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.5629 - accuracy: 0.8150 - val_loss: 0.5930 - val_accuracy: 0.8425 - 153ms/epoch - 5ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.5640 - accuracy: 0.8132 - val_loss: 0.5914 - val_accuracy: 0.8425 - 160ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.5620 - accuracy: 0.8155 - val_loss: 0.5897 - val_accuracy: 0.8425 - 139ms/epoch - 4ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.5619 - accuracy: 0.8135 - val_loss: 0.5886 - val_accuracy: 0.8425 - 148ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.5568 - accuracy: 0.8156 - val_loss: 0.5868 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.5583 - accuracy: 0.8136 - val_loss: 0.5856 - val_accuracy: 0.8425 - 164ms/epoch - 5ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.5529 - accuracy: 0.8175 - val_loss: 0.5843 - val_accuracy: 0.8425 - 136ms/epoch - 4ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.5536 - accuracy: 0.8159 - val_loss: 0.5834 - val_accuracy: 0.8425 - 146ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.5542 - accuracy: 0.8156 - val_loss: 0.5828 - val_accuracy: 0.8425 - 141ms/epoch - 4ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.5511 - accuracy: 0.8164 - val_loss: 0.5817 - val_accuracy: 0.8425 - 131ms/epoch - 4ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.5513 - accuracy: 0.8154 - val_loss: 0.5814 - val_accuracy: 0.8425 - 154ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.5502 - accuracy: 0.8157 - val_loss: 0.5810 - val_accuracy: 0.8425 - 155ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.5526 - accuracy: 0.8160 - val_loss: 0.5811 - val_accuracy: 0.8425 - 146ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.5519 - accuracy: 0.8138 - val_loss: 0.5808 - val_accuracy: 0.8425 - 153ms/epoch - 5ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.5510 - accuracy: 0.8154 - val_loss: 0.5803 - val_accuracy: 0.8425 - 131ms/epoch - 4ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.5509 - accuracy: 0.8139 - val_loss: 0.5800 - val_accuracy: 0.8425 - 156ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.5488 - accuracy: 0.8158 - val_loss: 0.5798 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.5473 - accuracy: 0.8161 - val_loss: 0.5787 - val_accuracy: 0.8425 - 149ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.5478 - accuracy: 0.8159 - val_loss: 0.5784 - val_accuracy: 0.8425 - 132ms/epoch - 4ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.5451 - accuracy: 0.8172 - val_loss: 0.5778 - val_accuracy: 0.8425 - 138ms/epoch - 4ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.5477 - accuracy: 0.8153 - val_loss: 0.5767 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.5479 - accuracy: 0.8147 - val_loss: 0.5770 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.5492 - accuracy: 0.8127 - val_loss: 0.5768 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.5481 - accuracy: 0.8144 - val_loss: 0.5769 - val_accuracy: 0.8425 - 137ms/epoch - 4ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.5468 - accuracy: 0.8140 - val_loss: 0.5772 - val_accuracy: 0.8425 - 144ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.5495 - accuracy: 0.8145 - val_loss: 0.5768 - val_accuracy: 0.8425 - 133ms/epoch - 4ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.5468 - accuracy: 0.8158 - val_loss: 0.5766 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.5466 - accuracy: 0.8133 - val_loss: 0.5760 - val_accuracy: 0.8425 - 146ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.5461 - accuracy: 0.8150 - val_loss: 0.5759 - val_accuracy: 0.8425 - 142ms/epoch - 4ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.5455 - accuracy: 0.8162 - val_loss: 0.5753 - val_accuracy: 0.8425 - 129ms/epoch - 4ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.5467 - accuracy: 0.8150 - val_loss: 0.5757 - val_accuracy: 0.8425 - 134ms/epoch - 4ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.5470 - accuracy: 0.8131 - val_loss: 0.5752 - val_accuracy: 0.8425 - 129ms/epoch - 4ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.5434 - accuracy: 0.8153 - val_loss: 0.5746 - val_accuracy: 0.8425 - 156ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.5457 - accuracy: 0.8161 - val_loss: 0.5740 - val_accuracy: 0.8425 - 144ms/epoch - 4ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.5456 - accuracy: 0.8150 - val_loss: 0.5746 - val_accuracy: 0.8425 - 136ms/epoch - 4ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.5440 - accuracy: 0.8165 - val_loss: 0.5747 - val_accuracy: 0.8425 - 144ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.5479 - accuracy: 0.8122 - val_loss: 0.5747 - val_accuracy: 0.8425 - 141ms/epoch - 4ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.5448 - accuracy: 0.8149 - val_loss: 0.5743 - val_accuracy: 0.8425 - 133ms/epoch - 4ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.5400 - accuracy: 0.8166 - val_loss: 0.5735 - val_accuracy: 0.8425 - 145ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.5461 - accuracy: 0.8147 - val_loss: 0.5732 - val_accuracy: 0.8425 - 141ms/epoch - 4ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.5448 - accuracy: 0.8153 - val_loss: 0.5733 - val_accuracy: 0.8425 - 140ms/epoch - 4ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.5418 - accuracy: 0.8150 - val_loss: 0.5721 - val_accuracy: 0.8425 - 135ms/epoch - 4ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.5450 - accuracy: 0.8139 - val_loss: 0.5726 - val_accuracy: 0.8425 - 133ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.5407 - accuracy: 0.8145 - val_loss: 0.5716 - val_accuracy: 0.8425 - 157ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.5418 - accuracy: 0.8148 - val_loss: 0.5720 - val_accuracy: 0.8425 - 148ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.5381 - accuracy: 0.8164 - val_loss: 0.5709 - val_accuracy: 0.8425 - 151ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.5449 - accuracy: 0.8139 - val_loss: 0.5714 - val_accuracy: 0.8425 - 144ms/epoch - 5ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.5425 - accuracy: 0.8152 - val_loss: 0.5714 - val_accuracy: 0.8425 - 139ms/epoch - 4ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.5440 - accuracy: 0.8138 - val_loss: 0.5715 - val_accuracy: 0.8425 - 137ms/epoch - 4ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.5442 - accuracy: 0.8134 - val_loss: 0.5720 - val_accuracy: 0.8425 - 144ms/epoch - 4ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.5408 - accuracy: 0.8159 - val_loss: 0.5719 - val_accuracy: 0.8425 - 131ms/epoch - 4ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.5442 - accuracy: 0.8142 - val_loss: 0.5709 - val_accuracy: 0.8425 - 130ms/epoch - 4ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.5397 - accuracy: 0.8150 - val_loss: 0.5717 - val_accuracy: 0.8425 - 134ms/epoch - 4ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.5438 - accuracy: 0.8144 - val_loss: 0.5702 - val_accuracy: 0.8425 - 147ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.5397 - accuracy: 0.8160 - val_loss: 0.5695 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.5424 - accuracy: 0.8136 - val_loss: 0.5702 - val_accuracy: 0.8425 - 144ms/epoch - 5ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.5387 - accuracy: 0.8164 - val_loss: 0.5708 - val_accuracy: 0.8425 - 137ms/epoch - 4ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.5395 - accuracy: 0.8147 - val_loss: 0.5692 - val_accuracy: 0.8425 - 150ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.5389 - accuracy: 0.8151 - val_loss: 0.5698 - val_accuracy: 0.8425 - 143ms/epoch - 4ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.5405 - accuracy: 0.8154 - val_loss: 0.5688 - val_accuracy: 0.8425 - 141ms/epoch - 4ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.5399 - accuracy: 0.8147 - val_loss: 0.5687 - val_accuracy: 0.8425 - 133ms/epoch - 4ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.5392 - accuracy: 0.8151 - val_loss: 0.5696 - val_accuracy: 0.8425 - 150ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.5430 - accuracy: 0.8139 - val_loss: 0.5686 - val_accuracy: 0.8425 - 142ms/epoch - 4ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.5402 - accuracy: 0.8145 - val_loss: 0.5684 - val_accuracy: 0.8425 - 142ms/epoch - 4ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.5400 - accuracy: 0.8136 - val_loss: 0.5687 - val_accuracy: 0.8425 - 134ms/epoch - 4ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.5374 - accuracy: 0.8174 - val_loss: 0.5687 - val_accuracy: 0.8425 - 140ms/epoch - 4ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.5394 - accuracy: 0.8139 - val_loss: 0.5682 - val_accuracy: 0.8425 - 145ms/epoch - 5ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.5372 - accuracy: 0.8157 - val_loss: 0.5685 - val_accuracy: 0.8425 - 145ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.5380 - accuracy: 0.8159 - val_loss: 0.5678 - val_accuracy: 0.8425 - 155ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.5395 - accuracy: 0.8146 - val_loss: 0.5669 - val_accuracy: 0.8425 - 135ms/epoch - 4ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.5405 - accuracy: 0.8141 - val_loss: 0.5670 - val_accuracy: 0.8425 - 149ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.5372 - accuracy: 0.8161 - val_loss: 0.5670 - val_accuracy: 0.8425 - 152ms/epoch - 5ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 4)           136       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 4)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 4)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 4)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 5         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 141\n",
            "Trainable params: 141\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.8061 - accuracy: 0.4684 - val_loss: 0.7949 - val_accuracy: 0.3156 - 586ms/epoch - 37ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.7813 - accuracy: 0.6200 - val_loss: 0.7733 - val_accuracy: 0.7489 - 90ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7609 - accuracy: 0.7467 - val_loss: 0.7557 - val_accuracy: 0.7613 - 95ms/epoch - 6ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7416 - accuracy: 0.7830 - val_loss: 0.7386 - val_accuracy: 0.8455 - 100ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.7235 - accuracy: 0.7904 - val_loss: 0.7224 - val_accuracy: 0.8518 - 106ms/epoch - 7ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.7062 - accuracy: 0.8099 - val_loss: 0.7072 - val_accuracy: 0.8546 - 95ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6891 - accuracy: 0.8298 - val_loss: 0.6927 - val_accuracy: 0.8686 - 97ms/epoch - 6ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6725 - accuracy: 0.8357 - val_loss: 0.6793 - val_accuracy: 0.9621 - 98ms/epoch - 6ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6575 - accuracy: 0.8432 - val_loss: 0.6676 - val_accuracy: 0.9693 - 99ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.6438 - accuracy: 0.8737 - val_loss: 0.6570 - val_accuracy: 0.9693 - 103ms/epoch - 6ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.6315 - accuracy: 0.8834 - val_loss: 0.6477 - val_accuracy: 0.9693 - 97ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.6203 - accuracy: 0.8870 - val_loss: 0.6387 - val_accuracy: 0.9676 - 105ms/epoch - 7ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.6095 - accuracy: 0.8864 - val_loss: 0.6298 - val_accuracy: 0.9676 - 104ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.5989 - accuracy: 0.8856 - val_loss: 0.6209 - val_accuracy: 0.9676 - 101ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.5890 - accuracy: 0.8861 - val_loss: 0.6122 - val_accuracy: 0.9676 - 98ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.5800 - accuracy: 0.8850 - val_loss: 0.6038 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.5716 - accuracy: 0.8867 - val_loss: 0.5957 - val_accuracy: 0.9676 - 101ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.5631 - accuracy: 0.8885 - val_loss: 0.5875 - val_accuracy: 0.9676 - 97ms/epoch - 6ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.5545 - accuracy: 0.8916 - val_loss: 0.5795 - val_accuracy: 0.9676 - 103ms/epoch - 6ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.5460 - accuracy: 0.8910 - val_loss: 0.5715 - val_accuracy: 0.9676 - 91ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.5385 - accuracy: 0.8924 - val_loss: 0.5642 - val_accuracy: 0.9676 - 88ms/epoch - 6ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.5318 - accuracy: 0.8906 - val_loss: 0.5571 - val_accuracy: 0.9676 - 100ms/epoch - 6ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.5257 - accuracy: 0.8915 - val_loss: 0.5502 - val_accuracy: 0.9676 - 103ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.5196 - accuracy: 0.8916 - val_loss: 0.5435 - val_accuracy: 0.9676 - 93ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.5131 - accuracy: 0.8926 - val_loss: 0.5369 - val_accuracy: 0.9676 - 113ms/epoch - 7ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.5081 - accuracy: 0.8927 - val_loss: 0.5305 - val_accuracy: 0.9676 - 98ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.5030 - accuracy: 0.8942 - val_loss: 0.5243 - val_accuracy: 0.9676 - 102ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.4973 - accuracy: 0.8937 - val_loss: 0.5182 - val_accuracy: 0.9676 - 102ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.4930 - accuracy: 0.8929 - val_loss: 0.5122 - val_accuracy: 0.9676 - 103ms/epoch - 6ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.4876 - accuracy: 0.8934 - val_loss: 0.5064 - val_accuracy: 0.9676 - 96ms/epoch - 6ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.4831 - accuracy: 0.8929 - val_loss: 0.5006 - val_accuracy: 0.9676 - 96ms/epoch - 6ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.4783 - accuracy: 0.8921 - val_loss: 0.4951 - val_accuracy: 0.9676 - 115ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.4746 - accuracy: 0.8927 - val_loss: 0.4896 - val_accuracy: 0.9676 - 95ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.4703 - accuracy: 0.8930 - val_loss: 0.4843 - val_accuracy: 0.9676 - 106ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.4660 - accuracy: 0.8940 - val_loss: 0.4791 - val_accuracy: 0.9676 - 97ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.4629 - accuracy: 0.8917 - val_loss: 0.4741 - val_accuracy: 0.9676 - 106ms/epoch - 7ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.4584 - accuracy: 0.8932 - val_loss: 0.4692 - val_accuracy: 0.9676 - 101ms/epoch - 6ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.4554 - accuracy: 0.8916 - val_loss: 0.4644 - val_accuracy: 0.9676 - 98ms/epoch - 6ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.4522 - accuracy: 0.8935 - val_loss: 0.4597 - val_accuracy: 0.9676 - 103ms/epoch - 6ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.4493 - accuracy: 0.8911 - val_loss: 0.4551 - val_accuracy: 0.9676 - 98ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.4446 - accuracy: 0.8934 - val_loss: 0.4506 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.4419 - accuracy: 0.8932 - val_loss: 0.4463 - val_accuracy: 0.9676 - 103ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.4391 - accuracy: 0.8920 - val_loss: 0.4421 - val_accuracy: 0.9676 - 90ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.4358 - accuracy: 0.8931 - val_loss: 0.4379 - val_accuracy: 0.9676 - 312ms/epoch - 20ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.4343 - accuracy: 0.8929 - val_loss: 0.4339 - val_accuracy: 0.9676 - 90ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.4321 - accuracy: 0.8922 - val_loss: 0.4301 - val_accuracy: 0.9676 - 100ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.4291 - accuracy: 0.8931 - val_loss: 0.4262 - val_accuracy: 0.9676 - 108ms/epoch - 7ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.4271 - accuracy: 0.8932 - val_loss: 0.4224 - val_accuracy: 0.9676 - 105ms/epoch - 7ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.4244 - accuracy: 0.8926 - val_loss: 0.4188 - val_accuracy: 0.9676 - 90ms/epoch - 6ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.4226 - accuracy: 0.8928 - val_loss: 0.4153 - val_accuracy: 0.9676 - 102ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.4207 - accuracy: 0.8929 - val_loss: 0.4118 - val_accuracy: 0.9676 - 96ms/epoch - 6ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.4180 - accuracy: 0.8929 - val_loss: 0.4084 - val_accuracy: 0.9676 - 102ms/epoch - 6ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.4161 - accuracy: 0.8917 - val_loss: 0.4052 - val_accuracy: 0.9676 - 99ms/epoch - 6ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.4137 - accuracy: 0.8931 - val_loss: 0.4020 - val_accuracy: 0.9676 - 101ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.4115 - accuracy: 0.8937 - val_loss: 0.3989 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.4097 - accuracy: 0.8929 - val_loss: 0.3958 - val_accuracy: 0.9676 - 98ms/epoch - 6ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.4082 - accuracy: 0.8931 - val_loss: 0.3928 - val_accuracy: 0.9676 - 95ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.4052 - accuracy: 0.8952 - val_loss: 0.3898 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.4048 - accuracy: 0.8932 - val_loss: 0.3870 - val_accuracy: 0.9676 - 90ms/epoch - 6ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.4042 - accuracy: 0.8929 - val_loss: 0.3842 - val_accuracy: 0.9676 - 96ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.4025 - accuracy: 0.8942 - val_loss: 0.3814 - val_accuracy: 0.9676 - 90ms/epoch - 6ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.4003 - accuracy: 0.8942 - val_loss: 0.3788 - val_accuracy: 0.9676 - 106ms/epoch - 7ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.3980 - accuracy: 0.8950 - val_loss: 0.3763 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.3970 - accuracy: 0.8942 - val_loss: 0.3739 - val_accuracy: 0.9676 - 104ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.3970 - accuracy: 0.8923 - val_loss: 0.3714 - val_accuracy: 0.9676 - 94ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.3960 - accuracy: 0.8926 - val_loss: 0.3691 - val_accuracy: 0.9676 - 89ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.3945 - accuracy: 0.8937 - val_loss: 0.3668 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.3925 - accuracy: 0.8942 - val_loss: 0.3647 - val_accuracy: 0.9676 - 92ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.3917 - accuracy: 0.8939 - val_loss: 0.3625 - val_accuracy: 0.9605 - 102ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.3913 - accuracy: 0.8939 - val_loss: 0.3604 - val_accuracy: 0.9605 - 91ms/epoch - 6ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.3887 - accuracy: 0.8945 - val_loss: 0.3583 - val_accuracy: 0.9605 - 97ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.3888 - accuracy: 0.8945 - val_loss: 0.3563 - val_accuracy: 0.9605 - 91ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.3879 - accuracy: 0.8928 - val_loss: 0.3544 - val_accuracy: 0.9605 - 102ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.3854 - accuracy: 0.8948 - val_loss: 0.3525 - val_accuracy: 0.9605 - 111ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.3850 - accuracy: 0.8946 - val_loss: 0.3506 - val_accuracy: 0.9605 - 106ms/epoch - 7ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.3856 - accuracy: 0.8927 - val_loss: 0.3488 - val_accuracy: 0.9605 - 99ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.3844 - accuracy: 0.8923 - val_loss: 0.3470 - val_accuracy: 0.9605 - 96ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.3829 - accuracy: 0.8938 - val_loss: 0.3453 - val_accuracy: 0.9605 - 92ms/epoch - 6ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.3810 - accuracy: 0.8952 - val_loss: 0.3437 - val_accuracy: 0.9605 - 99ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.3807 - accuracy: 0.8938 - val_loss: 0.3421 - val_accuracy: 0.9621 - 93ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.3801 - accuracy: 0.8942 - val_loss: 0.3405 - val_accuracy: 0.9627 - 92ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.3787 - accuracy: 0.8946 - val_loss: 0.3389 - val_accuracy: 0.9627 - 89ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.3789 - accuracy: 0.8933 - val_loss: 0.3375 - val_accuracy: 0.9627 - 104ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.3781 - accuracy: 0.8947 - val_loss: 0.3360 - val_accuracy: 0.9627 - 89ms/epoch - 6ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.3769 - accuracy: 0.8955 - val_loss: 0.3345 - val_accuracy: 0.9627 - 104ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.3768 - accuracy: 0.8958 - val_loss: 0.3332 - val_accuracy: 0.9627 - 91ms/epoch - 6ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.3760 - accuracy: 0.8965 - val_loss: 0.3318 - val_accuracy: 0.9627 - 100ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.3748 - accuracy: 0.8969 - val_loss: 0.3304 - val_accuracy: 0.9627 - 91ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.3730 - accuracy: 0.8974 - val_loss: 0.3291 - val_accuracy: 0.9627 - 98ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.3733 - accuracy: 0.8967 - val_loss: 0.3280 - val_accuracy: 0.9627 - 98ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.3733 - accuracy: 0.8966 - val_loss: 0.3268 - val_accuracy: 0.9627 - 109ms/epoch - 7ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.3724 - accuracy: 0.8967 - val_loss: 0.3257 - val_accuracy: 0.9627 - 91ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.3716 - accuracy: 0.8964 - val_loss: 0.3246 - val_accuracy: 0.9627 - 98ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.3697 - accuracy: 0.8968 - val_loss: 0.3234 - val_accuracy: 0.9627 - 90ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.3702 - accuracy: 0.8979 - val_loss: 0.3224 - val_accuracy: 0.9627 - 103ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.3700 - accuracy: 0.8983 - val_loss: 0.3213 - val_accuracy: 0.9627 - 91ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.3681 - accuracy: 0.8999 - val_loss: 0.3202 - val_accuracy: 0.9627 - 92ms/epoch - 6ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.3674 - accuracy: 0.9002 - val_loss: 0.3192 - val_accuracy: 0.9627 - 97ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.3684 - accuracy: 0.8999 - val_loss: 0.3181 - val_accuracy: 0.9627 - 99ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.3660 - accuracy: 0.9007 - val_loss: 0.3171 - val_accuracy: 0.9627 - 99ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 8)           272       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 8)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 289\n",
            "Trainable params: 289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8343 - accuracy: 0.5346 - val_loss: 0.8005 - val_accuracy: 0.4934 - 625ms/epoch - 20ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7620 - accuracy: 0.5890 - val_loss: 0.7400 - val_accuracy: 0.5848 - 157ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.7097 - accuracy: 0.6808 - val_loss: 0.6971 - val_accuracy: 0.9698 - 137ms/epoch - 4ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6705 - accuracy: 0.7348 - val_loss: 0.6673 - val_accuracy: 0.9698 - 145ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6437 - accuracy: 0.8423 - val_loss: 0.6479 - val_accuracy: 0.9693 - 141ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.6235 - accuracy: 0.8527 - val_loss: 0.6301 - val_accuracy: 0.9693 - 149ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.6044 - accuracy: 0.8681 - val_loss: 0.6119 - val_accuracy: 0.9695 - 144ms/epoch - 4ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.5849 - accuracy: 0.8731 - val_loss: 0.5932 - val_accuracy: 0.9695 - 143ms/epoch - 4ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.5671 - accuracy: 0.8723 - val_loss: 0.5744 - val_accuracy: 0.9693 - 133ms/epoch - 4ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.5475 - accuracy: 0.8862 - val_loss: 0.5555 - val_accuracy: 0.9693 - 133ms/epoch - 4ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.5291 - accuracy: 0.8862 - val_loss: 0.5365 - val_accuracy: 0.9693 - 133ms/epoch - 4ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.5112 - accuracy: 0.8844 - val_loss: 0.5176 - val_accuracy: 0.9693 - 138ms/epoch - 4ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.4944 - accuracy: 0.8856 - val_loss: 0.4994 - val_accuracy: 0.9693 - 153ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.4781 - accuracy: 0.8855 - val_loss: 0.4818 - val_accuracy: 0.9693 - 151ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.4640 - accuracy: 0.8863 - val_loss: 0.4651 - val_accuracy: 0.9693 - 142ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.4503 - accuracy: 0.8862 - val_loss: 0.4493 - val_accuracy: 0.9693 - 133ms/epoch - 4ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.4367 - accuracy: 0.8891 - val_loss: 0.4348 - val_accuracy: 0.9693 - 132ms/epoch - 4ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.4233 - accuracy: 0.8933 - val_loss: 0.4212 - val_accuracy: 0.9698 - 137ms/epoch - 4ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.4134 - accuracy: 0.8937 - val_loss: 0.4086 - val_accuracy: 0.9698 - 140ms/epoch - 4ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.4042 - accuracy: 0.8957 - val_loss: 0.3969 - val_accuracy: 0.9698 - 152ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.3960 - accuracy: 0.8963 - val_loss: 0.3860 - val_accuracy: 0.9698 - 139ms/epoch - 4ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.3870 - accuracy: 0.8972 - val_loss: 0.3756 - val_accuracy: 0.9698 - 162ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.3803 - accuracy: 0.8991 - val_loss: 0.3665 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.3732 - accuracy: 0.8996 - val_loss: 0.3584 - val_accuracy: 0.9849 - 145ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.3697 - accuracy: 0.8992 - val_loss: 0.3510 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.3642 - accuracy: 0.9009 - val_loss: 0.3440 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.3586 - accuracy: 0.9059 - val_loss: 0.3373 - val_accuracy: 0.9849 - 149ms/epoch - 5ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.3550 - accuracy: 0.9056 - val_loss: 0.3311 - val_accuracy: 0.9849 - 133ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.3514 - accuracy: 0.9054 - val_loss: 0.3263 - val_accuracy: 0.9849 - 143ms/epoch - 4ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.3480 - accuracy: 0.9059 - val_loss: 0.3211 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.3434 - accuracy: 0.9068 - val_loss: 0.3161 - val_accuracy: 0.9849 - 134ms/epoch - 4ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.3406 - accuracy: 0.9060 - val_loss: 0.3114 - val_accuracy: 0.9849 - 161ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.3382 - accuracy: 0.9067 - val_loss: 0.3076 - val_accuracy: 0.9849 - 168ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.3340 - accuracy: 0.9068 - val_loss: 0.3037 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.3332 - accuracy: 0.9071 - val_loss: 0.2993 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.3284 - accuracy: 0.9076 - val_loss: 0.2959 - val_accuracy: 0.9849 - 157ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.3254 - accuracy: 0.9081 - val_loss: 0.2918 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.3244 - accuracy: 0.9089 - val_loss: 0.2881 - val_accuracy: 0.9849 - 128ms/epoch - 4ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.3220 - accuracy: 0.9088 - val_loss: 0.2852 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.3213 - accuracy: 0.9088 - val_loss: 0.2815 - val_accuracy: 0.9849 - 134ms/epoch - 4ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.3171 - accuracy: 0.9096 - val_loss: 0.2789 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.3155 - accuracy: 0.9097 - val_loss: 0.2752 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.3129 - accuracy: 0.9105 - val_loss: 0.2726 - val_accuracy: 0.9849 - 148ms/epoch - 5ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.3107 - accuracy: 0.9111 - val_loss: 0.2695 - val_accuracy: 0.9849 - 134ms/epoch - 4ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.3099 - accuracy: 0.9106 - val_loss: 0.2674 - val_accuracy: 0.9849 - 133ms/epoch - 4ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.3073 - accuracy: 0.9110 - val_loss: 0.2649 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.3070 - accuracy: 0.9104 - val_loss: 0.2633 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.3039 - accuracy: 0.9118 - val_loss: 0.2609 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.3043 - accuracy: 0.9106 - val_loss: 0.2591 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.3028 - accuracy: 0.9111 - val_loss: 0.2572 - val_accuracy: 0.9849 - 157ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.3024 - accuracy: 0.9108 - val_loss: 0.2555 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2999 - accuracy: 0.9110 - val_loss: 0.2548 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2982 - accuracy: 0.9130 - val_loss: 0.2528 - val_accuracy: 0.9849 - 136ms/epoch - 4ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2967 - accuracy: 0.9130 - val_loss: 0.2518 - val_accuracy: 0.9849 - 148ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2925 - accuracy: 0.9193 - val_loss: 0.2504 - val_accuracy: 0.9849 - 149ms/epoch - 5ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2940 - accuracy: 0.9205 - val_loss: 0.2482 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2923 - accuracy: 0.9196 - val_loss: 0.2469 - val_accuracy: 0.9849 - 132ms/epoch - 4ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2935 - accuracy: 0.9190 - val_loss: 0.2462 - val_accuracy: 0.9849 - 143ms/epoch - 4ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2897 - accuracy: 0.9210 - val_loss: 0.2450 - val_accuracy: 0.9849 - 150ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2905 - accuracy: 0.9204 - val_loss: 0.2435 - val_accuracy: 0.9849 - 139ms/epoch - 4ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2881 - accuracy: 0.9209 - val_loss: 0.2429 - val_accuracy: 0.9849 - 130ms/epoch - 4ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2860 - accuracy: 0.9219 - val_loss: 0.2416 - val_accuracy: 0.9849 - 147ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2857 - accuracy: 0.9245 - val_loss: 0.2404 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2860 - accuracy: 0.9242 - val_loss: 0.2402 - val_accuracy: 0.9849 - 138ms/epoch - 4ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2829 - accuracy: 0.9230 - val_loss: 0.2381 - val_accuracy: 0.9849 - 132ms/epoch - 4ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2826 - accuracy: 0.9262 - val_loss: 0.2380 - val_accuracy: 0.9849 - 133ms/epoch - 4ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2804 - accuracy: 0.9285 - val_loss: 0.2358 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2808 - accuracy: 0.9282 - val_loss: 0.2356 - val_accuracy: 0.9849 - 163ms/epoch - 5ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2794 - accuracy: 0.9326 - val_loss: 0.2348 - val_accuracy: 0.9849 - 148ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2810 - accuracy: 0.9329 - val_loss: 0.2333 - val_accuracy: 0.9849 - 136ms/epoch - 4ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2799 - accuracy: 0.9344 - val_loss: 0.2309 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2750 - accuracy: 0.9335 - val_loss: 0.2295 - val_accuracy: 0.9849 - 147ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2770 - accuracy: 0.9336 - val_loss: 0.2280 - val_accuracy: 0.9849 - 136ms/epoch - 4ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2758 - accuracy: 0.9345 - val_loss: 0.2261 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2744 - accuracy: 0.9352 - val_loss: 0.2252 - val_accuracy: 0.9849 - 150ms/epoch - 5ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.2759 - accuracy: 0.9382 - val_loss: 0.2237 - val_accuracy: 0.9849 - 145ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.2742 - accuracy: 0.9397 - val_loss: 0.2229 - val_accuracy: 0.9849 - 152ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.2720 - accuracy: 0.9436 - val_loss: 0.2221 - val_accuracy: 0.9849 - 164ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.2715 - accuracy: 0.9467 - val_loss: 0.2206 - val_accuracy: 0.9849 - 138ms/epoch - 4ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.2716 - accuracy: 0.9425 - val_loss: 0.2192 - val_accuracy: 0.9849 - 151ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.2704 - accuracy: 0.9446 - val_loss: 0.2180 - val_accuracy: 0.9849 - 132ms/epoch - 4ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.2694 - accuracy: 0.9448 - val_loss: 0.2172 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.2689 - accuracy: 0.9491 - val_loss: 0.2164 - val_accuracy: 0.9849 - 156ms/epoch - 5ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.2692 - accuracy: 0.9459 - val_loss: 0.2147 - val_accuracy: 0.9849 - 139ms/epoch - 4ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.2677 - accuracy: 0.9466 - val_loss: 0.2142 - val_accuracy: 0.9849 - 149ms/epoch - 5ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.2680 - accuracy: 0.9456 - val_loss: 0.2133 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.2700 - accuracy: 0.9454 - val_loss: 0.2124 - val_accuracy: 0.9849 - 145ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.2666 - accuracy: 0.9469 - val_loss: 0.2120 - val_accuracy: 0.9849 - 150ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.2665 - accuracy: 0.9482 - val_loss: 0.2110 - val_accuracy: 0.9849 - 148ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.2672 - accuracy: 0.9457 - val_loss: 0.2104 - val_accuracy: 0.9849 - 167ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.2657 - accuracy: 0.9485 - val_loss: 0.2103 - val_accuracy: 0.9849 - 150ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.2640 - accuracy: 0.9516 - val_loss: 0.2098 - val_accuracy: 0.9849 - 150ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.2663 - accuracy: 0.9514 - val_loss: 0.2094 - val_accuracy: 0.9849 - 138ms/epoch - 4ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.2630 - accuracy: 0.9512 - val_loss: 0.2086 - val_accuracy: 0.9849 - 157ms/epoch - 5ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.2641 - accuracy: 0.9506 - val_loss: 0.2079 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.2601 - accuracy: 0.9531 - val_loss: 0.2078 - val_accuracy: 0.9849 - 149ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.2645 - accuracy: 0.9515 - val_loss: 0.2067 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.2602 - accuracy: 0.9501 - val_loss: 0.2063 - val_accuracy: 0.9849 - 164ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.2639 - accuracy: 0.9514 - val_loss: 0.2057 - val_accuracy: 0.9849 - 144ms/epoch - 4ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.2607 - accuracy: 0.9523 - val_loss: 0.2055 - val_accuracy: 0.9849 - 143ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 8)           272       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 8)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 289\n",
            "Trainable params: 289\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.8780 - accuracy: 0.3976 - val_loss: 0.8534 - val_accuracy: 0.4292 - 594ms/epoch - 37ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8299 - accuracy: 0.4410 - val_loss: 0.8101 - val_accuracy: 0.4846 - 99ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7878 - accuracy: 0.5639 - val_loss: 0.7727 - val_accuracy: 0.9748 - 97ms/epoch - 6ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7529 - accuracy: 0.7301 - val_loss: 0.7424 - val_accuracy: 0.9695 - 101ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.7241 - accuracy: 0.8696 - val_loss: 0.7171 - val_accuracy: 0.9695 - 97ms/epoch - 6ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6985 - accuracy: 0.8804 - val_loss: 0.6945 - val_accuracy: 0.9690 - 110ms/epoch - 7ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6760 - accuracy: 0.8819 - val_loss: 0.6753 - val_accuracy: 0.9690 - 93ms/epoch - 6ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6572 - accuracy: 0.8800 - val_loss: 0.6589 - val_accuracy: 0.9690 - 107ms/epoch - 7ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6411 - accuracy: 0.8774 - val_loss: 0.6459 - val_accuracy: 0.9690 - 96ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.6276 - accuracy: 0.8767 - val_loss: 0.6339 - val_accuracy: 0.9690 - 105ms/epoch - 7ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.6143 - accuracy: 0.8783 - val_loss: 0.6214 - val_accuracy: 0.9690 - 97ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.6018 - accuracy: 0.8778 - val_loss: 0.6094 - val_accuracy: 0.9695 - 119ms/epoch - 7ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5896 - accuracy: 0.8764 - val_loss: 0.5971 - val_accuracy: 0.9695 - 96ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.5768 - accuracy: 0.8777 - val_loss: 0.5846 - val_accuracy: 0.9695 - 102ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.5642 - accuracy: 0.8771 - val_loss: 0.5719 - val_accuracy: 0.9695 - 92ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.5529 - accuracy: 0.8770 - val_loss: 0.5593 - val_accuracy: 0.9695 - 103ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.5405 - accuracy: 0.8775 - val_loss: 0.5470 - val_accuracy: 0.9695 - 98ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.5288 - accuracy: 0.8776 - val_loss: 0.5347 - val_accuracy: 0.9695 - 119ms/epoch - 7ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.5179 - accuracy: 0.8800 - val_loss: 0.5229 - val_accuracy: 0.9695 - 97ms/epoch - 6ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.5069 - accuracy: 0.8794 - val_loss: 0.5114 - val_accuracy: 0.9701 - 105ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.4982 - accuracy: 0.8805 - val_loss: 0.5005 - val_accuracy: 0.9701 - 110ms/epoch - 7ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.4874 - accuracy: 0.8806 - val_loss: 0.4899 - val_accuracy: 0.9701 - 102ms/epoch - 6ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.4781 - accuracy: 0.8821 - val_loss: 0.4798 - val_accuracy: 0.9701 - 96ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.4713 - accuracy: 0.8817 - val_loss: 0.4701 - val_accuracy: 0.9701 - 110ms/epoch - 7ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.4618 - accuracy: 0.8845 - val_loss: 0.4609 - val_accuracy: 0.9698 - 92ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.4549 - accuracy: 0.8847 - val_loss: 0.4519 - val_accuracy: 0.9698 - 114ms/epoch - 7ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.4459 - accuracy: 0.8870 - val_loss: 0.4434 - val_accuracy: 0.9920 - 101ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.4382 - accuracy: 0.8885 - val_loss: 0.4352 - val_accuracy: 0.9920 - 94ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.4316 - accuracy: 0.8884 - val_loss: 0.4276 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.4258 - accuracy: 0.8886 - val_loss: 0.4204 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.4194 - accuracy: 0.8902 - val_loss: 0.4134 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.4140 - accuracy: 0.8896 - val_loss: 0.4070 - val_accuracy: 0.9920 - 104ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.4073 - accuracy: 0.8923 - val_loss: 0.4005 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.4025 - accuracy: 0.8919 - val_loss: 0.3944 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.3990 - accuracy: 0.8930 - val_loss: 0.3889 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.3927 - accuracy: 0.8972 - val_loss: 0.3834 - val_accuracy: 0.9920 - 97ms/epoch - 6ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.3884 - accuracy: 0.8986 - val_loss: 0.3781 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.3838 - accuracy: 0.9003 - val_loss: 0.3731 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.3816 - accuracy: 0.9040 - val_loss: 0.3687 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.3769 - accuracy: 0.9102 - val_loss: 0.3644 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.3736 - accuracy: 0.9119 - val_loss: 0.3597 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.3700 - accuracy: 0.9147 - val_loss: 0.3559 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.3651 - accuracy: 0.9178 - val_loss: 0.3520 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.3613 - accuracy: 0.9272 - val_loss: 0.3483 - val_accuracy: 0.9920 - 97ms/epoch - 6ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.3589 - accuracy: 0.9323 - val_loss: 0.3445 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.3547 - accuracy: 0.9337 - val_loss: 0.3405 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.3530 - accuracy: 0.9394 - val_loss: 0.3374 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.3498 - accuracy: 0.9460 - val_loss: 0.3343 - val_accuracy: 0.9920 - 97ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.3461 - accuracy: 0.9462 - val_loss: 0.3311 - val_accuracy: 0.9920 - 111ms/epoch - 7ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.3434 - accuracy: 0.9417 - val_loss: 0.3280 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.3412 - accuracy: 0.9310 - val_loss: 0.3248 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.3386 - accuracy: 0.9305 - val_loss: 0.3219 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.3358 - accuracy: 0.9304 - val_loss: 0.3189 - val_accuracy: 0.9849 - 105ms/epoch - 7ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.3335 - accuracy: 0.9312 - val_loss: 0.3168 - val_accuracy: 0.9849 - 102ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.3305 - accuracy: 0.9333 - val_loss: 0.3146 - val_accuracy: 0.9849 - 99ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.3286 - accuracy: 0.9341 - val_loss: 0.3117 - val_accuracy: 0.9849 - 105ms/epoch - 7ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.3258 - accuracy: 0.9348 - val_loss: 0.3096 - val_accuracy: 0.9849 - 103ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.3220 - accuracy: 0.9377 - val_loss: 0.3073 - val_accuracy: 0.9849 - 112ms/epoch - 7ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.3204 - accuracy: 0.9373 - val_loss: 0.3045 - val_accuracy: 0.9849 - 98ms/epoch - 6ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.3188 - accuracy: 0.9357 - val_loss: 0.3024 - val_accuracy: 0.9849 - 106ms/epoch - 7ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.3172 - accuracy: 0.9369 - val_loss: 0.3009 - val_accuracy: 0.9849 - 99ms/epoch - 6ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.3159 - accuracy: 0.9357 - val_loss: 0.2982 - val_accuracy: 0.9849 - 104ms/epoch - 6ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.3133 - accuracy: 0.9367 - val_loss: 0.2961 - val_accuracy: 0.9849 - 104ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.3133 - accuracy: 0.9362 - val_loss: 0.2948 - val_accuracy: 0.9849 - 96ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.3095 - accuracy: 0.9366 - val_loss: 0.2928 - val_accuracy: 0.9849 - 113ms/epoch - 7ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.3078 - accuracy: 0.9367 - val_loss: 0.2917 - val_accuracy: 0.9849 - 97ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.3041 - accuracy: 0.9387 - val_loss: 0.2900 - val_accuracy: 0.9849 - 114ms/epoch - 7ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.3041 - accuracy: 0.9379 - val_loss: 0.2883 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.3035 - accuracy: 0.9346 - val_loss: 0.2861 - val_accuracy: 0.9849 - 109ms/epoch - 7ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.3000 - accuracy: 0.9391 - val_loss: 0.2845 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2986 - accuracy: 0.9379 - val_loss: 0.2831 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2980 - accuracy: 0.9375 - val_loss: 0.2808 - val_accuracy: 0.9920 - 114ms/epoch - 7ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2961 - accuracy: 0.9376 - val_loss: 0.2788 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2950 - accuracy: 0.9365 - val_loss: 0.2770 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2934 - accuracy: 0.9381 - val_loss: 0.2757 - val_accuracy: 0.9920 - 96ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2891 - accuracy: 0.9403 - val_loss: 0.2740 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2909 - accuracy: 0.9374 - val_loss: 0.2715 - val_accuracy: 0.9920 - 95ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2903 - accuracy: 0.9364 - val_loss: 0.2697 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2874 - accuracy: 0.9377 - val_loss: 0.2677 - val_accuracy: 0.9920 - 94ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2851 - accuracy: 0.9399 - val_loss: 0.2663 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2853 - accuracy: 0.9391 - val_loss: 0.2641 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2835 - accuracy: 0.9385 - val_loss: 0.2626 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2794 - accuracy: 0.9426 - val_loss: 0.2607 - val_accuracy: 0.9920 - 119ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2808 - accuracy: 0.9412 - val_loss: 0.2588 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2781 - accuracy: 0.9404 - val_loss: 0.2570 - val_accuracy: 0.9920 - 112ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2784 - accuracy: 0.9402 - val_loss: 0.2559 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2772 - accuracy: 0.9399 - val_loss: 0.2541 - val_accuracy: 0.9920 - 95ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2757 - accuracy: 0.9410 - val_loss: 0.2529 - val_accuracy: 0.9920 - 102ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2743 - accuracy: 0.9413 - val_loss: 0.2509 - val_accuracy: 0.9920 - 98ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2747 - accuracy: 0.9386 - val_loss: 0.2496 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2736 - accuracy: 0.9394 - val_loss: 0.2480 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2740 - accuracy: 0.9386 - val_loss: 0.2464 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2718 - accuracy: 0.9389 - val_loss: 0.2447 - val_accuracy: 0.9920 - 114ms/epoch - 7ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2715 - accuracy: 0.9393 - val_loss: 0.2440 - val_accuracy: 0.9920 - 94ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2711 - accuracy: 0.9387 - val_loss: 0.2422 - val_accuracy: 0.9920 - 110ms/epoch - 7ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2706 - accuracy: 0.9398 - val_loss: 0.2426 - val_accuracy: 0.9923 - 98ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2692 - accuracy: 0.9404 - val_loss: 0.2407 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2695 - accuracy: 0.9379 - val_loss: 0.2394 - val_accuracy: 0.9923 - 102ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2658 - accuracy: 0.9400 - val_loss: 0.2382 - val_accuracy: 0.9923 - 112ms/epoch - 7ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2679 - accuracy: 0.9378 - val_loss: 0.2376 - val_accuracy: 0.9923 - 95ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 8)           272       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 8)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 281\n",
            "Trainable params: 281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8476 - accuracy: 0.5074 - val_loss: 0.8076 - val_accuracy: 0.4341 - 623ms/epoch - 19ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7760 - accuracy: 0.6222 - val_loss: 0.7466 - val_accuracy: 0.8046 - 146ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.7217 - accuracy: 0.6462 - val_loss: 0.7055 - val_accuracy: 0.8071 - 142ms/epoch - 4ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6876 - accuracy: 0.7451 - val_loss: 0.6838 - val_accuracy: 0.9149 - 152ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6677 - accuracy: 0.8020 - val_loss: 0.6727 - val_accuracy: 0.9978 - 135ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.6533 - accuracy: 0.8083 - val_loss: 0.6619 - val_accuracy: 0.9978 - 135ms/epoch - 4ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.6373 - accuracy: 0.8179 - val_loss: 0.6490 - val_accuracy: 0.9970 - 136ms/epoch - 4ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.6188 - accuracy: 0.8231 - val_loss: 0.6343 - val_accuracy: 0.9934 - 152ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.6008 - accuracy: 0.8254 - val_loss: 0.6187 - val_accuracy: 0.9934 - 143ms/epoch - 4ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.5814 - accuracy: 0.8665 - val_loss: 0.6020 - val_accuracy: 0.9934 - 156ms/epoch - 5ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.5626 - accuracy: 0.8758 - val_loss: 0.5847 - val_accuracy: 0.9934 - 135ms/epoch - 4ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.5438 - accuracy: 0.8797 - val_loss: 0.5674 - val_accuracy: 0.9934 - 152ms/epoch - 5ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.5265 - accuracy: 0.8923 - val_loss: 0.5503 - val_accuracy: 0.9923 - 149ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.5101 - accuracy: 0.8918 - val_loss: 0.5340 - val_accuracy: 0.9923 - 158ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.4952 - accuracy: 0.8934 - val_loss: 0.5185 - val_accuracy: 0.9923 - 140ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.4808 - accuracy: 0.8937 - val_loss: 0.5032 - val_accuracy: 0.9923 - 156ms/epoch - 5ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.4678 - accuracy: 0.8926 - val_loss: 0.4883 - val_accuracy: 0.9923 - 142ms/epoch - 4ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.4523 - accuracy: 0.8932 - val_loss: 0.4739 - val_accuracy: 0.9923 - 163ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.4418 - accuracy: 0.8937 - val_loss: 0.4602 - val_accuracy: 0.9923 - 138ms/epoch - 4ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.4326 - accuracy: 0.8933 - val_loss: 0.4476 - val_accuracy: 0.9923 - 164ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.4235 - accuracy: 0.8942 - val_loss: 0.4361 - val_accuracy: 0.9923 - 158ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.4156 - accuracy: 0.8923 - val_loss: 0.4253 - val_accuracy: 0.9923 - 155ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.4075 - accuracy: 0.8963 - val_loss: 0.4151 - val_accuracy: 0.9923 - 146ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.3985 - accuracy: 0.8985 - val_loss: 0.4055 - val_accuracy: 0.9923 - 135ms/epoch - 4ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.3940 - accuracy: 0.8981 - val_loss: 0.3966 - val_accuracy: 0.9923 - 153ms/epoch - 5ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.3883 - accuracy: 0.8974 - val_loss: 0.3885 - val_accuracy: 0.9920 - 137ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.3813 - accuracy: 0.8987 - val_loss: 0.3808 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.3761 - accuracy: 0.8991 - val_loss: 0.3734 - val_accuracy: 0.9920 - 144ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.3720 - accuracy: 0.8986 - val_loss: 0.3667 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.3676 - accuracy: 0.9000 - val_loss: 0.3603 - val_accuracy: 0.9920 - 152ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.3643 - accuracy: 0.9031 - val_loss: 0.3544 - val_accuracy: 0.9920 - 157ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.3613 - accuracy: 0.9034 - val_loss: 0.3488 - val_accuracy: 0.9920 - 155ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.3570 - accuracy: 0.9038 - val_loss: 0.3432 - val_accuracy: 0.9920 - 146ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.3552 - accuracy: 0.9055 - val_loss: 0.3384 - val_accuracy: 0.9920 - 135ms/epoch - 4ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.3523 - accuracy: 0.9078 - val_loss: 0.3338 - val_accuracy: 0.9920 - 151ms/epoch - 5ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.3478 - accuracy: 0.9081 - val_loss: 0.3292 - val_accuracy: 0.9920 - 155ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.3451 - accuracy: 0.9083 - val_loss: 0.3249 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.3428 - accuracy: 0.9092 - val_loss: 0.3207 - val_accuracy: 0.9920 - 151ms/epoch - 5ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.3407 - accuracy: 0.9086 - val_loss: 0.3172 - val_accuracy: 0.9920 - 134ms/epoch - 4ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.3397 - accuracy: 0.9082 - val_loss: 0.3140 - val_accuracy: 0.9920 - 137ms/epoch - 4ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.3348 - accuracy: 0.9092 - val_loss: 0.3102 - val_accuracy: 0.9920 - 155ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.3327 - accuracy: 0.9088 - val_loss: 0.3070 - val_accuracy: 0.9920 - 135ms/epoch - 4ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.3311 - accuracy: 0.9090 - val_loss: 0.3039 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.3291 - accuracy: 0.9092 - val_loss: 0.3008 - val_accuracy: 0.9920 - 149ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.3273 - accuracy: 0.9089 - val_loss: 0.2981 - val_accuracy: 0.9920 - 134ms/epoch - 4ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.3261 - accuracy: 0.9079 - val_loss: 0.2958 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.3227 - accuracy: 0.9089 - val_loss: 0.2928 - val_accuracy: 0.9920 - 132ms/epoch - 4ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.3209 - accuracy: 0.9096 - val_loss: 0.2906 - val_accuracy: 0.9920 - 132ms/epoch - 4ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.3218 - accuracy: 0.9093 - val_loss: 0.2884 - val_accuracy: 0.9920 - 133ms/epoch - 4ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.3206 - accuracy: 0.9088 - val_loss: 0.2858 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.3192 - accuracy: 0.9096 - val_loss: 0.2834 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.3153 - accuracy: 0.9100 - val_loss: 0.2814 - val_accuracy: 0.9920 - 135ms/epoch - 4ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.3151 - accuracy: 0.9104 - val_loss: 0.2793 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.3138 - accuracy: 0.9111 - val_loss: 0.2778 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.3099 - accuracy: 0.9118 - val_loss: 0.2759 - val_accuracy: 0.9920 - 139ms/epoch - 4ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.3090 - accuracy: 0.9116 - val_loss: 0.2729 - val_accuracy: 0.9920 - 133ms/epoch - 4ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.3087 - accuracy: 0.9126 - val_loss: 0.2714 - val_accuracy: 0.9920 - 129ms/epoch - 4ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.3092 - accuracy: 0.9114 - val_loss: 0.2696 - val_accuracy: 0.9920 - 149ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.3052 - accuracy: 0.9126 - val_loss: 0.2677 - val_accuracy: 0.9920 - 146ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.3058 - accuracy: 0.9126 - val_loss: 0.2663 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.3038 - accuracy: 0.9126 - val_loss: 0.2645 - val_accuracy: 0.9920 - 152ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.3025 - accuracy: 0.9130 - val_loss: 0.2632 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.3008 - accuracy: 0.9137 - val_loss: 0.2613 - val_accuracy: 0.9920 - 153ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.3014 - accuracy: 0.9127 - val_loss: 0.2607 - val_accuracy: 0.9920 - 162ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2998 - accuracy: 0.9124 - val_loss: 0.2586 - val_accuracy: 0.9920 - 159ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.3012 - accuracy: 0.9112 - val_loss: 0.2575 - val_accuracy: 0.9920 - 158ms/epoch - 5ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2966 - accuracy: 0.9124 - val_loss: 0.2559 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2950 - accuracy: 0.9129 - val_loss: 0.2541 - val_accuracy: 0.9920 - 158ms/epoch - 5ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2944 - accuracy: 0.9123 - val_loss: 0.2528 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2965 - accuracy: 0.9119 - val_loss: 0.2511 - val_accuracy: 0.9920 - 144ms/epoch - 4ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2955 - accuracy: 0.9126 - val_loss: 0.2499 - val_accuracy: 0.9849 - 139ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2912 - accuracy: 0.9128 - val_loss: 0.2489 - val_accuracy: 0.9920 - 142ms/epoch - 4ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2929 - accuracy: 0.9126 - val_loss: 0.2472 - val_accuracy: 0.9920 - 148ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2919 - accuracy: 0.9128 - val_loss: 0.2459 - val_accuracy: 0.9920 - 146ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2873 - accuracy: 0.9131 - val_loss: 0.2449 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.2901 - accuracy: 0.9127 - val_loss: 0.2434 - val_accuracy: 0.9920 - 146ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.2882 - accuracy: 0.9145 - val_loss: 0.2423 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.2849 - accuracy: 0.9159 - val_loss: 0.2412 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.2854 - accuracy: 0.9203 - val_loss: 0.2399 - val_accuracy: 0.9920 - 140ms/epoch - 4ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.2859 - accuracy: 0.9132 - val_loss: 0.2390 - val_accuracy: 0.9920 - 134ms/epoch - 4ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.2844 - accuracy: 0.9175 - val_loss: 0.2372 - val_accuracy: 0.9920 - 140ms/epoch - 4ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.2834 - accuracy: 0.9226 - val_loss: 0.2363 - val_accuracy: 0.9920 - 153ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.2812 - accuracy: 0.9205 - val_loss: 0.2348 - val_accuracy: 0.9920 - 144ms/epoch - 5ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.2834 - accuracy: 0.9162 - val_loss: 0.2335 - val_accuracy: 0.9920 - 135ms/epoch - 4ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.2814 - accuracy: 0.9160 - val_loss: 0.2326 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.2831 - accuracy: 0.9172 - val_loss: 0.2317 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.2829 - accuracy: 0.9189 - val_loss: 0.2308 - val_accuracy: 0.9920 - 158ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.2803 - accuracy: 0.9227 - val_loss: 0.2297 - val_accuracy: 0.9920 - 162ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.2809 - accuracy: 0.9195 - val_loss: 0.2283 - val_accuracy: 0.9920 - 141ms/epoch - 4ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.2794 - accuracy: 0.9219 - val_loss: 0.2276 - val_accuracy: 0.9920 - 131ms/epoch - 4ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.2774 - accuracy: 0.9222 - val_loss: 0.2270 - val_accuracy: 0.9920 - 145ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.2766 - accuracy: 0.9216 - val_loss: 0.2257 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.2785 - accuracy: 0.9225 - val_loss: 0.2250 - val_accuracy: 0.9920 - 139ms/epoch - 4ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.2764 - accuracy: 0.9221 - val_loss: 0.2241 - val_accuracy: 0.9849 - 156ms/epoch - 5ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.2758 - accuracy: 0.9196 - val_loss: 0.2231 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.2728 - accuracy: 0.9244 - val_loss: 0.2226 - val_accuracy: 0.9920 - 149ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.2777 - accuracy: 0.9195 - val_loss: 0.2211 - val_accuracy: 0.9849 - 141ms/epoch - 4ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.2737 - accuracy: 0.9231 - val_loss: 0.2205 - val_accuracy: 0.9849 - 144ms/epoch - 4ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.2786 - accuracy: 0.9175 - val_loss: 0.2206 - val_accuracy: 0.9920 - 133ms/epoch - 4ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.2747 - accuracy: 0.9233 - val_loss: 0.2193 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 8)           272       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 8)           0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 8)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 8)                 0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 9         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 281\n",
            "Trainable params: 281\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.8579 - accuracy: 0.4770 - val_loss: 0.8333 - val_accuracy: 0.3425 - 567ms/epoch - 35ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8137 - accuracy: 0.6079 - val_loss: 0.7961 - val_accuracy: 0.5524 - 97ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7780 - accuracy: 0.6726 - val_loss: 0.7656 - val_accuracy: 0.9130 - 331ms/epoch - 21ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7487 - accuracy: 0.7660 - val_loss: 0.7401 - val_accuracy: 0.9130 - 99ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.7238 - accuracy: 0.8114 - val_loss: 0.7183 - val_accuracy: 0.9130 - 101ms/epoch - 6ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.7025 - accuracy: 0.8393 - val_loss: 0.7002 - val_accuracy: 0.9130 - 101ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6852 - accuracy: 0.8532 - val_loss: 0.6865 - val_accuracy: 0.9130 - 105ms/epoch - 7ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6716 - accuracy: 0.8576 - val_loss: 0.6771 - val_accuracy: 0.9503 - 106ms/epoch - 7ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6629 - accuracy: 0.8559 - val_loss: 0.6714 - val_accuracy: 0.9503 - 95ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.6557 - accuracy: 0.8528 - val_loss: 0.6659 - val_accuracy: 0.9503 - 123ms/epoch - 8ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.6492 - accuracy: 0.8649 - val_loss: 0.6603 - val_accuracy: 0.9503 - 105ms/epoch - 7ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.6418 - accuracy: 0.8651 - val_loss: 0.6544 - val_accuracy: 0.9503 - 100ms/epoch - 6ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.6346 - accuracy: 0.8652 - val_loss: 0.6485 - val_accuracy: 0.9498 - 100ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.6269 - accuracy: 0.8672 - val_loss: 0.6423 - val_accuracy: 0.9498 - 112ms/epoch - 7ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.6194 - accuracy: 0.8633 - val_loss: 0.6359 - val_accuracy: 0.9498 - 95ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.6116 - accuracy: 0.8644 - val_loss: 0.6293 - val_accuracy: 0.9470 - 109ms/epoch - 7ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.6039 - accuracy: 0.8686 - val_loss: 0.6225 - val_accuracy: 0.9462 - 101ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.5957 - accuracy: 0.8664 - val_loss: 0.6158 - val_accuracy: 0.9462 - 106ms/epoch - 7ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.5876 - accuracy: 0.8650 - val_loss: 0.6088 - val_accuracy: 0.9459 - 99ms/epoch - 6ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.5815 - accuracy: 0.8651 - val_loss: 0.6019 - val_accuracy: 0.9459 - 111ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.5732 - accuracy: 0.8673 - val_loss: 0.5951 - val_accuracy: 0.9459 - 101ms/epoch - 6ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.5651 - accuracy: 0.8693 - val_loss: 0.5881 - val_accuracy: 0.9459 - 110ms/epoch - 7ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.5588 - accuracy: 0.8689 - val_loss: 0.5811 - val_accuracy: 0.9459 - 96ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.5514 - accuracy: 0.8703 - val_loss: 0.5744 - val_accuracy: 0.9459 - 100ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.5440 - accuracy: 0.8728 - val_loss: 0.5677 - val_accuracy: 0.9459 - 109ms/epoch - 7ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.5383 - accuracy: 0.8711 - val_loss: 0.5610 - val_accuracy: 0.9459 - 96ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.5309 - accuracy: 0.8710 - val_loss: 0.5541 - val_accuracy: 0.9459 - 123ms/epoch - 8ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.5240 - accuracy: 0.8712 - val_loss: 0.5473 - val_accuracy: 0.9459 - 98ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.5170 - accuracy: 0.8725 - val_loss: 0.5408 - val_accuracy: 0.9459 - 110ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.5114 - accuracy: 0.8724 - val_loss: 0.5342 - val_accuracy: 0.9459 - 101ms/epoch - 6ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.5052 - accuracy: 0.8708 - val_loss: 0.5279 - val_accuracy: 0.9459 - 105ms/epoch - 7ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.5005 - accuracy: 0.8701 - val_loss: 0.5217 - val_accuracy: 0.9459 - 107ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.4934 - accuracy: 0.8698 - val_loss: 0.5155 - val_accuracy: 0.9459 - 101ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.4890 - accuracy: 0.8719 - val_loss: 0.5095 - val_accuracy: 0.9934 - 113ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.4840 - accuracy: 0.8740 - val_loss: 0.5036 - val_accuracy: 0.9934 - 99ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.4782 - accuracy: 0.8743 - val_loss: 0.4977 - val_accuracy: 0.9934 - 110ms/epoch - 7ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.4725 - accuracy: 0.8801 - val_loss: 0.4921 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.4668 - accuracy: 0.9366 - val_loss: 0.4864 - val_accuracy: 0.9934 - 111ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.4630 - accuracy: 0.9363 - val_loss: 0.4808 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.4592 - accuracy: 0.9397 - val_loss: 0.4756 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.4533 - accuracy: 0.9389 - val_loss: 0.4700 - val_accuracy: 0.9934 - 99ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.4470 - accuracy: 0.9321 - val_loss: 0.4646 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.4455 - accuracy: 0.9245 - val_loss: 0.4594 - val_accuracy: 0.9934 - 97ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.4408 - accuracy: 0.9287 - val_loss: 0.4546 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.4364 - accuracy: 0.9389 - val_loss: 0.4493 - val_accuracy: 0.9934 - 98ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.4319 - accuracy: 0.9174 - val_loss: 0.4443 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.4269 - accuracy: 0.9130 - val_loss: 0.4396 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.4261 - accuracy: 0.9225 - val_loss: 0.4351 - val_accuracy: 0.9934 - 98ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.4226 - accuracy: 0.9126 - val_loss: 0.4307 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.4170 - accuracy: 0.9154 - val_loss: 0.4262 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.4137 - accuracy: 0.9154 - val_loss: 0.4216 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.4120 - accuracy: 0.9133 - val_loss: 0.4173 - val_accuracy: 0.9934 - 119ms/epoch - 7ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.4087 - accuracy: 0.9141 - val_loss: 0.4133 - val_accuracy: 0.9934 - 103ms/epoch - 6ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.4055 - accuracy: 0.9142 - val_loss: 0.4092 - val_accuracy: 0.9934 - 121ms/epoch - 8ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.4031 - accuracy: 0.9206 - val_loss: 0.4052 - val_accuracy: 0.9934 - 95ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.4014 - accuracy: 0.9229 - val_loss: 0.4016 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.3947 - accuracy: 0.9247 - val_loss: 0.3976 - val_accuracy: 0.9934 - 97ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.3957 - accuracy: 0.9238 - val_loss: 0.3939 - val_accuracy: 0.9934 - 110ms/epoch - 7ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.3929 - accuracy: 0.9237 - val_loss: 0.3904 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.3890 - accuracy: 0.9246 - val_loss: 0.3869 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.3868 - accuracy: 0.9259 - val_loss: 0.3837 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.3868 - accuracy: 0.9245 - val_loss: 0.3803 - val_accuracy: 0.9934 - 111ms/epoch - 7ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.3843 - accuracy: 0.9247 - val_loss: 0.3772 - val_accuracy: 0.9934 - 96ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.3818 - accuracy: 0.9234 - val_loss: 0.3743 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.3784 - accuracy: 0.9252 - val_loss: 0.3712 - val_accuracy: 0.9934 - 103ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.3778 - accuracy: 0.9245 - val_loss: 0.3681 - val_accuracy: 0.9934 - 117ms/epoch - 7ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.3720 - accuracy: 0.9250 - val_loss: 0.3651 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.3730 - accuracy: 0.9238 - val_loss: 0.3624 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.3699 - accuracy: 0.9250 - val_loss: 0.3598 - val_accuracy: 0.9934 - 96ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.3683 - accuracy: 0.9260 - val_loss: 0.3566 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.3660 - accuracy: 0.9253 - val_loss: 0.3538 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.3658 - accuracy: 0.9266 - val_loss: 0.3515 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.3668 - accuracy: 0.9249 - val_loss: 0.3492 - val_accuracy: 0.9934 - 99ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.3621 - accuracy: 0.9248 - val_loss: 0.3466 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.3586 - accuracy: 0.9246 - val_loss: 0.3442 - val_accuracy: 0.9934 - 101ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.3573 - accuracy: 0.9292 - val_loss: 0.3419 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.3590 - accuracy: 0.9245 - val_loss: 0.3391 - val_accuracy: 0.9934 - 97ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.3578 - accuracy: 0.9242 - val_loss: 0.3371 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.3533 - accuracy: 0.9251 - val_loss: 0.3349 - val_accuracy: 0.9934 - 95ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.3536 - accuracy: 0.9248 - val_loss: 0.3325 - val_accuracy: 0.9934 - 98ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.3517 - accuracy: 0.9229 - val_loss: 0.3303 - val_accuracy: 0.9934 - 96ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.3511 - accuracy: 0.9273 - val_loss: 0.3280 - val_accuracy: 0.9934 - 104ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.3499 - accuracy: 0.9405 - val_loss: 0.3259 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.3475 - accuracy: 0.9301 - val_loss: 0.3236 - val_accuracy: 0.9934 - 114ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.3457 - accuracy: 0.9468 - val_loss: 0.3219 - val_accuracy: 0.9934 - 108ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.3475 - accuracy: 0.9490 - val_loss: 0.3198 - val_accuracy: 0.9934 - 109ms/epoch - 7ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.3456 - accuracy: 0.9503 - val_loss: 0.3178 - val_accuracy: 0.9934 - 108ms/epoch - 7ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.3409 - accuracy: 0.9460 - val_loss: 0.3157 - val_accuracy: 0.9934 - 94ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.3425 - accuracy: 0.9475 - val_loss: 0.3140 - val_accuracy: 0.9934 - 104ms/epoch - 7ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.3409 - accuracy: 0.9526 - val_loss: 0.3120 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.3386 - accuracy: 0.9513 - val_loss: 0.3100 - val_accuracy: 0.9934 - 96ms/epoch - 6ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.3387 - accuracy: 0.9505 - val_loss: 0.3082 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.3380 - accuracy: 0.9504 - val_loss: 0.3063 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.3353 - accuracy: 0.9520 - val_loss: 0.3046 - val_accuracy: 0.9934 - 98ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.3341 - accuracy: 0.9527 - val_loss: 0.3027 - val_accuracy: 0.9934 - 101ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.3330 - accuracy: 0.9520 - val_loss: 0.3010 - val_accuracy: 0.9934 - 99ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.3368 - accuracy: 0.9523 - val_loss: 0.2995 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.3333 - accuracy: 0.9529 - val_loss: 0.2981 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.3315 - accuracy: 0.9535 - val_loss: 0.2967 - val_accuracy: 0.9934 - 103ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.3284 - accuracy: 0.9555 - val_loss: 0.2947 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 16)          544       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 16)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 577\n",
            "Trainable params: 577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8819 - accuracy: 0.7093 - val_loss: 0.8116 - val_accuracy: 0.9918 - 620ms/epoch - 19ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7503 - accuracy: 0.8132 - val_loss: 0.7086 - val_accuracy: 0.9923 - 139ms/epoch - 4ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6653 - accuracy: 0.8738 - val_loss: 0.6536 - val_accuracy: 0.9923 - 152ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6249 - accuracy: 0.8893 - val_loss: 0.6292 - val_accuracy: 0.9923 - 152ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.5982 - accuracy: 0.8889 - val_loss: 0.6051 - val_accuracy: 0.9923 - 141ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5711 - accuracy: 0.8911 - val_loss: 0.5788 - val_accuracy: 0.9923 - 148ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5428 - accuracy: 0.8917 - val_loss: 0.5508 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.5155 - accuracy: 0.8930 - val_loss: 0.5229 - val_accuracy: 0.9920 - 145ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.4880 - accuracy: 0.8944 - val_loss: 0.4957 - val_accuracy: 0.9920 - 149ms/epoch - 5ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.4634 - accuracy: 0.8964 - val_loss: 0.4703 - val_accuracy: 0.9920 - 136ms/epoch - 4ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.4416 - accuracy: 0.9024 - val_loss: 0.4478 - val_accuracy: 0.9920 - 135ms/epoch - 4ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.4223 - accuracy: 0.9172 - val_loss: 0.4275 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.4049 - accuracy: 0.9425 - val_loss: 0.4089 - val_accuracy: 0.9920 - 150ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.3904 - accuracy: 0.9586 - val_loss: 0.3933 - val_accuracy: 0.9920 - 157ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.3766 - accuracy: 0.9670 - val_loss: 0.3781 - val_accuracy: 0.9920 - 140ms/epoch - 4ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.3640 - accuracy: 0.9693 - val_loss: 0.3650 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.3543 - accuracy: 0.9704 - val_loss: 0.3532 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.3440 - accuracy: 0.9707 - val_loss: 0.3425 - val_accuracy: 0.9849 - 146ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.3340 - accuracy: 0.9710 - val_loss: 0.3323 - val_accuracy: 0.9849 - 143ms/epoch - 4ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.3275 - accuracy: 0.9691 - val_loss: 0.3236 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.3182 - accuracy: 0.9720 - val_loss: 0.3141 - val_accuracy: 0.9849 - 144ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.3103 - accuracy: 0.9721 - val_loss: 0.3061 - val_accuracy: 0.9849 - 156ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.3041 - accuracy: 0.9721 - val_loss: 0.3001 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.2980 - accuracy: 0.9721 - val_loss: 0.2918 - val_accuracy: 0.9849 - 140ms/epoch - 4ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.2917 - accuracy: 0.9720 - val_loss: 0.2858 - val_accuracy: 0.9849 - 134ms/epoch - 4ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.2858 - accuracy: 0.9722 - val_loss: 0.2789 - val_accuracy: 0.9849 - 137ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.2825 - accuracy: 0.9727 - val_loss: 0.2727 - val_accuracy: 0.9849 - 135ms/epoch - 4ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.2754 - accuracy: 0.9747 - val_loss: 0.2666 - val_accuracy: 0.9849 - 142ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.2709 - accuracy: 0.9735 - val_loss: 0.2618 - val_accuracy: 0.9920 - 138ms/epoch - 4ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.2667 - accuracy: 0.9761 - val_loss: 0.2567 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.2641 - accuracy: 0.9749 - val_loss: 0.2512 - val_accuracy: 0.9920 - 158ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2613 - accuracy: 0.9742 - val_loss: 0.2472 - val_accuracy: 0.9920 - 150ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2588 - accuracy: 0.9751 - val_loss: 0.2421 - val_accuracy: 0.9920 - 143ms/epoch - 4ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2536 - accuracy: 0.9749 - val_loss: 0.2387 - val_accuracy: 0.9923 - 139ms/epoch - 4ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2528 - accuracy: 0.9752 - val_loss: 0.2350 - val_accuracy: 0.9934 - 144ms/epoch - 4ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2489 - accuracy: 0.9746 - val_loss: 0.2311 - val_accuracy: 0.9934 - 150ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2502 - accuracy: 0.9739 - val_loss: 0.2278 - val_accuracy: 0.9934 - 140ms/epoch - 4ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2456 - accuracy: 0.9744 - val_loss: 0.2244 - val_accuracy: 0.9934 - 153ms/epoch - 5ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2445 - accuracy: 0.9744 - val_loss: 0.2215 - val_accuracy: 0.9973 - 138ms/epoch - 4ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2419 - accuracy: 0.9738 - val_loss: 0.2185 - val_accuracy: 0.9973 - 148ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2429 - accuracy: 0.9739 - val_loss: 0.2166 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2409 - accuracy: 0.9736 - val_loss: 0.2139 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2387 - accuracy: 0.9735 - val_loss: 0.2120 - val_accuracy: 0.9973 - 150ms/epoch - 5ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2386 - accuracy: 0.9742 - val_loss: 0.2106 - val_accuracy: 0.9973 - 135ms/epoch - 4ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2359 - accuracy: 0.9738 - val_loss: 0.2080 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2369 - accuracy: 0.9734 - val_loss: 0.2064 - val_accuracy: 0.9973 - 153ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2332 - accuracy: 0.9742 - val_loss: 0.2048 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2337 - accuracy: 0.9748 - val_loss: 0.2034 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2317 - accuracy: 0.9737 - val_loss: 0.2018 - val_accuracy: 0.9973 - 136ms/epoch - 4ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2320 - accuracy: 0.9747 - val_loss: 0.2013 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2317 - accuracy: 0.9742 - val_loss: 0.2001 - val_accuracy: 0.9973 - 144ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2308 - accuracy: 0.9753 - val_loss: 0.1982 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2288 - accuracy: 0.9751 - val_loss: 0.1977 - val_accuracy: 0.9973 - 140ms/epoch - 4ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2265 - accuracy: 0.9769 - val_loss: 0.1968 - val_accuracy: 0.9973 - 152ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2263 - accuracy: 0.9768 - val_loss: 0.1962 - val_accuracy: 0.9973 - 139ms/epoch - 4ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2254 - accuracy: 0.9777 - val_loss: 0.1940 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2261 - accuracy: 0.9768 - val_loss: 0.1934 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2239 - accuracy: 0.9758 - val_loss: 0.1928 - val_accuracy: 0.9973 - 153ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2233 - accuracy: 0.9779 - val_loss: 0.1922 - val_accuracy: 0.9973 - 144ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2235 - accuracy: 0.9785 - val_loss: 0.1912 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2239 - accuracy: 0.9791 - val_loss: 0.1904 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2213 - accuracy: 0.9783 - val_loss: 0.1901 - val_accuracy: 0.9973 - 134ms/epoch - 4ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2198 - accuracy: 0.9794 - val_loss: 0.1895 - val_accuracy: 0.9973 - 138ms/epoch - 4ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2209 - accuracy: 0.9794 - val_loss: 0.1886 - val_accuracy: 0.9973 - 145ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2189 - accuracy: 0.9798 - val_loss: 0.1889 - val_accuracy: 0.9973 - 152ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2180 - accuracy: 0.9799 - val_loss: 0.1879 - val_accuracy: 0.9973 - 137ms/epoch - 4ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2188 - accuracy: 0.9796 - val_loss: 0.1872 - val_accuracy: 0.9973 - 139ms/epoch - 4ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2177 - accuracy: 0.9796 - val_loss: 0.1861 - val_accuracy: 0.9973 - 140ms/epoch - 4ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2187 - accuracy: 0.9790 - val_loss: 0.1855 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2151 - accuracy: 0.9804 - val_loss: 0.1847 - val_accuracy: 0.9973 - 146ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2170 - accuracy: 0.9791 - val_loss: 0.1849 - val_accuracy: 0.9973 - 136ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2145 - accuracy: 0.9800 - val_loss: 0.1838 - val_accuracy: 0.9973 - 146ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2155 - accuracy: 0.9795 - val_loss: 0.1829 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2141 - accuracy: 0.9800 - val_loss: 0.1835 - val_accuracy: 0.9973 - 139ms/epoch - 4ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2129 - accuracy: 0.9804 - val_loss: 0.1819 - val_accuracy: 0.9973 - 136ms/epoch - 4ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.2114 - accuracy: 0.9805 - val_loss: 0.1803 - val_accuracy: 0.9973 - 140ms/epoch - 4ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.2133 - accuracy: 0.9801 - val_loss: 0.1813 - val_accuracy: 0.9973 - 138ms/epoch - 4ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.2107 - accuracy: 0.9801 - val_loss: 0.1789 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.2111 - accuracy: 0.9794 - val_loss: 0.1798 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.2093 - accuracy: 0.9808 - val_loss: 0.1786 - val_accuracy: 0.9973 - 152ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.2088 - accuracy: 0.9809 - val_loss: 0.1785 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.2102 - accuracy: 0.9790 - val_loss: 0.1781 - val_accuracy: 0.9973 - 150ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.2109 - accuracy: 0.9798 - val_loss: 0.1785 - val_accuracy: 0.9978 - 135ms/epoch - 4ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.2069 - accuracy: 0.9810 - val_loss: 0.1773 - val_accuracy: 0.9978 - 135ms/epoch - 4ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.2071 - accuracy: 0.9808 - val_loss: 0.1763 - val_accuracy: 0.9978 - 146ms/epoch - 5ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.2075 - accuracy: 0.9801 - val_loss: 0.1755 - val_accuracy: 0.9978 - 153ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.2042 - accuracy: 0.9804 - val_loss: 0.1746 - val_accuracy: 0.9978 - 139ms/epoch - 4ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.2067 - accuracy: 0.9804 - val_loss: 0.1740 - val_accuracy: 0.9978 - 145ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.2040 - accuracy: 0.9801 - val_loss: 0.1741 - val_accuracy: 0.9978 - 136ms/epoch - 4ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.2050 - accuracy: 0.9805 - val_loss: 0.1721 - val_accuracy: 0.9978 - 150ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.2034 - accuracy: 0.9802 - val_loss: 0.1717 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.2014 - accuracy: 0.9809 - val_loss: 0.1715 - val_accuracy: 0.9978 - 146ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.2025 - accuracy: 0.9803 - val_loss: 0.1716 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.2052 - accuracy: 0.9798 - val_loss: 0.1711 - val_accuracy: 0.9978 - 172ms/epoch - 5ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.2022 - accuracy: 0.9800 - val_loss: 0.1705 - val_accuracy: 0.9978 - 143ms/epoch - 4ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.2001 - accuracy: 0.9812 - val_loss: 0.1679 - val_accuracy: 0.9978 - 145ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.1993 - accuracy: 0.9809 - val_loss: 0.1683 - val_accuracy: 0.9978 - 146ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.1999 - accuracy: 0.9804 - val_loss: 0.1677 - val_accuracy: 0.9978 - 162ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.1982 - accuracy: 0.9810 - val_loss: 0.1665 - val_accuracy: 0.9978 - 144ms/epoch - 4ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.2004 - accuracy: 0.9803 - val_loss: 0.1672 - val_accuracy: 0.9978 - 142ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 16)          544       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 16)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 577\n",
            "Trainable params: 577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.9221 - accuracy: 0.7292 - val_loss: 0.8801 - val_accuracy: 0.9915 - 809ms/epoch - 51ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8432 - accuracy: 0.8784 - val_loss: 0.8139 - val_accuracy: 0.9975 - 101ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7801 - accuracy: 0.8934 - val_loss: 0.7601 - val_accuracy: 0.9975 - 113ms/epoch - 7ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7299 - accuracy: 0.9022 - val_loss: 0.7190 - val_accuracy: 0.9975 - 101ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6913 - accuracy: 0.9005 - val_loss: 0.6884 - val_accuracy: 0.9531 - 107ms/epoch - 7ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6641 - accuracy: 0.8935 - val_loss: 0.6696 - val_accuracy: 0.9531 - 103ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6471 - accuracy: 0.8725 - val_loss: 0.6587 - val_accuracy: 0.9525 - 113ms/epoch - 7ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6345 - accuracy: 0.8689 - val_loss: 0.6484 - val_accuracy: 0.9490 - 118ms/epoch - 7ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6218 - accuracy: 0.8714 - val_loss: 0.6379 - val_accuracy: 0.9490 - 100ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.6082 - accuracy: 0.8776 - val_loss: 0.6270 - val_accuracy: 0.9931 - 109ms/epoch - 7ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5937 - accuracy: 0.8910 - val_loss: 0.6152 - val_accuracy: 0.9931 - 99ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5795 - accuracy: 0.8977 - val_loss: 0.6028 - val_accuracy: 0.9931 - 114ms/epoch - 7ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5643 - accuracy: 0.9155 - val_loss: 0.5899 - val_accuracy: 0.9931 - 100ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.5494 - accuracy: 0.9352 - val_loss: 0.5765 - val_accuracy: 0.9931 - 117ms/epoch - 7ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.5328 - accuracy: 0.9481 - val_loss: 0.5625 - val_accuracy: 0.9931 - 103ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.5176 - accuracy: 0.9594 - val_loss: 0.5488 - val_accuracy: 0.9931 - 114ms/epoch - 7ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.5040 - accuracy: 0.9662 - val_loss: 0.5351 - val_accuracy: 0.9931 - 113ms/epoch - 7ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4879 - accuracy: 0.9696 - val_loss: 0.5215 - val_accuracy: 0.9931 - 113ms/epoch - 7ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4746 - accuracy: 0.9708 - val_loss: 0.5082 - val_accuracy: 0.9931 - 106ms/epoch - 7ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4602 - accuracy: 0.9713 - val_loss: 0.4951 - val_accuracy: 0.9931 - 114ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.4482 - accuracy: 0.9718 - val_loss: 0.4825 - val_accuracy: 0.9920 - 111ms/epoch - 7ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.4347 - accuracy: 0.9734 - val_loss: 0.4706 - val_accuracy: 0.9920 - 111ms/epoch - 7ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.4239 - accuracy: 0.9741 - val_loss: 0.4591 - val_accuracy: 0.9920 - 100ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.4134 - accuracy: 0.9749 - val_loss: 0.4482 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.4028 - accuracy: 0.9749 - val_loss: 0.4375 - val_accuracy: 0.9920 - 99ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.3928 - accuracy: 0.9747 - val_loss: 0.4275 - val_accuracy: 0.9920 - 113ms/epoch - 7ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.3842 - accuracy: 0.9745 - val_loss: 0.4174 - val_accuracy: 0.9920 - 101ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.3737 - accuracy: 0.9743 - val_loss: 0.4079 - val_accuracy: 0.9931 - 114ms/epoch - 7ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.3660 - accuracy: 0.9736 - val_loss: 0.3984 - val_accuracy: 0.9931 - 116ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3587 - accuracy: 0.9724 - val_loss: 0.3895 - val_accuracy: 0.9931 - 111ms/epoch - 7ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3501 - accuracy: 0.9738 - val_loss: 0.3812 - val_accuracy: 0.9934 - 101ms/epoch - 6ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3449 - accuracy: 0.9734 - val_loss: 0.3730 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3392 - accuracy: 0.9727 - val_loss: 0.3653 - val_accuracy: 0.9934 - 100ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3316 - accuracy: 0.9742 - val_loss: 0.3582 - val_accuracy: 0.9934 - 116ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.3265 - accuracy: 0.9736 - val_loss: 0.3512 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.3209 - accuracy: 0.9753 - val_loss: 0.3445 - val_accuracy: 0.9934 - 108ms/epoch - 7ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.3172 - accuracy: 0.9738 - val_loss: 0.3384 - val_accuracy: 0.9934 - 102ms/epoch - 6ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.3121 - accuracy: 0.9746 - val_loss: 0.3322 - val_accuracy: 0.9934 - 106ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.3080 - accuracy: 0.9754 - val_loss: 0.3268 - val_accuracy: 0.9970 - 107ms/epoch - 7ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.3038 - accuracy: 0.9751 - val_loss: 0.3212 - val_accuracy: 0.9970 - 108ms/epoch - 7ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.3012 - accuracy: 0.9746 - val_loss: 0.3162 - val_accuracy: 0.9970 - 100ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.2973 - accuracy: 0.9752 - val_loss: 0.3111 - val_accuracy: 0.9970 - 112ms/epoch - 7ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.2944 - accuracy: 0.9750 - val_loss: 0.3064 - val_accuracy: 0.9970 - 100ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.2917 - accuracy: 0.9738 - val_loss: 0.3010 - val_accuracy: 0.9970 - 117ms/epoch - 7ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.2875 - accuracy: 0.9756 - val_loss: 0.2971 - val_accuracy: 0.9970 - 100ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.2858 - accuracy: 0.9756 - val_loss: 0.2928 - val_accuracy: 0.9973 - 114ms/epoch - 7ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.2818 - accuracy: 0.9746 - val_loss: 0.2883 - val_accuracy: 0.9970 - 109ms/epoch - 7ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.2785 - accuracy: 0.9761 - val_loss: 0.2844 - val_accuracy: 0.9973 - 99ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.2766 - accuracy: 0.9760 - val_loss: 0.2811 - val_accuracy: 0.9973 - 106ms/epoch - 7ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.2742 - accuracy: 0.9759 - val_loss: 0.2771 - val_accuracy: 0.9973 - 110ms/epoch - 7ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.2735 - accuracy: 0.9763 - val_loss: 0.2738 - val_accuracy: 0.9973 - 106ms/epoch - 7ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.2723 - accuracy: 0.9751 - val_loss: 0.2704 - val_accuracy: 0.9973 - 99ms/epoch - 6ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.2672 - accuracy: 0.9768 - val_loss: 0.2667 - val_accuracy: 0.9973 - 103ms/epoch - 6ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.2666 - accuracy: 0.9764 - val_loss: 0.2636 - val_accuracy: 0.9973 - 101ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.2628 - accuracy: 0.9763 - val_loss: 0.2610 - val_accuracy: 0.9973 - 109ms/epoch - 7ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.2612 - accuracy: 0.9768 - val_loss: 0.2575 - val_accuracy: 0.9973 - 107ms/epoch - 7ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.2604 - accuracy: 0.9761 - val_loss: 0.2543 - val_accuracy: 0.9973 - 112ms/epoch - 7ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.2580 - accuracy: 0.9764 - val_loss: 0.2517 - val_accuracy: 0.9973 - 110ms/epoch - 7ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.2572 - accuracy: 0.9763 - val_loss: 0.2491 - val_accuracy: 0.9973 - 107ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.2556 - accuracy: 0.9767 - val_loss: 0.2466 - val_accuracy: 0.9973 - 101ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.2534 - accuracy: 0.9763 - val_loss: 0.2442 - val_accuracy: 0.9973 - 105ms/epoch - 7ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.2527 - accuracy: 0.9768 - val_loss: 0.2412 - val_accuracy: 0.9973 - 101ms/epoch - 6ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.2513 - accuracy: 0.9764 - val_loss: 0.2390 - val_accuracy: 0.9973 - 110ms/epoch - 7ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.2476 - accuracy: 0.9770 - val_loss: 0.2367 - val_accuracy: 0.9978 - 109ms/epoch - 7ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.2480 - accuracy: 0.9763 - val_loss: 0.2342 - val_accuracy: 0.9973 - 113ms/epoch - 7ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.2455 - accuracy: 0.9764 - val_loss: 0.2323 - val_accuracy: 0.9978 - 101ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2454 - accuracy: 0.9758 - val_loss: 0.2304 - val_accuracy: 0.9978 - 119ms/epoch - 7ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2453 - accuracy: 0.9756 - val_loss: 0.2288 - val_accuracy: 0.9978 - 101ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2430 - accuracy: 0.9764 - val_loss: 0.2271 - val_accuracy: 0.9978 - 112ms/epoch - 7ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2407 - accuracy: 0.9777 - val_loss: 0.2249 - val_accuracy: 0.9978 - 108ms/epoch - 7ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2423 - accuracy: 0.9769 - val_loss: 0.2237 - val_accuracy: 0.9978 - 117ms/epoch - 7ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2371 - accuracy: 0.9779 - val_loss: 0.2219 - val_accuracy: 0.9978 - 99ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2371 - accuracy: 0.9762 - val_loss: 0.2207 - val_accuracy: 0.9978 - 110ms/epoch - 7ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2377 - accuracy: 0.9771 - val_loss: 0.2189 - val_accuracy: 0.9978 - 105ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2362 - accuracy: 0.9763 - val_loss: 0.2177 - val_accuracy: 0.9978 - 108ms/epoch - 7ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2364 - accuracy: 0.9769 - val_loss: 0.2165 - val_accuracy: 0.9978 - 111ms/epoch - 7ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2338 - accuracy: 0.9767 - val_loss: 0.2154 - val_accuracy: 0.9978 - 118ms/epoch - 7ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2334 - accuracy: 0.9763 - val_loss: 0.2139 - val_accuracy: 0.9978 - 105ms/epoch - 7ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2309 - accuracy: 0.9768 - val_loss: 0.2130 - val_accuracy: 0.9978 - 103ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2310 - accuracy: 0.9772 - val_loss: 0.2111 - val_accuracy: 0.9978 - 105ms/epoch - 7ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2303 - accuracy: 0.9775 - val_loss: 0.2107 - val_accuracy: 0.9978 - 107ms/epoch - 7ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2290 - accuracy: 0.9764 - val_loss: 0.2088 - val_accuracy: 0.9978 - 104ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2280 - accuracy: 0.9768 - val_loss: 0.2081 - val_accuracy: 0.9978 - 114ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2297 - accuracy: 0.9759 - val_loss: 0.2068 - val_accuracy: 0.9978 - 99ms/epoch - 6ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2284 - accuracy: 0.9769 - val_loss: 0.2065 - val_accuracy: 0.9978 - 118ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2254 - accuracy: 0.9781 - val_loss: 0.2049 - val_accuracy: 0.9978 - 121ms/epoch - 8ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2260 - accuracy: 0.9767 - val_loss: 0.2039 - val_accuracy: 0.9978 - 107ms/epoch - 7ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2253 - accuracy: 0.9770 - val_loss: 0.2029 - val_accuracy: 0.9978 - 107ms/epoch - 7ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2257 - accuracy: 0.9764 - val_loss: 0.2023 - val_accuracy: 0.9978 - 105ms/epoch - 7ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2223 - accuracy: 0.9770 - val_loss: 0.2015 - val_accuracy: 0.9978 - 108ms/epoch - 7ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2226 - accuracy: 0.9770 - val_loss: 0.2002 - val_accuracy: 0.9978 - 104ms/epoch - 7ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2213 - accuracy: 0.9768 - val_loss: 0.2000 - val_accuracy: 0.9978 - 103ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2196 - accuracy: 0.9770 - val_loss: 0.1987 - val_accuracy: 0.9978 - 99ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2223 - accuracy: 0.9771 - val_loss: 0.1988 - val_accuracy: 0.9978 - 111ms/epoch - 7ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2195 - accuracy: 0.9775 - val_loss: 0.1974 - val_accuracy: 0.9978 - 113ms/epoch - 7ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2171 - accuracy: 0.9780 - val_loss: 0.1970 - val_accuracy: 0.9978 - 102ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2175 - accuracy: 0.9771 - val_loss: 0.1966 - val_accuracy: 0.9978 - 108ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2202 - accuracy: 0.9757 - val_loss: 0.1954 - val_accuracy: 0.9978 - 102ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2182 - accuracy: 0.9769 - val_loss: 0.1960 - val_accuracy: 0.9978 - 114ms/epoch - 7ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2173 - accuracy: 0.9775 - val_loss: 0.1947 - val_accuracy: 0.9978 - 101ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 16)          544       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 16)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 561\n",
            "Trainable params: 561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.8831 - accuracy: 0.7132 - val_loss: 0.8158 - val_accuracy: 0.8428 - 613ms/epoch - 19ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7539 - accuracy: 0.8560 - val_loss: 0.7187 - val_accuracy: 0.8428 - 159ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6740 - accuracy: 0.8777 - val_loss: 0.6699 - val_accuracy: 0.9503 - 158ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6392 - accuracy: 0.8934 - val_loss: 0.6522 - val_accuracy: 0.9503 - 142ms/epoch - 4ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6180 - accuracy: 0.9025 - val_loss: 0.6358 - val_accuracy: 0.9503 - 144ms/epoch - 4ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5965 - accuracy: 0.9167 - val_loss: 0.6185 - val_accuracy: 0.9498 - 150ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5750 - accuracy: 0.9252 - val_loss: 0.6005 - val_accuracy: 0.9498 - 145ms/epoch - 5ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.5533 - accuracy: 0.9315 - val_loss: 0.5819 - val_accuracy: 0.9498 - 154ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.5319 - accuracy: 0.9392 - val_loss: 0.5634 - val_accuracy: 0.9498 - 142ms/epoch - 4ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.5123 - accuracy: 0.9446 - val_loss: 0.5448 - val_accuracy: 0.9517 - 237ms/epoch - 7ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.4939 - accuracy: 0.9485 - val_loss: 0.5270 - val_accuracy: 0.9973 - 218ms/epoch - 7ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.4735 - accuracy: 0.9542 - val_loss: 0.5091 - val_accuracy: 0.9973 - 224ms/epoch - 7ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.4593 - accuracy: 0.9539 - val_loss: 0.4925 - val_accuracy: 0.9973 - 228ms/epoch - 7ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.4428 - accuracy: 0.9575 - val_loss: 0.4769 - val_accuracy: 0.9973 - 216ms/epoch - 7ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.4265 - accuracy: 0.9606 - val_loss: 0.4619 - val_accuracy: 0.9973 - 219ms/epoch - 7ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.4144 - accuracy: 0.9605 - val_loss: 0.4478 - val_accuracy: 0.9973 - 231ms/epoch - 7ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.4013 - accuracy: 0.9654 - val_loss: 0.4343 - val_accuracy: 0.9973 - 176ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.3907 - accuracy: 0.9665 - val_loss: 0.4217 - val_accuracy: 0.9973 - 140ms/epoch - 4ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.3804 - accuracy: 0.9678 - val_loss: 0.4099 - val_accuracy: 0.9973 - 145ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.3717 - accuracy: 0.9687 - val_loss: 0.3991 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.3623 - accuracy: 0.9698 - val_loss: 0.3882 - val_accuracy: 0.9973 - 152ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.3550 - accuracy: 0.9686 - val_loss: 0.3785 - val_accuracy: 0.9973 - 139ms/epoch - 4ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.3475 - accuracy: 0.9691 - val_loss: 0.3689 - val_accuracy: 0.9973 - 150ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.3383 - accuracy: 0.9700 - val_loss: 0.3602 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.3333 - accuracy: 0.9700 - val_loss: 0.3518 - val_accuracy: 0.9973 - 145ms/epoch - 5ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.3286 - accuracy: 0.9686 - val_loss: 0.3438 - val_accuracy: 0.9973 - 141ms/epoch - 4ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.3217 - accuracy: 0.9694 - val_loss: 0.3367 - val_accuracy: 0.9973 - 153ms/epoch - 5ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.3169 - accuracy: 0.9697 - val_loss: 0.3295 - val_accuracy: 0.9973 - 140ms/epoch - 4ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.3111 - accuracy: 0.9703 - val_loss: 0.3221 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.3071 - accuracy: 0.9699 - val_loss: 0.3161 - val_accuracy: 0.9973 - 150ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.3024 - accuracy: 0.9688 - val_loss: 0.3096 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2971 - accuracy: 0.9706 - val_loss: 0.3040 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2951 - accuracy: 0.9706 - val_loss: 0.2981 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2902 - accuracy: 0.9704 - val_loss: 0.2928 - val_accuracy: 0.9978 - 147ms/epoch - 5ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2867 - accuracy: 0.9714 - val_loss: 0.2876 - val_accuracy: 0.9978 - 150ms/epoch - 5ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2848 - accuracy: 0.9721 - val_loss: 0.2835 - val_accuracy: 0.9978 - 170ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2805 - accuracy: 0.9727 - val_loss: 0.2787 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2774 - accuracy: 0.9736 - val_loss: 0.2743 - val_accuracy: 0.9978 - 143ms/epoch - 4ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2780 - accuracy: 0.9721 - val_loss: 0.2708 - val_accuracy: 0.9978 - 145ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2737 - accuracy: 0.9734 - val_loss: 0.2675 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2711 - accuracy: 0.9738 - val_loss: 0.2636 - val_accuracy: 0.9978 - 172ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2699 - accuracy: 0.9746 - val_loss: 0.2605 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2675 - accuracy: 0.9760 - val_loss: 0.2572 - val_accuracy: 0.9978 - 158ms/epoch - 5ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2668 - accuracy: 0.9754 - val_loss: 0.2552 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2646 - accuracy: 0.9757 - val_loss: 0.2516 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2621 - accuracy: 0.9754 - val_loss: 0.2495 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2624 - accuracy: 0.9741 - val_loss: 0.2470 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2576 - accuracy: 0.9761 - val_loss: 0.2437 - val_accuracy: 0.9978 - 150ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2583 - accuracy: 0.9757 - val_loss: 0.2420 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2532 - accuracy: 0.9764 - val_loss: 0.2401 - val_accuracy: 0.9978 - 162ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2540 - accuracy: 0.9756 - val_loss: 0.2381 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2531 - accuracy: 0.9752 - val_loss: 0.2359 - val_accuracy: 0.9978 - 147ms/epoch - 5ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2498 - accuracy: 0.9753 - val_loss: 0.2335 - val_accuracy: 0.9978 - 148ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2510 - accuracy: 0.9751 - val_loss: 0.2317 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2492 - accuracy: 0.9753 - val_loss: 0.2300 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2464 - accuracy: 0.9749 - val_loss: 0.2279 - val_accuracy: 0.9978 - 146ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2445 - accuracy: 0.9751 - val_loss: 0.2273 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2450 - accuracy: 0.9751 - val_loss: 0.2246 - val_accuracy: 0.9978 - 150ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2413 - accuracy: 0.9761 - val_loss: 0.2229 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2423 - accuracy: 0.9748 - val_loss: 0.2213 - val_accuracy: 0.9978 - 144ms/epoch - 5ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2399 - accuracy: 0.9752 - val_loss: 0.2205 - val_accuracy: 0.9978 - 152ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2372 - accuracy: 0.9761 - val_loss: 0.2184 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2378 - accuracy: 0.9766 - val_loss: 0.2170 - val_accuracy: 0.9978 - 163ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2365 - accuracy: 0.9759 - val_loss: 0.2154 - val_accuracy: 0.9978 - 161ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2356 - accuracy: 0.9751 - val_loss: 0.2142 - val_accuracy: 0.9978 - 141ms/epoch - 4ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2353 - accuracy: 0.9747 - val_loss: 0.2127 - val_accuracy: 0.9978 - 142ms/epoch - 4ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2329 - accuracy: 0.9758 - val_loss: 0.2118 - val_accuracy: 0.9978 - 167ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2330 - accuracy: 0.9760 - val_loss: 0.2100 - val_accuracy: 0.9978 - 162ms/epoch - 5ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2299 - accuracy: 0.9772 - val_loss: 0.2081 - val_accuracy: 0.9978 - 159ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2311 - accuracy: 0.9747 - val_loss: 0.2071 - val_accuracy: 0.9978 - 160ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2310 - accuracy: 0.9756 - val_loss: 0.2060 - val_accuracy: 0.9978 - 142ms/epoch - 4ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2297 - accuracy: 0.9757 - val_loss: 0.2047 - val_accuracy: 0.9978 - 153ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2251 - accuracy: 0.9765 - val_loss: 0.2033 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2265 - accuracy: 0.9767 - val_loss: 0.2021 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2283 - accuracy: 0.9755 - val_loss: 0.2014 - val_accuracy: 0.9978 - 153ms/epoch - 5ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.2254 - accuracy: 0.9769 - val_loss: 0.1998 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.2274 - accuracy: 0.9745 - val_loss: 0.1990 - val_accuracy: 0.9978 - 145ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.2267 - accuracy: 0.9755 - val_loss: 0.1980 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.2248 - accuracy: 0.9755 - val_loss: 0.1975 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.2233 - accuracy: 0.9768 - val_loss: 0.1961 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.2237 - accuracy: 0.9757 - val_loss: 0.1951 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.2223 - accuracy: 0.9760 - val_loss: 0.1950 - val_accuracy: 0.9978 - 160ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.2215 - accuracy: 0.9765 - val_loss: 0.1930 - val_accuracy: 0.9978 - 149ms/epoch - 5ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.2221 - accuracy: 0.9765 - val_loss: 0.1917 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.2190 - accuracy: 0.9769 - val_loss: 0.1915 - val_accuracy: 0.9978 - 142ms/epoch - 4ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.2196 - accuracy: 0.9763 - val_loss: 0.1904 - val_accuracy: 0.9978 - 151ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.2198 - accuracy: 0.9764 - val_loss: 0.1895 - val_accuracy: 0.9978 - 173ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.2183 - accuracy: 0.9764 - val_loss: 0.1886 - val_accuracy: 0.9978 - 160ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.2196 - accuracy: 0.9761 - val_loss: 0.1884 - val_accuracy: 0.9978 - 148ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.2182 - accuracy: 0.9762 - val_loss: 0.1875 - val_accuracy: 0.9978 - 150ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.2167 - accuracy: 0.9772 - val_loss: 0.1866 - val_accuracy: 0.9978 - 139ms/epoch - 4ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.2161 - accuracy: 0.9773 - val_loss: 0.1859 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.2146 - accuracy: 0.9768 - val_loss: 0.1849 - val_accuracy: 0.9978 - 155ms/epoch - 5ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.2166 - accuracy: 0.9764 - val_loss: 0.1844 - val_accuracy: 0.9978 - 142ms/epoch - 4ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.2154 - accuracy: 0.9768 - val_loss: 0.1836 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.2124 - accuracy: 0.9782 - val_loss: 0.1829 - val_accuracy: 0.9978 - 152ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.2156 - accuracy: 0.9766 - val_loss: 0.1820 - val_accuracy: 0.9978 - 144ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.2153 - accuracy: 0.9761 - val_loss: 0.1817 - val_accuracy: 0.9978 - 168ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.2136 - accuracy: 0.9767 - val_loss: 0.1813 - val_accuracy: 0.9978 - 154ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.2119 - accuracy: 0.9761 - val_loss: 0.1807 - val_accuracy: 0.9978 - 140ms/epoch - 4ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 16)          544       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 16)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 16)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 16)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 561\n",
            "Trainable params: 561\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 0.9262 - accuracy: 0.4788 - val_loss: 0.8953 - val_accuracy: 0.4577 - 613ms/epoch - 38ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8458 - accuracy: 0.5757 - val_loss: 0.8227 - val_accuracy: 0.5875 - 110ms/epoch - 7ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7761 - accuracy: 0.8317 - val_loss: 0.7609 - val_accuracy: 0.9690 - 109ms/epoch - 7ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7178 - accuracy: 0.8828 - val_loss: 0.7120 - val_accuracy: 0.9679 - 112ms/epoch - 7ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6720 - accuracy: 0.8772 - val_loss: 0.6757 - val_accuracy: 0.9690 - 106ms/epoch - 7ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6381 - accuracy: 0.8798 - val_loss: 0.6511 - val_accuracy: 0.9690 - 111ms/epoch - 7ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6165 - accuracy: 0.8799 - val_loss: 0.6383 - val_accuracy: 0.9679 - 108ms/epoch - 7ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6022 - accuracy: 0.8825 - val_loss: 0.6267 - val_accuracy: 0.9679 - 123ms/epoch - 8ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.5891 - accuracy: 0.8824 - val_loss: 0.6157 - val_accuracy: 0.9679 - 108ms/epoch - 7ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.5764 - accuracy: 0.8821 - val_loss: 0.6047 - val_accuracy: 0.9693 - 121ms/epoch - 8ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5639 - accuracy: 0.8836 - val_loss: 0.5934 - val_accuracy: 0.9698 - 125ms/epoch - 8ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5511 - accuracy: 0.8842 - val_loss: 0.5821 - val_accuracy: 0.9701 - 110ms/epoch - 7ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5394 - accuracy: 0.8843 - val_loss: 0.5709 - val_accuracy: 0.9701 - 109ms/epoch - 7ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.5275 - accuracy: 0.8862 - val_loss: 0.5595 - val_accuracy: 0.9701 - 111ms/epoch - 7ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.5147 - accuracy: 0.8868 - val_loss: 0.5481 - val_accuracy: 0.9701 - 107ms/epoch - 7ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.5037 - accuracy: 0.8885 - val_loss: 0.5370 - val_accuracy: 0.9701 - 115ms/epoch - 7ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.4937 - accuracy: 0.8894 - val_loss: 0.5262 - val_accuracy: 0.9701 - 106ms/epoch - 7ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4833 - accuracy: 0.8919 - val_loss: 0.5157 - val_accuracy: 0.9701 - 115ms/epoch - 7ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4733 - accuracy: 0.8939 - val_loss: 0.5053 - val_accuracy: 0.9701 - 123ms/epoch - 8ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4637 - accuracy: 0.8951 - val_loss: 0.4951 - val_accuracy: 0.9923 - 105ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.4547 - accuracy: 0.8974 - val_loss: 0.4854 - val_accuracy: 0.9923 - 109ms/epoch - 7ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.4469 - accuracy: 0.8981 - val_loss: 0.4759 - val_accuracy: 0.9923 - 105ms/epoch - 7ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.4388 - accuracy: 0.9006 - val_loss: 0.4670 - val_accuracy: 0.9923 - 121ms/epoch - 8ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.4311 - accuracy: 0.9022 - val_loss: 0.4585 - val_accuracy: 0.9923 - 105ms/epoch - 7ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.4247 - accuracy: 0.9022 - val_loss: 0.4502 - val_accuracy: 0.9923 - 111ms/epoch - 7ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.4185 - accuracy: 0.9038 - val_loss: 0.4423 - val_accuracy: 0.9923 - 105ms/epoch - 7ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.4111 - accuracy: 0.9053 - val_loss: 0.4350 - val_accuracy: 0.9923 - 120ms/epoch - 8ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.4062 - accuracy: 0.9056 - val_loss: 0.4277 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.4002 - accuracy: 0.9059 - val_loss: 0.4209 - val_accuracy: 0.9920 - 115ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3955 - accuracy: 0.9067 - val_loss: 0.4144 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3913 - accuracy: 0.9072 - val_loss: 0.4082 - val_accuracy: 0.9920 - 114ms/epoch - 7ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3851 - accuracy: 0.9077 - val_loss: 0.4020 - val_accuracy: 0.9920 - 104ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3814 - accuracy: 0.9074 - val_loss: 0.3962 - val_accuracy: 0.9920 - 118ms/epoch - 7ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3772 - accuracy: 0.9082 - val_loss: 0.3908 - val_accuracy: 0.9920 - 109ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.3732 - accuracy: 0.9083 - val_loss: 0.3858 - val_accuracy: 0.9920 - 106ms/epoch - 7ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.3692 - accuracy: 0.9086 - val_loss: 0.3800 - val_accuracy: 0.9920 - 114ms/epoch - 7ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.3662 - accuracy: 0.9086 - val_loss: 0.3755 - val_accuracy: 0.9920 - 112ms/epoch - 7ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.3622 - accuracy: 0.9079 - val_loss: 0.3712 - val_accuracy: 0.9920 - 117ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.3588 - accuracy: 0.9092 - val_loss: 0.3663 - val_accuracy: 0.9920 - 105ms/epoch - 7ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.3565 - accuracy: 0.9148 - val_loss: 0.3621 - val_accuracy: 0.9920 - 122ms/epoch - 8ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.3538 - accuracy: 0.9172 - val_loss: 0.3584 - val_accuracy: 0.9920 - 103ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.3500 - accuracy: 0.9303 - val_loss: 0.3540 - val_accuracy: 0.9920 - 121ms/epoch - 8ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.3474 - accuracy: 0.9403 - val_loss: 0.3509 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.3462 - accuracy: 0.9493 - val_loss: 0.3472 - val_accuracy: 0.9920 - 116ms/epoch - 7ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.3440 - accuracy: 0.9563 - val_loss: 0.3437 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.3405 - accuracy: 0.9493 - val_loss: 0.3407 - val_accuracy: 0.9920 - 122ms/epoch - 8ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.3381 - accuracy: 0.9538 - val_loss: 0.3376 - val_accuracy: 0.9849 - 104ms/epoch - 7ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.3364 - accuracy: 0.9388 - val_loss: 0.3343 - val_accuracy: 0.9849 - 121ms/epoch - 8ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.3333 - accuracy: 0.9409 - val_loss: 0.3318 - val_accuracy: 0.9849 - 111ms/epoch - 7ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.3322 - accuracy: 0.9400 - val_loss: 0.3291 - val_accuracy: 0.9849 - 114ms/epoch - 7ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.3306 - accuracy: 0.9398 - val_loss: 0.3261 - val_accuracy: 0.9849 - 118ms/epoch - 7ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.3286 - accuracy: 0.9403 - val_loss: 0.3237 - val_accuracy: 0.9849 - 126ms/epoch - 8ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.3253 - accuracy: 0.9420 - val_loss: 0.3214 - val_accuracy: 0.9849 - 108ms/epoch - 7ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.3236 - accuracy: 0.9417 - val_loss: 0.3188 - val_accuracy: 0.9849 - 106ms/epoch - 7ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.3215 - accuracy: 0.9420 - val_loss: 0.3161 - val_accuracy: 0.9849 - 111ms/epoch - 7ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.3195 - accuracy: 0.9417 - val_loss: 0.3140 - val_accuracy: 0.9849 - 106ms/epoch - 7ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.3202 - accuracy: 0.9400 - val_loss: 0.3117 - val_accuracy: 0.9849 - 116ms/epoch - 7ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.3179 - accuracy: 0.9407 - val_loss: 0.3096 - val_accuracy: 0.9849 - 113ms/epoch - 7ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.3134 - accuracy: 0.9426 - val_loss: 0.3075 - val_accuracy: 0.9849 - 111ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.3152 - accuracy: 0.9389 - val_loss: 0.3052 - val_accuracy: 0.9849 - 110ms/epoch - 7ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.3115 - accuracy: 0.9418 - val_loss: 0.3037 - val_accuracy: 0.9849 - 120ms/epoch - 7ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.3077 - accuracy: 0.9425 - val_loss: 0.3016 - val_accuracy: 0.9849 - 107ms/epoch - 7ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.3066 - accuracy: 0.9422 - val_loss: 0.2994 - val_accuracy: 0.9849 - 135ms/epoch - 8ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.3036 - accuracy: 0.9414 - val_loss: 0.2976 - val_accuracy: 0.9849 - 108ms/epoch - 7ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.3024 - accuracy: 0.9420 - val_loss: 0.2955 - val_accuracy: 0.9849 - 108ms/epoch - 7ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.3025 - accuracy: 0.9402 - val_loss: 0.2937 - val_accuracy: 0.9849 - 111ms/epoch - 7ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2997 - accuracy: 0.9437 - val_loss: 0.2924 - val_accuracy: 0.9849 - 105ms/epoch - 7ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2967 - accuracy: 0.9431 - val_loss: 0.2898 - val_accuracy: 0.9849 - 114ms/epoch - 7ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2949 - accuracy: 0.9431 - val_loss: 0.2888 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2923 - accuracy: 0.9458 - val_loss: 0.2861 - val_accuracy: 0.9920 - 122ms/epoch - 8ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2932 - accuracy: 0.9415 - val_loss: 0.2839 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2888 - accuracy: 0.9443 - val_loss: 0.2822 - val_accuracy: 0.9920 - 114ms/epoch - 7ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2882 - accuracy: 0.9427 - val_loss: 0.2804 - val_accuracy: 0.9920 - 107ms/epoch - 7ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2873 - accuracy: 0.9443 - val_loss: 0.2786 - val_accuracy: 0.9920 - 126ms/epoch - 8ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2859 - accuracy: 0.9449 - val_loss: 0.2763 - val_accuracy: 0.9920 - 108ms/epoch - 7ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2829 - accuracy: 0.9459 - val_loss: 0.2744 - val_accuracy: 0.9920 - 122ms/epoch - 8ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2826 - accuracy: 0.9453 - val_loss: 0.2726 - val_accuracy: 0.9923 - 105ms/epoch - 7ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2810 - accuracy: 0.9460 - val_loss: 0.2712 - val_accuracy: 0.9923 - 125ms/epoch - 8ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2795 - accuracy: 0.9469 - val_loss: 0.2693 - val_accuracy: 0.9923 - 107ms/epoch - 7ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2770 - accuracy: 0.9497 - val_loss: 0.2678 - val_accuracy: 0.9923 - 113ms/epoch - 7ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2767 - accuracy: 0.9487 - val_loss: 0.2666 - val_accuracy: 0.9934 - 107ms/epoch - 7ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2746 - accuracy: 0.9508 - val_loss: 0.2645 - val_accuracy: 0.9934 - 126ms/epoch - 8ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2735 - accuracy: 0.9515 - val_loss: 0.2632 - val_accuracy: 0.9934 - 104ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2731 - accuracy: 0.9503 - val_loss: 0.2614 - val_accuracy: 0.9934 - 105ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2707 - accuracy: 0.9515 - val_loss: 0.2601 - val_accuracy: 0.9973 - 109ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2704 - accuracy: 0.9510 - val_loss: 0.2584 - val_accuracy: 0.9973 - 105ms/epoch - 7ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2689 - accuracy: 0.9519 - val_loss: 0.2565 - val_accuracy: 0.9973 - 130ms/epoch - 8ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2673 - accuracy: 0.9512 - val_loss: 0.2552 - val_accuracy: 0.9973 - 111ms/epoch - 7ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2691 - accuracy: 0.9488 - val_loss: 0.2538 - val_accuracy: 0.9973 - 106ms/epoch - 7ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2681 - accuracy: 0.9487 - val_loss: 0.2528 - val_accuracy: 0.9973 - 115ms/epoch - 7ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2679 - accuracy: 0.9492 - val_loss: 0.2513 - val_accuracy: 0.9973 - 104ms/epoch - 6ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2639 - accuracy: 0.9511 - val_loss: 0.2505 - val_accuracy: 0.9973 - 114ms/epoch - 7ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2632 - accuracy: 0.9513 - val_loss: 0.2493 - val_accuracy: 0.9973 - 105ms/epoch - 7ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2633 - accuracy: 0.9513 - val_loss: 0.2478 - val_accuracy: 0.9973 - 117ms/epoch - 7ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2624 - accuracy: 0.9506 - val_loss: 0.2467 - val_accuracy: 0.9973 - 103ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2625 - accuracy: 0.9490 - val_loss: 0.2454 - val_accuracy: 0.9973 - 117ms/epoch - 7ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2593 - accuracy: 0.9521 - val_loss: 0.2444 - val_accuracy: 0.9973 - 102ms/epoch - 6ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2610 - accuracy: 0.9497 - val_loss: 0.2430 - val_accuracy: 0.9973 - 119ms/epoch - 7ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2597 - accuracy: 0.9509 - val_loss: 0.2423 - val_accuracy: 0.9973 - 114ms/epoch - 7ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2597 - accuracy: 0.9489 - val_loss: 0.2403 - val_accuracy: 0.9973 - 111ms/epoch - 7ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 32)          1088      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,153\n",
            "Trainable params: 1,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.9400 - accuracy: 0.9083 - val_loss: 0.8197 - val_accuracy: 0.9973 - 634ms/epoch - 20ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7350 - accuracy: 0.9680 - val_loss: 0.6885 - val_accuracy: 0.9970 - 153ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6541 - accuracy: 0.9765 - val_loss: 0.6547 - val_accuracy: 0.9934 - 150ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6175 - accuracy: 0.9709 - val_loss: 0.6254 - val_accuracy: 0.9923 - 163ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.5788 - accuracy: 0.9620 - val_loss: 0.5918 - val_accuracy: 0.9923 - 169ms/epoch - 5ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5399 - accuracy: 0.9552 - val_loss: 0.5561 - val_accuracy: 0.9923 - 162ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5026 - accuracy: 0.9545 - val_loss: 0.5210 - val_accuracy: 0.9920 - 164ms/epoch - 5ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.4688 - accuracy: 0.9725 - val_loss: 0.4891 - val_accuracy: 0.9920 - 160ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.4405 - accuracy: 0.9763 - val_loss: 0.4610 - val_accuracy: 0.9920 - 159ms/epoch - 5ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.4166 - accuracy: 0.9770 - val_loss: 0.4356 - val_accuracy: 0.9920 - 147ms/epoch - 5ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.3958 - accuracy: 0.9762 - val_loss: 0.4142 - val_accuracy: 0.9920 - 158ms/epoch - 5ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.3770 - accuracy: 0.9764 - val_loss: 0.3951 - val_accuracy: 0.9920 - 161ms/epoch - 5ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.3620 - accuracy: 0.9760 - val_loss: 0.3777 - val_accuracy: 0.9920 - 169ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.3474 - accuracy: 0.9753 - val_loss: 0.3623 - val_accuracy: 0.9920 - 159ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.3339 - accuracy: 0.9768 - val_loss: 0.3486 - val_accuracy: 0.9920 - 167ms/epoch - 5ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.3234 - accuracy: 0.9774 - val_loss: 0.3360 - val_accuracy: 0.9920 - 166ms/epoch - 5ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.3123 - accuracy: 0.9786 - val_loss: 0.3249 - val_accuracy: 0.9920 - 163ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.3029 - accuracy: 0.9785 - val_loss: 0.3146 - val_accuracy: 0.9923 - 151ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.2946 - accuracy: 0.9793 - val_loss: 0.3052 - val_accuracy: 0.9934 - 152ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.2868 - accuracy: 0.9797 - val_loss: 0.2953 - val_accuracy: 0.9934 - 154ms/epoch - 5ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.2810 - accuracy: 0.9805 - val_loss: 0.2870 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.2749 - accuracy: 0.9807 - val_loss: 0.2800 - val_accuracy: 0.9973 - 169ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.2709 - accuracy: 0.9802 - val_loss: 0.2730 - val_accuracy: 0.9973 - 165ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.2655 - accuracy: 0.9808 - val_loss: 0.2663 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.2614 - accuracy: 0.9809 - val_loss: 0.2606 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.2574 - accuracy: 0.9803 - val_loss: 0.2556 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.2542 - accuracy: 0.9799 - val_loss: 0.2510 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.2526 - accuracy: 0.9798 - val_loss: 0.2472 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.2494 - accuracy: 0.9802 - val_loss: 0.2435 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.2478 - accuracy: 0.9798 - val_loss: 0.2393 - val_accuracy: 0.9973 - 146ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.2459 - accuracy: 0.9794 - val_loss: 0.2355 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2429 - accuracy: 0.9797 - val_loss: 0.2331 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2413 - accuracy: 0.9779 - val_loss: 0.2297 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2403 - accuracy: 0.9788 - val_loss: 0.2276 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2378 - accuracy: 0.9779 - val_loss: 0.2249 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2354 - accuracy: 0.9792 - val_loss: 0.2227 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2340 - accuracy: 0.9798 - val_loss: 0.2197 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2336 - accuracy: 0.9793 - val_loss: 0.2175 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2309 - accuracy: 0.9779 - val_loss: 0.2153 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2302 - accuracy: 0.9784 - val_loss: 0.2134 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2279 - accuracy: 0.9785 - val_loss: 0.2120 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2253 - accuracy: 0.9785 - val_loss: 0.2088 - val_accuracy: 0.9973 - 146ms/epoch - 5ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2269 - accuracy: 0.9779 - val_loss: 0.2083 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2243 - accuracy: 0.9784 - val_loss: 0.2065 - val_accuracy: 0.9973 - 144ms/epoch - 4ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2225 - accuracy: 0.9779 - val_loss: 0.2048 - val_accuracy: 0.9973 - 176ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2223 - accuracy: 0.9770 - val_loss: 0.2026 - val_accuracy: 0.9973 - 152ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2192 - accuracy: 0.9794 - val_loss: 0.2014 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2184 - accuracy: 0.9789 - val_loss: 0.2004 - val_accuracy: 0.9973 - 161ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2176 - accuracy: 0.9783 - val_loss: 0.1982 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2171 - accuracy: 0.9787 - val_loss: 0.1972 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2166 - accuracy: 0.9781 - val_loss: 0.1949 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2165 - accuracy: 0.9777 - val_loss: 0.1950 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2136 - accuracy: 0.9774 - val_loss: 0.1929 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2130 - accuracy: 0.9782 - val_loss: 0.1922 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2106 - accuracy: 0.9789 - val_loss: 0.1905 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2108 - accuracy: 0.9784 - val_loss: 0.1908 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2100 - accuracy: 0.9781 - val_loss: 0.1886 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2084 - accuracy: 0.9787 - val_loss: 0.1882 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2091 - accuracy: 0.9779 - val_loss: 0.1868 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2075 - accuracy: 0.9778 - val_loss: 0.1857 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2059 - accuracy: 0.9782 - val_loss: 0.1852 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2034 - accuracy: 0.9797 - val_loss: 0.1836 - val_accuracy: 0.9973 - 148ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2048 - accuracy: 0.9782 - val_loss: 0.1823 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2034 - accuracy: 0.9785 - val_loss: 0.1818 - val_accuracy: 0.9973 - 161ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2012 - accuracy: 0.9792 - val_loss: 0.1792 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2005 - accuracy: 0.9790 - val_loss: 0.1794 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2017 - accuracy: 0.9793 - val_loss: 0.1788 - val_accuracy: 0.9973 - 161ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2017 - accuracy: 0.9778 - val_loss: 0.1774 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.1996 - accuracy: 0.9784 - val_loss: 0.1770 - val_accuracy: 0.9973 - 148ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.1989 - accuracy: 0.9784 - val_loss: 0.1760 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.1977 - accuracy: 0.9787 - val_loss: 0.1762 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.1980 - accuracy: 0.9786 - val_loss: 0.1742 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.1957 - accuracy: 0.9781 - val_loss: 0.1736 - val_accuracy: 0.9973 - 171ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.1946 - accuracy: 0.9777 - val_loss: 0.1720 - val_accuracy: 0.9973 - 149ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.1943 - accuracy: 0.9793 - val_loss: 0.1723 - val_accuracy: 0.9973 - 143ms/epoch - 4ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.1925 - accuracy: 0.9788 - val_loss: 0.1707 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.1939 - accuracy: 0.9787 - val_loss: 0.1703 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.1935 - accuracy: 0.9791 - val_loss: 0.1693 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.1908 - accuracy: 0.9784 - val_loss: 0.1687 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.1906 - accuracy: 0.9787 - val_loss: 0.1670 - val_accuracy: 0.9973 - 165ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.1893 - accuracy: 0.9786 - val_loss: 0.1664 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.1895 - accuracy: 0.9783 - val_loss: 0.1662 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.1892 - accuracy: 0.9783 - val_loss: 0.1652 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.1880 - accuracy: 0.9794 - val_loss: 0.1642 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.1867 - accuracy: 0.9787 - val_loss: 0.1629 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.1864 - accuracy: 0.9786 - val_loss: 0.1618 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.1868 - accuracy: 0.9784 - val_loss: 0.1618 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.1845 - accuracy: 0.9787 - val_loss: 0.1618 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.1829 - accuracy: 0.9790 - val_loss: 0.1596 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.1830 - accuracy: 0.9790 - val_loss: 0.1597 - val_accuracy: 0.9973 - 146ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.1823 - accuracy: 0.9790 - val_loss: 0.1595 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.1832 - accuracy: 0.9793 - val_loss: 0.1574 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.1834 - accuracy: 0.9796 - val_loss: 0.1579 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.1828 - accuracy: 0.9791 - val_loss: 0.1567 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.1814 - accuracy: 0.9795 - val_loss: 0.1567 - val_accuracy: 0.9973 - 156ms/epoch - 5ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.1810 - accuracy: 0.9789 - val_loss: 0.1571 - val_accuracy: 0.9973 - 151ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.1811 - accuracy: 0.9787 - val_loss: 0.1546 - val_accuracy: 0.9973 - 147ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.1788 - accuracy: 0.9793 - val_loss: 0.1546 - val_accuracy: 0.9973 - 150ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.1793 - accuracy: 0.9787 - val_loss: 0.1551 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.1800 - accuracy: 0.9785 - val_loss: 0.1532 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 32)          1088      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,153\n",
            "Trainable params: 1,153\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 1.0028 - accuracy: 0.8028 - val_loss: 0.9263 - val_accuracy: 0.9890 - 589ms/epoch - 37ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8660 - accuracy: 0.9574 - val_loss: 0.8143 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7660 - accuracy: 0.9670 - val_loss: 0.7339 - val_accuracy: 0.9975 - 127ms/epoch - 8ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.6975 - accuracy: 0.9691 - val_loss: 0.6862 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6613 - accuracy: 0.9700 - val_loss: 0.6678 - val_accuracy: 0.9975 - 112ms/epoch - 7ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6425 - accuracy: 0.9700 - val_loss: 0.6533 - val_accuracy: 0.9970 - 115ms/epoch - 7ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6252 - accuracy: 0.9710 - val_loss: 0.6395 - val_accuracy: 0.9970 - 119ms/epoch - 7ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6072 - accuracy: 0.9729 - val_loss: 0.6244 - val_accuracy: 0.9970 - 114ms/epoch - 7ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.5882 - accuracy: 0.9730 - val_loss: 0.6081 - val_accuracy: 0.9970 - 133ms/epoch - 8ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.5693 - accuracy: 0.9745 - val_loss: 0.5909 - val_accuracy: 0.9942 - 123ms/epoch - 8ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5501 - accuracy: 0.9735 - val_loss: 0.5732 - val_accuracy: 0.9934 - 115ms/epoch - 7ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5304 - accuracy: 0.9754 - val_loss: 0.5552 - val_accuracy: 0.9934 - 124ms/epoch - 8ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5113 - accuracy: 0.9767 - val_loss: 0.5374 - val_accuracy: 0.9934 - 112ms/epoch - 7ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.4928 - accuracy: 0.9761 - val_loss: 0.5195 - val_accuracy: 0.9934 - 124ms/epoch - 8ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.4747 - accuracy: 0.9773 - val_loss: 0.5026 - val_accuracy: 0.9934 - 120ms/epoch - 8ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.4587 - accuracy: 0.9772 - val_loss: 0.4863 - val_accuracy: 0.9934 - 138ms/epoch - 9ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.4421 - accuracy: 0.9778 - val_loss: 0.4708 - val_accuracy: 0.9934 - 120ms/epoch - 7ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4282 - accuracy: 0.9779 - val_loss: 0.4560 - val_accuracy: 0.9934 - 123ms/epoch - 8ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4148 - accuracy: 0.9775 - val_loss: 0.4417 - val_accuracy: 0.9970 - 117ms/epoch - 7ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4019 - accuracy: 0.9786 - val_loss: 0.4284 - val_accuracy: 0.9970 - 112ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.3899 - accuracy: 0.9787 - val_loss: 0.4158 - val_accuracy: 0.9970 - 132ms/epoch - 8ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.3795 - accuracy: 0.9796 - val_loss: 0.4041 - val_accuracy: 0.9970 - 110ms/epoch - 7ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.3702 - accuracy: 0.9794 - val_loss: 0.3929 - val_accuracy: 0.9970 - 122ms/epoch - 8ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.3607 - accuracy: 0.9794 - val_loss: 0.3824 - val_accuracy: 0.9970 - 116ms/epoch - 7ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.3519 - accuracy: 0.9795 - val_loss: 0.3727 - val_accuracy: 0.9970 - 120ms/epoch - 7ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.3440 - accuracy: 0.9800 - val_loss: 0.3631 - val_accuracy: 0.9970 - 112ms/epoch - 7ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.3363 - accuracy: 0.9800 - val_loss: 0.3547 - val_accuracy: 0.9970 - 115ms/epoch - 7ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.3298 - accuracy: 0.9799 - val_loss: 0.3461 - val_accuracy: 0.9970 - 119ms/epoch - 7ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.3247 - accuracy: 0.9792 - val_loss: 0.3386 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3189 - accuracy: 0.9791 - val_loss: 0.3311 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3130 - accuracy: 0.9792 - val_loss: 0.3243 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3075 - accuracy: 0.9797 - val_loss: 0.3176 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3040 - accuracy: 0.9788 - val_loss: 0.3113 - val_accuracy: 0.9975 - 116ms/epoch - 7ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3002 - accuracy: 0.9788 - val_loss: 0.3057 - val_accuracy: 0.9975 - 110ms/epoch - 7ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.2947 - accuracy: 0.9789 - val_loss: 0.3000 - val_accuracy: 0.9975 - 116ms/epoch - 7ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.2924 - accuracy: 0.9789 - val_loss: 0.2948 - val_accuracy: 0.9975 - 130ms/epoch - 8ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.2885 - accuracy: 0.9790 - val_loss: 0.2898 - val_accuracy: 0.9975 - 123ms/epoch - 8ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.2857 - accuracy: 0.9783 - val_loss: 0.2855 - val_accuracy: 0.9975 - 115ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.2824 - accuracy: 0.9787 - val_loss: 0.2808 - val_accuracy: 0.9975 - 124ms/epoch - 8ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.2803 - accuracy: 0.9788 - val_loss: 0.2767 - val_accuracy: 0.9975 - 112ms/epoch - 7ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.2765 - accuracy: 0.9788 - val_loss: 0.2727 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.2733 - accuracy: 0.9794 - val_loss: 0.2686 - val_accuracy: 0.9975 - 121ms/epoch - 8ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.2712 - accuracy: 0.9781 - val_loss: 0.2653 - val_accuracy: 0.9975 - 131ms/epoch - 8ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.2691 - accuracy: 0.9780 - val_loss: 0.2620 - val_accuracy: 0.9975 - 133ms/epoch - 8ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.2668 - accuracy: 0.9775 - val_loss: 0.2583 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.2669 - accuracy: 0.9770 - val_loss: 0.2559 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.2634 - accuracy: 0.9785 - val_loss: 0.2523 - val_accuracy: 0.9975 - 125ms/epoch - 8ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.2613 - accuracy: 0.9776 - val_loss: 0.2497 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.2590 - accuracy: 0.9774 - val_loss: 0.2469 - val_accuracy: 0.9975 - 119ms/epoch - 7ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.2583 - accuracy: 0.9778 - val_loss: 0.2440 - val_accuracy: 0.9975 - 128ms/epoch - 8ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.2564 - accuracy: 0.9778 - val_loss: 0.2420 - val_accuracy: 0.9975 - 112ms/epoch - 7ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.2551 - accuracy: 0.9763 - val_loss: 0.2394 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.2524 - accuracy: 0.9775 - val_loss: 0.2370 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.2514 - accuracy: 0.9775 - val_loss: 0.2347 - val_accuracy: 0.9975 - 123ms/epoch - 8ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.2475 - accuracy: 0.9782 - val_loss: 0.2329 - val_accuracy: 0.9975 - 110ms/epoch - 7ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.2489 - accuracy: 0.9768 - val_loss: 0.2312 - val_accuracy: 0.9975 - 134ms/epoch - 8ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.2462 - accuracy: 0.9774 - val_loss: 0.2291 - val_accuracy: 0.9975 - 116ms/epoch - 7ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.2460 - accuracy: 0.9774 - val_loss: 0.2277 - val_accuracy: 0.9975 - 124ms/epoch - 8ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.2450 - accuracy: 0.9774 - val_loss: 0.2262 - val_accuracy: 0.9975 - 117ms/epoch - 7ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.2410 - accuracy: 0.9783 - val_loss: 0.2245 - val_accuracy: 0.9975 - 122ms/epoch - 8ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.2408 - accuracy: 0.9780 - val_loss: 0.2237 - val_accuracy: 0.9975 - 113ms/epoch - 7ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.2405 - accuracy: 0.9786 - val_loss: 0.2217 - val_accuracy: 0.9975 - 123ms/epoch - 8ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.2408 - accuracy: 0.9772 - val_loss: 0.2203 - val_accuracy: 0.9975 - 121ms/epoch - 8ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.2386 - accuracy: 0.9779 - val_loss: 0.2193 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.2356 - accuracy: 0.9782 - val_loss: 0.2174 - val_accuracy: 0.9975 - 122ms/epoch - 8ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.2357 - accuracy: 0.9781 - val_loss: 0.2169 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2354 - accuracy: 0.9778 - val_loss: 0.2157 - val_accuracy: 0.9975 - 121ms/epoch - 8ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2346 - accuracy: 0.9776 - val_loss: 0.2149 - val_accuracy: 0.9975 - 120ms/epoch - 7ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2330 - accuracy: 0.9785 - val_loss: 0.2146 - val_accuracy: 0.9975 - 118ms/epoch - 7ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2321 - accuracy: 0.9777 - val_loss: 0.2134 - val_accuracy: 0.9975 - 110ms/epoch - 7ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2314 - accuracy: 0.9776 - val_loss: 0.2124 - val_accuracy: 0.9975 - 124ms/epoch - 8ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2297 - accuracy: 0.9785 - val_loss: 0.2117 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2294 - accuracy: 0.9780 - val_loss: 0.2108 - val_accuracy: 0.9975 - 116ms/epoch - 7ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2277 - accuracy: 0.9782 - val_loss: 0.2105 - val_accuracy: 0.9975 - 113ms/epoch - 7ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2279 - accuracy: 0.9772 - val_loss: 0.2094 - val_accuracy: 0.9975 - 113ms/epoch - 7ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2266 - accuracy: 0.9781 - val_loss: 0.2092 - val_accuracy: 0.9975 - 123ms/epoch - 8ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2280 - accuracy: 0.9782 - val_loss: 0.2083 - val_accuracy: 0.9975 - 110ms/epoch - 7ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2251 - accuracy: 0.9784 - val_loss: 0.2079 - val_accuracy: 0.9975 - 121ms/epoch - 8ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2260 - accuracy: 0.9777 - val_loss: 0.2072 - val_accuracy: 0.9975 - 115ms/epoch - 7ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2225 - accuracy: 0.9789 - val_loss: 0.2065 - val_accuracy: 0.9975 - 115ms/epoch - 7ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2231 - accuracy: 0.9775 - val_loss: 0.2063 - val_accuracy: 0.9975 - 123ms/epoch - 8ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2222 - accuracy: 0.9784 - val_loss: 0.2053 - val_accuracy: 0.9975 - 118ms/epoch - 7ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2207 - accuracy: 0.9789 - val_loss: 0.2048 - val_accuracy: 0.9975 - 112ms/epoch - 7ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2215 - accuracy: 0.9775 - val_loss: 0.2050 - val_accuracy: 0.9975 - 119ms/epoch - 7ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2208 - accuracy: 0.9780 - val_loss: 0.2044 - val_accuracy: 0.9975 - 111ms/epoch - 7ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2206 - accuracy: 0.9779 - val_loss: 0.2040 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2209 - accuracy: 0.9773 - val_loss: 0.2030 - val_accuracy: 0.9975 - 115ms/epoch - 7ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2169 - accuracy: 0.9788 - val_loss: 0.2027 - val_accuracy: 0.9975 - 114ms/epoch - 7ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2183 - accuracy: 0.9782 - val_loss: 0.2022 - val_accuracy: 0.9978 - 111ms/epoch - 7ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2177 - accuracy: 0.9785 - val_loss: 0.2020 - val_accuracy: 0.9978 - 115ms/epoch - 7ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2175 - accuracy: 0.9788 - val_loss: 0.2020 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2159 - accuracy: 0.9777 - val_loss: 0.2011 - val_accuracy: 0.9978 - 117ms/epoch - 7ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2163 - accuracy: 0.9788 - val_loss: 0.2011 - val_accuracy: 0.9978 - 111ms/epoch - 7ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2149 - accuracy: 0.9787 - val_loss: 0.2003 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2137 - accuracy: 0.9791 - val_loss: 0.2001 - val_accuracy: 0.9978 - 113ms/epoch - 7ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2155 - accuracy: 0.9781 - val_loss: 0.1997 - val_accuracy: 0.9978 - 120ms/epoch - 7ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2146 - accuracy: 0.9793 - val_loss: 0.1994 - val_accuracy: 0.9978 - 114ms/epoch - 7ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2122 - accuracy: 0.9784 - val_loss: 0.1990 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2125 - accuracy: 0.9784 - val_loss: 0.1989 - val_accuracy: 0.9978 - 113ms/epoch - 7ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2111 - accuracy: 0.9800 - val_loss: 0.1981 - val_accuracy: 0.9978 - 119ms/epoch - 7ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 32)          1088      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,121\n",
            "Trainable params: 1,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.9386 - accuracy: 0.6396 - val_loss: 0.8228 - val_accuracy: 0.9945 - 656ms/epoch - 20ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7399 - accuracy: 0.8904 - val_loss: 0.6973 - val_accuracy: 0.9934 - 174ms/epoch - 5ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6655 - accuracy: 0.8973 - val_loss: 0.6726 - val_accuracy: 0.9934 - 170ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6399 - accuracy: 0.8979 - val_loss: 0.6546 - val_accuracy: 0.9934 - 166ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.6149 - accuracy: 0.8944 - val_loss: 0.6338 - val_accuracy: 0.9701 - 169ms/epoch - 5ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5880 - accuracy: 0.8953 - val_loss: 0.6108 - val_accuracy: 0.9701 - 163ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5598 - accuracy: 0.8970 - val_loss: 0.5864 - val_accuracy: 0.9923 - 157ms/epoch - 5ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.5326 - accuracy: 0.9003 - val_loss: 0.5618 - val_accuracy: 0.9923 - 165ms/epoch - 5ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.5069 - accuracy: 0.9039 - val_loss: 0.5377 - val_accuracy: 0.9923 - 174ms/epoch - 5ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.4821 - accuracy: 0.9062 - val_loss: 0.5140 - val_accuracy: 0.9923 - 175ms/epoch - 5ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.4598 - accuracy: 0.9074 - val_loss: 0.4919 - val_accuracy: 0.9923 - 156ms/epoch - 5ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.4410 - accuracy: 0.9082 - val_loss: 0.4714 - val_accuracy: 0.9923 - 171ms/epoch - 5ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.4232 - accuracy: 0.9261 - val_loss: 0.4529 - val_accuracy: 0.9923 - 173ms/epoch - 5ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.4069 - accuracy: 0.9688 - val_loss: 0.4358 - val_accuracy: 0.9923 - 160ms/epoch - 5ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.3940 - accuracy: 0.9742 - val_loss: 0.4203 - val_accuracy: 0.9923 - 175ms/epoch - 5ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.3818 - accuracy: 0.9746 - val_loss: 0.4065 - val_accuracy: 0.9923 - 156ms/epoch - 5ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.3706 - accuracy: 0.9729 - val_loss: 0.3937 - val_accuracy: 0.9923 - 171ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.3612 - accuracy: 0.9738 - val_loss: 0.3825 - val_accuracy: 0.9923 - 167ms/epoch - 5ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.3516 - accuracy: 0.9749 - val_loss: 0.3718 - val_accuracy: 0.9923 - 161ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.3444 - accuracy: 0.9754 - val_loss: 0.3624 - val_accuracy: 0.9923 - 185ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.3357 - accuracy: 0.9762 - val_loss: 0.3539 - val_accuracy: 0.9923 - 164ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.3303 - accuracy: 0.9775 - val_loss: 0.3455 - val_accuracy: 0.9920 - 164ms/epoch - 5ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.3244 - accuracy: 0.9765 - val_loss: 0.3381 - val_accuracy: 0.9920 - 167ms/epoch - 5ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.3179 - accuracy: 0.9768 - val_loss: 0.3313 - val_accuracy: 0.9923 - 170ms/epoch - 5ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.3123 - accuracy: 0.9773 - val_loss: 0.3243 - val_accuracy: 0.9923 - 158ms/epoch - 5ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.3061 - accuracy: 0.9784 - val_loss: 0.3180 - val_accuracy: 0.9923 - 157ms/epoch - 5ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.3015 - accuracy: 0.9802 - val_loss: 0.3123 - val_accuracy: 0.9934 - 166ms/epoch - 5ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.2970 - accuracy: 0.9809 - val_loss: 0.3069 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.2927 - accuracy: 0.9809 - val_loss: 0.3017 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.2883 - accuracy: 0.9810 - val_loss: 0.2967 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.2838 - accuracy: 0.9810 - val_loss: 0.2919 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2796 - accuracy: 0.9811 - val_loss: 0.2870 - val_accuracy: 0.9973 - 161ms/epoch - 5ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2758 - accuracy: 0.9818 - val_loss: 0.2824 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2736 - accuracy: 0.9812 - val_loss: 0.2781 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2704 - accuracy: 0.9818 - val_loss: 0.2742 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2669 - accuracy: 0.9815 - val_loss: 0.2700 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2648 - accuracy: 0.9816 - val_loss: 0.2663 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2620 - accuracy: 0.9812 - val_loss: 0.2625 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2609 - accuracy: 0.9813 - val_loss: 0.2593 - val_accuracy: 0.9973 - 169ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2579 - accuracy: 0.9813 - val_loss: 0.2563 - val_accuracy: 0.9973 - 179ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2575 - accuracy: 0.9799 - val_loss: 0.2533 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2556 - accuracy: 0.9808 - val_loss: 0.2504 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2529 - accuracy: 0.9816 - val_loss: 0.2476 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2523 - accuracy: 0.9807 - val_loss: 0.2452 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2489 - accuracy: 0.9813 - val_loss: 0.2428 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2498 - accuracy: 0.9809 - val_loss: 0.2404 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2487 - accuracy: 0.9805 - val_loss: 0.2383 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2481 - accuracy: 0.9801 - val_loss: 0.2364 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2464 - accuracy: 0.9805 - val_loss: 0.2344 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2459 - accuracy: 0.9811 - val_loss: 0.2329 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2445 - accuracy: 0.9812 - val_loss: 0.2312 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2433 - accuracy: 0.9805 - val_loss: 0.2296 - val_accuracy: 0.9973 - 176ms/epoch - 5ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2407 - accuracy: 0.9811 - val_loss: 0.2279 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2407 - accuracy: 0.9812 - val_loss: 0.2264 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2399 - accuracy: 0.9810 - val_loss: 0.2248 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2387 - accuracy: 0.9806 - val_loss: 0.2236 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2397 - accuracy: 0.9805 - val_loss: 0.2225 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2368 - accuracy: 0.9809 - val_loss: 0.2213 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2356 - accuracy: 0.9810 - val_loss: 0.2200 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2359 - accuracy: 0.9809 - val_loss: 0.2188 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2352 - accuracy: 0.9807 - val_loss: 0.2176 - val_accuracy: 0.9973 - 161ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2343 - accuracy: 0.9807 - val_loss: 0.2165 - val_accuracy: 0.9973 - 165ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2333 - accuracy: 0.9806 - val_loss: 0.2155 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2327 - accuracy: 0.9804 - val_loss: 0.2145 - val_accuracy: 0.9973 - 162ms/epoch - 5ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2303 - accuracy: 0.9805 - val_loss: 0.2135 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2291 - accuracy: 0.9811 - val_loss: 0.2123 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2283 - accuracy: 0.9811 - val_loss: 0.2110 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2282 - accuracy: 0.9809 - val_loss: 0.2102 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2269 - accuracy: 0.9816 - val_loss: 0.2092 - val_accuracy: 0.9978 - 158ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2275 - accuracy: 0.9809 - val_loss: 0.2081 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2253 - accuracy: 0.9807 - val_loss: 0.2072 - val_accuracy: 0.9978 - 170ms/epoch - 5ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2252 - accuracy: 0.9810 - val_loss: 0.2063 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2250 - accuracy: 0.9803 - val_loss: 0.2052 - val_accuracy: 0.9973 - 169ms/epoch - 5ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2243 - accuracy: 0.9806 - val_loss: 0.2044 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2210 - accuracy: 0.9816 - val_loss: 0.2032 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.2226 - accuracy: 0.9803 - val_loss: 0.2024 - val_accuracy: 0.9973 - 153ms/epoch - 5ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.2198 - accuracy: 0.9820 - val_loss: 0.2014 - val_accuracy: 0.9973 - 155ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.2202 - accuracy: 0.9811 - val_loss: 0.2003 - val_accuracy: 0.9973 - 165ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.2219 - accuracy: 0.9805 - val_loss: 0.1998 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.2188 - accuracy: 0.9809 - val_loss: 0.1988 - val_accuracy: 0.9973 - 157ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.2182 - accuracy: 0.9811 - val_loss: 0.1979 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.2203 - accuracy: 0.9803 - val_loss: 0.1973 - val_accuracy: 0.9973 - 176ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.2163 - accuracy: 0.9809 - val_loss: 0.1961 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.2153 - accuracy: 0.9814 - val_loss: 0.1954 - val_accuracy: 0.9978 - 161ms/epoch - 5ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.2157 - accuracy: 0.9810 - val_loss: 0.1945 - val_accuracy: 0.9978 - 164ms/epoch - 5ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.2160 - accuracy: 0.9806 - val_loss: 0.1938 - val_accuracy: 0.9973 - 159ms/epoch - 5ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.2146 - accuracy: 0.9812 - val_loss: 0.1927 - val_accuracy: 0.9973 - 158ms/epoch - 5ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.2147 - accuracy: 0.9813 - val_loss: 0.1918 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.2152 - accuracy: 0.9809 - val_loss: 0.1913 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.2141 - accuracy: 0.9805 - val_loss: 0.1905 - val_accuracy: 0.9973 - 174ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.2108 - accuracy: 0.9806 - val_loss: 0.1897 - val_accuracy: 0.9973 - 154ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.2124 - accuracy: 0.9802 - val_loss: 0.1889 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.2115 - accuracy: 0.9809 - val_loss: 0.1883 - val_accuracy: 0.9973 - 160ms/epoch - 5ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.2095 - accuracy: 0.9810 - val_loss: 0.1872 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.2105 - accuracy: 0.9809 - val_loss: 0.1865 - val_accuracy: 0.9973 - 165ms/epoch - 5ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.2089 - accuracy: 0.9805 - val_loss: 0.1859 - val_accuracy: 0.9973 - 163ms/epoch - 5ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.2084 - accuracy: 0.9811 - val_loss: 0.1851 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.2081 - accuracy: 0.9808 - val_loss: 0.1847 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.2065 - accuracy: 0.9813 - val_loss: 0.1837 - val_accuracy: 0.9973 - 176ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.2067 - accuracy: 0.9804 - val_loss: 0.1830 - val_accuracy: 0.9978 - 157ms/epoch - 5ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 32)          1088      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 32)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 32)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,121\n",
            "Trainable params: 1,121\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 1.0285 - accuracy: 0.6689 - val_loss: 0.9507 - val_accuracy: 0.8655 - 614ms/epoch - 38ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8848 - accuracy: 0.8369 - val_loss: 0.8313 - val_accuracy: 0.9978 - 136ms/epoch - 8ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7766 - accuracy: 0.9254 - val_loss: 0.7437 - val_accuracy: 0.9978 - 123ms/epoch - 8ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.7042 - accuracy: 0.9285 - val_loss: 0.6951 - val_accuracy: 0.9978 - 122ms/epoch - 8ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6679 - accuracy: 0.9402 - val_loss: 0.6774 - val_accuracy: 0.9970 - 122ms/epoch - 8ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6512 - accuracy: 0.9098 - val_loss: 0.6658 - val_accuracy: 0.9934 - 119ms/epoch - 7ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6379 - accuracy: 0.8772 - val_loss: 0.6561 - val_accuracy: 0.9934 - 122ms/epoch - 8ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6247 - accuracy: 0.8837 - val_loss: 0.6461 - val_accuracy: 0.9934 - 125ms/epoch - 8ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6115 - accuracy: 0.8851 - val_loss: 0.6353 - val_accuracy: 0.9934 - 124ms/epoch - 8ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.5976 - accuracy: 0.8892 - val_loss: 0.6236 - val_accuracy: 0.9934 - 130ms/epoch - 8ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5828 - accuracy: 0.8894 - val_loss: 0.6115 - val_accuracy: 0.9934 - 123ms/epoch - 8ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5681 - accuracy: 0.8941 - val_loss: 0.5991 - val_accuracy: 0.9934 - 124ms/epoch - 8ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5536 - accuracy: 0.8975 - val_loss: 0.5864 - val_accuracy: 0.9934 - 141ms/epoch - 9ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.5387 - accuracy: 0.8997 - val_loss: 0.5733 - val_accuracy: 0.9934 - 119ms/epoch - 7ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.5241 - accuracy: 0.9074 - val_loss: 0.5603 - val_accuracy: 0.9934 - 127ms/epoch - 8ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.5100 - accuracy: 0.9379 - val_loss: 0.5471 - val_accuracy: 0.9934 - 129ms/epoch - 8ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.4954 - accuracy: 0.9642 - val_loss: 0.5341 - val_accuracy: 0.9934 - 130ms/epoch - 8ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4821 - accuracy: 0.9700 - val_loss: 0.5215 - val_accuracy: 0.9934 - 121ms/epoch - 8ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4688 - accuracy: 0.9725 - val_loss: 0.5091 - val_accuracy: 0.9934 - 127ms/epoch - 8ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4568 - accuracy: 0.9742 - val_loss: 0.4970 - val_accuracy: 0.9934 - 119ms/epoch - 7ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.4450 - accuracy: 0.9758 - val_loss: 0.4854 - val_accuracy: 0.9934 - 123ms/epoch - 8ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.4336 - accuracy: 0.9770 - val_loss: 0.4742 - val_accuracy: 0.9934 - 125ms/epoch - 8ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.4234 - accuracy: 0.9783 - val_loss: 0.4634 - val_accuracy: 0.9945 - 121ms/epoch - 8ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.4133 - accuracy: 0.9789 - val_loss: 0.4528 - val_accuracy: 0.9973 - 123ms/epoch - 8ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.4040 - accuracy: 0.9792 - val_loss: 0.4430 - val_accuracy: 0.9973 - 122ms/epoch - 8ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.3955 - accuracy: 0.9792 - val_loss: 0.4336 - val_accuracy: 0.9973 - 126ms/epoch - 8ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.3875 - accuracy: 0.9798 - val_loss: 0.4246 - val_accuracy: 0.9973 - 127ms/epoch - 8ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.3794 - accuracy: 0.9791 - val_loss: 0.4161 - val_accuracy: 0.9973 - 122ms/epoch - 8ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.3711 - accuracy: 0.9790 - val_loss: 0.4080 - val_accuracy: 0.9973 - 122ms/epoch - 8ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3652 - accuracy: 0.9784 - val_loss: 0.4005 - val_accuracy: 0.9973 - 128ms/epoch - 8ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3588 - accuracy: 0.9779 - val_loss: 0.3934 - val_accuracy: 0.9973 - 120ms/epoch - 7ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3522 - accuracy: 0.9792 - val_loss: 0.3865 - val_accuracy: 0.9973 - 121ms/epoch - 8ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3470 - accuracy: 0.9790 - val_loss: 0.3801 - val_accuracy: 0.9973 - 130ms/epoch - 8ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3407 - accuracy: 0.9793 - val_loss: 0.3739 - val_accuracy: 0.9973 - 123ms/epoch - 8ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.3356 - accuracy: 0.9802 - val_loss: 0.3678 - val_accuracy: 0.9973 - 126ms/epoch - 8ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.3315 - accuracy: 0.9800 - val_loss: 0.3624 - val_accuracy: 0.9973 - 128ms/epoch - 8ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.3267 - accuracy: 0.9806 - val_loss: 0.3569 - val_accuracy: 0.9973 - 132ms/epoch - 8ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.3230 - accuracy: 0.9806 - val_loss: 0.3519 - val_accuracy: 0.9973 - 120ms/epoch - 7ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.3183 - accuracy: 0.9808 - val_loss: 0.3469 - val_accuracy: 0.9973 - 135ms/epoch - 8ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.3147 - accuracy: 0.9810 - val_loss: 0.3421 - val_accuracy: 0.9973 - 118ms/epoch - 7ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.3121 - accuracy: 0.9814 - val_loss: 0.3378 - val_accuracy: 0.9973 - 135ms/epoch - 8ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.3081 - accuracy: 0.9810 - val_loss: 0.3334 - val_accuracy: 0.9973 - 120ms/epoch - 7ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.3055 - accuracy: 0.9805 - val_loss: 0.3292 - val_accuracy: 0.9973 - 124ms/epoch - 8ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.3028 - accuracy: 0.9814 - val_loss: 0.3257 - val_accuracy: 0.9973 - 119ms/epoch - 7ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.2995 - accuracy: 0.9816 - val_loss: 0.3218 - val_accuracy: 0.9973 - 130ms/epoch - 8ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.2981 - accuracy: 0.9807 - val_loss: 0.3181 - val_accuracy: 0.9973 - 123ms/epoch - 8ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.2933 - accuracy: 0.9822 - val_loss: 0.3143 - val_accuracy: 0.9973 - 122ms/epoch - 8ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.2918 - accuracy: 0.9815 - val_loss: 0.3110 - val_accuracy: 0.9973 - 121ms/epoch - 8ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.2883 - accuracy: 0.9816 - val_loss: 0.3077 - val_accuracy: 0.9973 - 125ms/epoch - 8ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.2879 - accuracy: 0.9816 - val_loss: 0.3046 - val_accuracy: 0.9973 - 139ms/epoch - 9ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.2857 - accuracy: 0.9813 - val_loss: 0.3016 - val_accuracy: 0.9973 - 129ms/epoch - 8ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.2839 - accuracy: 0.9804 - val_loss: 0.2991 - val_accuracy: 0.9973 - 133ms/epoch - 8ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.2806 - accuracy: 0.9809 - val_loss: 0.2961 - val_accuracy: 0.9973 - 126ms/epoch - 8ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.2792 - accuracy: 0.9816 - val_loss: 0.2936 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.2775 - accuracy: 0.9811 - val_loss: 0.2911 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.2743 - accuracy: 0.9805 - val_loss: 0.2886 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.2735 - accuracy: 0.9813 - val_loss: 0.2860 - val_accuracy: 0.9978 - 146ms/epoch - 9ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.2729 - accuracy: 0.9811 - val_loss: 0.2840 - val_accuracy: 0.9978 - 123ms/epoch - 8ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.2691 - accuracy: 0.9808 - val_loss: 0.2819 - val_accuracy: 0.9978 - 130ms/epoch - 8ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.2684 - accuracy: 0.9804 - val_loss: 0.2794 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.2662 - accuracy: 0.9817 - val_loss: 0.2776 - val_accuracy: 0.9978 - 130ms/epoch - 8ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.2647 - accuracy: 0.9811 - val_loss: 0.2753 - val_accuracy: 0.9978 - 145ms/epoch - 9ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.2630 - accuracy: 0.9814 - val_loss: 0.2732 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.2619 - accuracy: 0.9815 - val_loss: 0.2713 - val_accuracy: 0.9978 - 133ms/epoch - 8ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.2601 - accuracy: 0.9817 - val_loss: 0.2695 - val_accuracy: 0.9978 - 117ms/epoch - 7ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.2579 - accuracy: 0.9814 - val_loss: 0.2674 - val_accuracy: 0.9978 - 137ms/epoch - 9ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2578 - accuracy: 0.9813 - val_loss: 0.2658 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2561 - accuracy: 0.9809 - val_loss: 0.2640 - val_accuracy: 0.9978 - 127ms/epoch - 8ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2548 - accuracy: 0.9813 - val_loss: 0.2626 - val_accuracy: 0.9978 - 123ms/epoch - 8ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2528 - accuracy: 0.9819 - val_loss: 0.2605 - val_accuracy: 0.9978 - 129ms/epoch - 8ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2519 - accuracy: 0.9814 - val_loss: 0.2590 - val_accuracy: 0.9978 - 134ms/epoch - 8ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2500 - accuracy: 0.9815 - val_loss: 0.2575 - val_accuracy: 0.9978 - 128ms/epoch - 8ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2505 - accuracy: 0.9809 - val_loss: 0.2560 - val_accuracy: 0.9978 - 137ms/epoch - 9ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2480 - accuracy: 0.9818 - val_loss: 0.2543 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2493 - accuracy: 0.9810 - val_loss: 0.2529 - val_accuracy: 0.9978 - 132ms/epoch - 8ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2472 - accuracy: 0.9815 - val_loss: 0.2516 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2475 - accuracy: 0.9811 - val_loss: 0.2503 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2455 - accuracy: 0.9820 - val_loss: 0.2490 - val_accuracy: 0.9978 - 131ms/epoch - 8ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2446 - accuracy: 0.9816 - val_loss: 0.2479 - val_accuracy: 0.9978 - 144ms/epoch - 9ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2436 - accuracy: 0.9817 - val_loss: 0.2465 - val_accuracy: 0.9978 - 127ms/epoch - 8ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2431 - accuracy: 0.9808 - val_loss: 0.2454 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2411 - accuracy: 0.9816 - val_loss: 0.2440 - val_accuracy: 0.9978 - 120ms/epoch - 8ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2410 - accuracy: 0.9815 - val_loss: 0.2431 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2392 - accuracy: 0.9819 - val_loss: 0.2419 - val_accuracy: 0.9978 - 128ms/epoch - 8ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2394 - accuracy: 0.9813 - val_loss: 0.2408 - val_accuracy: 0.9978 - 129ms/epoch - 8ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2386 - accuracy: 0.9816 - val_loss: 0.2397 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2385 - accuracy: 0.9817 - val_loss: 0.2387 - val_accuracy: 0.9978 - 138ms/epoch - 9ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2370 - accuracy: 0.9812 - val_loss: 0.2378 - val_accuracy: 0.9978 - 123ms/epoch - 8ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2351 - accuracy: 0.9820 - val_loss: 0.2364 - val_accuracy: 0.9978 - 125ms/epoch - 8ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2364 - accuracy: 0.9811 - val_loss: 0.2357 - val_accuracy: 0.9978 - 130ms/epoch - 8ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2347 - accuracy: 0.9820 - val_loss: 0.2348 - val_accuracy: 0.9978 - 124ms/epoch - 8ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2349 - accuracy: 0.9818 - val_loss: 0.2337 - val_accuracy: 0.9978 - 128ms/epoch - 8ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2330 - accuracy: 0.9823 - val_loss: 0.2327 - val_accuracy: 0.9978 - 129ms/epoch - 8ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2339 - accuracy: 0.9813 - val_loss: 0.2320 - val_accuracy: 0.9978 - 141ms/epoch - 9ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2332 - accuracy: 0.9812 - val_loss: 0.2311 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2316 - accuracy: 0.9820 - val_loss: 0.2300 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2331 - accuracy: 0.9814 - val_loss: 0.2296 - val_accuracy: 0.9978 - 120ms/epoch - 8ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2303 - accuracy: 0.9824 - val_loss: 0.2289 - val_accuracy: 0.9978 - 130ms/epoch - 8ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2304 - accuracy: 0.9826 - val_loss: 0.2279 - val_accuracy: 0.9978 - 126ms/epoch - 8ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2300 - accuracy: 0.9815 - val_loss: 0.2271 - val_accuracy: 0.9978 - 128ms/epoch - 8ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 64)          2176      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,305\n",
            "Trainable params: 2,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.9920 - accuracy: 0.8419 - val_loss: 0.7891 - val_accuracy: 0.9978 - 647ms/epoch - 20ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7101 - accuracy: 0.9678 - val_loss: 0.6837 - val_accuracy: 0.9937 - 178ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6523 - accuracy: 0.9779 - val_loss: 0.6522 - val_accuracy: 0.9923 - 169ms/epoch - 5ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6094 - accuracy: 0.9780 - val_loss: 0.6157 - val_accuracy: 0.9923 - 174ms/epoch - 5ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.5627 - accuracy: 0.9757 - val_loss: 0.5748 - val_accuracy: 0.9920 - 168ms/epoch - 5ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5184 - accuracy: 0.9749 - val_loss: 0.5348 - val_accuracy: 0.9920 - 167ms/epoch - 5ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.4803 - accuracy: 0.9767 - val_loss: 0.4990 - val_accuracy: 0.9920 - 174ms/epoch - 5ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.4473 - accuracy: 0.9767 - val_loss: 0.4674 - val_accuracy: 0.9920 - 183ms/epoch - 6ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.4179 - accuracy: 0.9770 - val_loss: 0.4391 - val_accuracy: 0.9920 - 182ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.3925 - accuracy: 0.9770 - val_loss: 0.4136 - val_accuracy: 0.9920 - 184ms/epoch - 6ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.3712 - accuracy: 0.9781 - val_loss: 0.3909 - val_accuracy: 0.9920 - 179ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.3514 - accuracy: 0.9790 - val_loss: 0.3699 - val_accuracy: 0.9920 - 201ms/epoch - 6ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.3344 - accuracy: 0.9814 - val_loss: 0.3521 - val_accuracy: 0.9923 - 186ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.3191 - accuracy: 0.9823 - val_loss: 0.3359 - val_accuracy: 0.9973 - 195ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.3064 - accuracy: 0.9831 - val_loss: 0.3222 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.2968 - accuracy: 0.9824 - val_loss: 0.3101 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.2871 - accuracy: 0.9824 - val_loss: 0.2992 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.2798 - accuracy: 0.9823 - val_loss: 0.2894 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.2736 - accuracy: 0.9821 - val_loss: 0.2807 - val_accuracy: 0.9973 - 176ms/epoch - 5ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.2684 - accuracy: 0.9824 - val_loss: 0.2732 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.2644 - accuracy: 0.9820 - val_loss: 0.2660 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.2614 - accuracy: 0.9816 - val_loss: 0.2607 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.2573 - accuracy: 0.9817 - val_loss: 0.2553 - val_accuracy: 0.9973 - 176ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.2542 - accuracy: 0.9814 - val_loss: 0.2514 - val_accuracy: 0.9973 - 193ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.2515 - accuracy: 0.9812 - val_loss: 0.2464 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.2482 - accuracy: 0.9809 - val_loss: 0.2425 - val_accuracy: 0.9973 - 191ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.2463 - accuracy: 0.9813 - val_loss: 0.2392 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.2433 - accuracy: 0.9807 - val_loss: 0.2357 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.2416 - accuracy: 0.9807 - val_loss: 0.2331 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.2401 - accuracy: 0.9803 - val_loss: 0.2297 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.2372 - accuracy: 0.9809 - val_loss: 0.2275 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2366 - accuracy: 0.9810 - val_loss: 0.2252 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2341 - accuracy: 0.9811 - val_loss: 0.2229 - val_accuracy: 0.9973 - 191ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2330 - accuracy: 0.9805 - val_loss: 0.2208 - val_accuracy: 0.9973 - 171ms/epoch - 5ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2300 - accuracy: 0.9802 - val_loss: 0.2185 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2293 - accuracy: 0.9805 - val_loss: 0.2168 - val_accuracy: 0.9973 - 193ms/epoch - 6ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2270 - accuracy: 0.9808 - val_loss: 0.2145 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2258 - accuracy: 0.9809 - val_loss: 0.2122 - val_accuracy: 0.9973 - 180ms/epoch - 6ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2257 - accuracy: 0.9808 - val_loss: 0.2114 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2239 - accuracy: 0.9798 - val_loss: 0.2103 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2221 - accuracy: 0.9805 - val_loss: 0.2078 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2195 - accuracy: 0.9806 - val_loss: 0.2054 - val_accuracy: 0.9973 - 177ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2183 - accuracy: 0.9809 - val_loss: 0.2038 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2167 - accuracy: 0.9805 - val_loss: 0.2021 - val_accuracy: 0.9973 - 169ms/epoch - 5ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2163 - accuracy: 0.9801 - val_loss: 0.2014 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2150 - accuracy: 0.9807 - val_loss: 0.1994 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2128 - accuracy: 0.9806 - val_loss: 0.1980 - val_accuracy: 0.9973 - 170ms/epoch - 5ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2127 - accuracy: 0.9810 - val_loss: 0.1976 - val_accuracy: 0.9973 - 174ms/epoch - 5ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2112 - accuracy: 0.9805 - val_loss: 0.1958 - val_accuracy: 0.9973 - 190ms/epoch - 6ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2082 - accuracy: 0.9813 - val_loss: 0.1937 - val_accuracy: 0.9973 - 179ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2092 - accuracy: 0.9803 - val_loss: 0.1926 - val_accuracy: 0.9973 - 180ms/epoch - 6ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2073 - accuracy: 0.9814 - val_loss: 0.1917 - val_accuracy: 0.9973 - 166ms/epoch - 5ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2062 - accuracy: 0.9808 - val_loss: 0.1906 - val_accuracy: 0.9973 - 168ms/epoch - 5ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2051 - accuracy: 0.9808 - val_loss: 0.1884 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2045 - accuracy: 0.9804 - val_loss: 0.1877 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2015 - accuracy: 0.9815 - val_loss: 0.1860 - val_accuracy: 0.9973 - 169ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2017 - accuracy: 0.9807 - val_loss: 0.1859 - val_accuracy: 0.9973 - 189ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.1996 - accuracy: 0.9803 - val_loss: 0.1845 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2006 - accuracy: 0.9813 - val_loss: 0.1837 - val_accuracy: 0.9978 - 178ms/epoch - 6ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.1987 - accuracy: 0.9814 - val_loss: 0.1818 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.1968 - accuracy: 0.9813 - val_loss: 0.1816 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.1974 - accuracy: 0.9803 - val_loss: 0.1795 - val_accuracy: 0.9973 - 173ms/epoch - 5ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.1966 - accuracy: 0.9808 - val_loss: 0.1789 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.1942 - accuracy: 0.9813 - val_loss: 0.1783 - val_accuracy: 0.9978 - 177ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.1945 - accuracy: 0.9801 - val_loss: 0.1769 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.1920 - accuracy: 0.9810 - val_loss: 0.1756 - val_accuracy: 0.9973 - 167ms/epoch - 5ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.1915 - accuracy: 0.9805 - val_loss: 0.1751 - val_accuracy: 0.9973 - 180ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.1918 - accuracy: 0.9812 - val_loss: 0.1735 - val_accuracy: 0.9973 - 178ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.1903 - accuracy: 0.9812 - val_loss: 0.1738 - val_accuracy: 0.9978 - 175ms/epoch - 5ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.1900 - accuracy: 0.9813 - val_loss: 0.1720 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.1876 - accuracy: 0.9812 - val_loss: 0.1717 - val_accuracy: 0.9978 - 182ms/epoch - 6ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.1879 - accuracy: 0.9816 - val_loss: 0.1696 - val_accuracy: 0.9973 - 175ms/epoch - 5ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.1870 - accuracy: 0.9811 - val_loss: 0.1701 - val_accuracy: 0.9978 - 195ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.1855 - accuracy: 0.9818 - val_loss: 0.1685 - val_accuracy: 0.9978 - 169ms/epoch - 5ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.1853 - accuracy: 0.9813 - val_loss: 0.1679 - val_accuracy: 0.9978 - 179ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.1859 - accuracy: 0.9811 - val_loss: 0.1662 - val_accuracy: 0.9973 - 187ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.1842 - accuracy: 0.9813 - val_loss: 0.1665 - val_accuracy: 0.9978 - 171ms/epoch - 5ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.1841 - accuracy: 0.9806 - val_loss: 0.1658 - val_accuracy: 0.9978 - 168ms/epoch - 5ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.1837 - accuracy: 0.9811 - val_loss: 0.1651 - val_accuracy: 0.9973 - 171ms/epoch - 5ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.1817 - accuracy: 0.9810 - val_loss: 0.1640 - val_accuracy: 0.9978 - 171ms/epoch - 5ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.1804 - accuracy: 0.9815 - val_loss: 0.1632 - val_accuracy: 0.9978 - 175ms/epoch - 5ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.1799 - accuracy: 0.9816 - val_loss: 0.1621 - val_accuracy: 0.9978 - 172ms/epoch - 5ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.1793 - accuracy: 0.9808 - val_loss: 0.1621 - val_accuracy: 0.9978 - 187ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.1807 - accuracy: 0.9797 - val_loss: 0.1623 - val_accuracy: 0.9978 - 187ms/epoch - 6ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.1781 - accuracy: 0.9815 - val_loss: 0.1600 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.1777 - accuracy: 0.9811 - val_loss: 0.1606 - val_accuracy: 0.9978 - 189ms/epoch - 6ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.1773 - accuracy: 0.9808 - val_loss: 0.1601 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.1773 - accuracy: 0.9808 - val_loss: 0.1581 - val_accuracy: 0.9973 - 180ms/epoch - 6ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.1759 - accuracy: 0.9813 - val_loss: 0.1574 - val_accuracy: 0.9978 - 170ms/epoch - 5ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.1756 - accuracy: 0.9816 - val_loss: 0.1573 - val_accuracy: 0.9978 - 163ms/epoch - 5ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.1753 - accuracy: 0.9812 - val_loss: 0.1567 - val_accuracy: 0.9978 - 168ms/epoch - 5ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.1727 - accuracy: 0.9816 - val_loss: 0.1563 - val_accuracy: 0.9978 - 164ms/epoch - 5ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.1722 - accuracy: 0.9816 - val_loss: 0.1545 - val_accuracy: 0.9978 - 171ms/epoch - 5ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.1733 - accuracy: 0.9816 - val_loss: 0.1548 - val_accuracy: 0.9978 - 179ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.1725 - accuracy: 0.9813 - val_loss: 0.1540 - val_accuracy: 0.9978 - 178ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.1721 - accuracy: 0.9814 - val_loss: 0.1532 - val_accuracy: 0.9978 - 177ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.1713 - accuracy: 0.9813 - val_loss: 0.1528 - val_accuracy: 0.9978 - 175ms/epoch - 5ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.1706 - accuracy: 0.9817 - val_loss: 0.1519 - val_accuracy: 0.9978 - 169ms/epoch - 5ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.1707 - accuracy: 0.9813 - val_loss: 0.1518 - val_accuracy: 0.9978 - 173ms/epoch - 5ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.1694 - accuracy: 0.9818 - val_loss: 0.1502 - val_accuracy: 0.9973 - 164ms/epoch - 5ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 64)          2176      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 2, 1, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 128)               0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,305\n",
            "Trainable params: 2,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 1.1105 - accuracy: 0.6982 - val_loss: 0.9673 - val_accuracy: 0.9931 - 615ms/epoch - 38ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8722 - accuracy: 0.8353 - val_loss: 0.7915 - val_accuracy: 0.9967 - 138ms/epoch - 9ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7396 - accuracy: 0.9335 - val_loss: 0.7114 - val_accuracy: 0.9978 - 139ms/epoch - 9ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.6942 - accuracy: 0.9608 - val_loss: 0.6941 - val_accuracy: 0.9937 - 135ms/epoch - 8ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6755 - accuracy: 0.9449 - val_loss: 0.6821 - val_accuracy: 0.9934 - 136ms/epoch - 8ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6598 - accuracy: 0.9311 - val_loss: 0.6703 - val_accuracy: 0.9934 - 140ms/epoch - 9ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6424 - accuracy: 0.9157 - val_loss: 0.6569 - val_accuracy: 0.9923 - 141ms/epoch - 9ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.6235 - accuracy: 0.9075 - val_loss: 0.6411 - val_accuracy: 0.9923 - 135ms/epoch - 8ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.6023 - accuracy: 0.9070 - val_loss: 0.6234 - val_accuracy: 0.9923 - 153ms/epoch - 10ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.5804 - accuracy: 0.9070 - val_loss: 0.6045 - val_accuracy: 0.9923 - 136ms/epoch - 9ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5583 - accuracy: 0.9094 - val_loss: 0.5851 - val_accuracy: 0.9923 - 160ms/epoch - 10ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5363 - accuracy: 0.9119 - val_loss: 0.5656 - val_accuracy: 0.9923 - 140ms/epoch - 9ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5160 - accuracy: 0.9165 - val_loss: 0.5464 - val_accuracy: 0.9923 - 135ms/epoch - 8ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.4969 - accuracy: 0.9253 - val_loss: 0.5281 - val_accuracy: 0.9920 - 141ms/epoch - 9ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.4784 - accuracy: 0.9332 - val_loss: 0.5104 - val_accuracy: 0.9920 - 137ms/epoch - 9ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.4617 - accuracy: 0.9476 - val_loss: 0.4940 - val_accuracy: 0.9920 - 146ms/epoch - 9ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.4466 - accuracy: 0.9638 - val_loss: 0.4784 - val_accuracy: 0.9920 - 143ms/epoch - 9ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4325 - accuracy: 0.9728 - val_loss: 0.4638 - val_accuracy: 0.9920 - 141ms/epoch - 9ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4196 - accuracy: 0.9767 - val_loss: 0.4500 - val_accuracy: 0.9920 - 133ms/epoch - 8ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4069 - accuracy: 0.9770 - val_loss: 0.4369 - val_accuracy: 0.9920 - 135ms/epoch - 8ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.3961 - accuracy: 0.9775 - val_loss: 0.4246 - val_accuracy: 0.9920 - 141ms/epoch - 9ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.3851 - accuracy: 0.9768 - val_loss: 0.4131 - val_accuracy: 0.9920 - 148ms/epoch - 9ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.3751 - accuracy: 0.9775 - val_loss: 0.4025 - val_accuracy: 0.9920 - 143ms/epoch - 9ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.3660 - accuracy: 0.9774 - val_loss: 0.3920 - val_accuracy: 0.9920 - 138ms/epoch - 9ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.3573 - accuracy: 0.9783 - val_loss: 0.3822 - val_accuracy: 0.9920 - 138ms/epoch - 9ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.3495 - accuracy: 0.9783 - val_loss: 0.3733 - val_accuracy: 0.9920 - 144ms/epoch - 9ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.3410 - accuracy: 0.9788 - val_loss: 0.3655 - val_accuracy: 0.9923 - 134ms/epoch - 8ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.3344 - accuracy: 0.9796 - val_loss: 0.3569 - val_accuracy: 0.9923 - 144ms/epoch - 9ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.3279 - accuracy: 0.9797 - val_loss: 0.3494 - val_accuracy: 0.9937 - 142ms/epoch - 9ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3210 - accuracy: 0.9802 - val_loss: 0.3420 - val_accuracy: 0.9937 - 140ms/epoch - 9ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3167 - accuracy: 0.9805 - val_loss: 0.3348 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3107 - accuracy: 0.9804 - val_loss: 0.3281 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3054 - accuracy: 0.9813 - val_loss: 0.3219 - val_accuracy: 0.9973 - 134ms/epoch - 8ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3006 - accuracy: 0.9810 - val_loss: 0.3156 - val_accuracy: 0.9973 - 134ms/epoch - 8ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.2956 - accuracy: 0.9817 - val_loss: 0.3095 - val_accuracy: 0.9973 - 143ms/epoch - 9ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.2914 - accuracy: 0.9815 - val_loss: 0.3040 - val_accuracy: 0.9973 - 142ms/epoch - 9ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.2872 - accuracy: 0.9809 - val_loss: 0.2991 - val_accuracy: 0.9973 - 142ms/epoch - 9ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.2837 - accuracy: 0.9813 - val_loss: 0.2947 - val_accuracy: 0.9973 - 134ms/epoch - 8ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.2797 - accuracy: 0.9816 - val_loss: 0.2901 - val_accuracy: 0.9973 - 143ms/epoch - 9ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.2778 - accuracy: 0.9816 - val_loss: 0.2855 - val_accuracy: 0.9973 - 135ms/epoch - 8ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.2746 - accuracy: 0.9820 - val_loss: 0.2813 - val_accuracy: 0.9973 - 134ms/epoch - 8ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.2710 - accuracy: 0.9817 - val_loss: 0.2777 - val_accuracy: 0.9973 - 136ms/epoch - 8ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.2682 - accuracy: 0.9813 - val_loss: 0.2741 - val_accuracy: 0.9973 - 145ms/epoch - 9ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.2669 - accuracy: 0.9813 - val_loss: 0.2711 - val_accuracy: 0.9973 - 146ms/epoch - 9ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.2652 - accuracy: 0.9816 - val_loss: 0.2678 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.2631 - accuracy: 0.9817 - val_loss: 0.2649 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.2610 - accuracy: 0.9814 - val_loss: 0.2618 - val_accuracy: 0.9973 - 146ms/epoch - 9ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.2597 - accuracy: 0.9814 - val_loss: 0.2592 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.2584 - accuracy: 0.9814 - val_loss: 0.2565 - val_accuracy: 0.9973 - 136ms/epoch - 9ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.2548 - accuracy: 0.9814 - val_loss: 0.2547 - val_accuracy: 0.9973 - 157ms/epoch - 10ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.2550 - accuracy: 0.9813 - val_loss: 0.2522 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.2544 - accuracy: 0.9815 - val_loss: 0.2500 - val_accuracy: 0.9973 - 133ms/epoch - 8ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.2524 - accuracy: 0.9817 - val_loss: 0.2477 - val_accuracy: 0.9973 - 147ms/epoch - 9ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.2519 - accuracy: 0.9809 - val_loss: 0.2460 - val_accuracy: 0.9973 - 134ms/epoch - 8ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.2500 - accuracy: 0.9813 - val_loss: 0.2442 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.2498 - accuracy: 0.9812 - val_loss: 0.2426 - val_accuracy: 0.9973 - 149ms/epoch - 9ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.2480 - accuracy: 0.9806 - val_loss: 0.2407 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.2460 - accuracy: 0.9815 - val_loss: 0.2389 - val_accuracy: 0.9973 - 145ms/epoch - 9ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.2452 - accuracy: 0.9805 - val_loss: 0.2373 - val_accuracy: 0.9973 - 144ms/epoch - 9ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.2445 - accuracy: 0.9811 - val_loss: 0.2356 - val_accuracy: 0.9973 - 153ms/epoch - 10ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.2427 - accuracy: 0.9815 - val_loss: 0.2341 - val_accuracy: 0.9973 - 147ms/epoch - 9ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.2422 - accuracy: 0.9814 - val_loss: 0.2327 - val_accuracy: 0.9973 - 142ms/epoch - 9ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.2412 - accuracy: 0.9816 - val_loss: 0.2314 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.2408 - accuracy: 0.9813 - val_loss: 0.2297 - val_accuracy: 0.9973 - 139ms/epoch - 9ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.2398 - accuracy: 0.9805 - val_loss: 0.2287 - val_accuracy: 0.9973 - 167ms/epoch - 10ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.2371 - accuracy: 0.9818 - val_loss: 0.2276 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2380 - accuracy: 0.9812 - val_loss: 0.2265 - val_accuracy: 0.9973 - 145ms/epoch - 9ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2364 - accuracy: 0.9809 - val_loss: 0.2252 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2358 - accuracy: 0.9810 - val_loss: 0.2236 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2345 - accuracy: 0.9813 - val_loss: 0.2227 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2343 - accuracy: 0.9809 - val_loss: 0.2225 - val_accuracy: 0.9973 - 141ms/epoch - 9ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2326 - accuracy: 0.9814 - val_loss: 0.2210 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2330 - accuracy: 0.9809 - val_loss: 0.2199 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2311 - accuracy: 0.9809 - val_loss: 0.2189 - val_accuracy: 0.9973 - 133ms/epoch - 8ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2320 - accuracy: 0.9811 - val_loss: 0.2184 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2299 - accuracy: 0.9812 - val_loss: 0.2178 - val_accuracy: 0.9973 - 136ms/epoch - 9ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2275 - accuracy: 0.9812 - val_loss: 0.2162 - val_accuracy: 0.9973 - 139ms/epoch - 9ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2289 - accuracy: 0.9806 - val_loss: 0.2147 - val_accuracy: 0.9973 - 139ms/epoch - 9ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2282 - accuracy: 0.9803 - val_loss: 0.2139 - val_accuracy: 0.9973 - 141ms/epoch - 9ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2255 - accuracy: 0.9815 - val_loss: 0.2129 - val_accuracy: 0.9973 - 138ms/epoch - 9ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2248 - accuracy: 0.9814 - val_loss: 0.2114 - val_accuracy: 0.9973 - 147ms/epoch - 9ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2244 - accuracy: 0.9802 - val_loss: 0.2108 - val_accuracy: 0.9973 - 136ms/epoch - 9ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2240 - accuracy: 0.9811 - val_loss: 0.2093 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2224 - accuracy: 0.9810 - val_loss: 0.2089 - val_accuracy: 0.9973 - 136ms/epoch - 9ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2239 - accuracy: 0.9802 - val_loss: 0.2082 - val_accuracy: 0.9973 - 132ms/epoch - 8ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2223 - accuracy: 0.9809 - val_loss: 0.2080 - val_accuracy: 0.9973 - 148ms/epoch - 9ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2220 - accuracy: 0.9807 - val_loss: 0.2067 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2211 - accuracy: 0.9801 - val_loss: 0.2057 - val_accuracy: 0.9973 - 141ms/epoch - 9ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2205 - accuracy: 0.9806 - val_loss: 0.2049 - val_accuracy: 0.9973 - 140ms/epoch - 9ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2178 - accuracy: 0.9817 - val_loss: 0.2040 - val_accuracy: 0.9973 - 146ms/epoch - 9ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2185 - accuracy: 0.9810 - val_loss: 0.2027 - val_accuracy: 0.9973 - 143ms/epoch - 9ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2177 - accuracy: 0.9812 - val_loss: 0.2019 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2169 - accuracy: 0.9810 - val_loss: 0.2012 - val_accuracy: 0.9973 - 144ms/epoch - 9ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2167 - accuracy: 0.9808 - val_loss: 0.2005 - val_accuracy: 0.9973 - 132ms/epoch - 8ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2153 - accuracy: 0.9808 - val_loss: 0.1998 - val_accuracy: 0.9973 - 153ms/epoch - 10ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2151 - accuracy: 0.9808 - val_loss: 0.1996 - val_accuracy: 0.9973 - 142ms/epoch - 9ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2139 - accuracy: 0.9811 - val_loss: 0.1985 - val_accuracy: 0.9973 - 147ms/epoch - 9ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2133 - accuracy: 0.9805 - val_loss: 0.1975 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2125 - accuracy: 0.9810 - val_loss: 0.1971 - val_accuracy: 0.9973 - 137ms/epoch - 9ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2130 - accuracy: 0.9804 - val_loss: 0.1955 - val_accuracy: 0.9973 - 139ms/epoch - 9ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 64)          2176      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,241\n",
            "Trainable params: 2,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "32/32 - 1s - loss: 0.9896 - accuracy: 0.8528 - val_loss: 0.7902 - val_accuracy: 0.9978 - 661ms/epoch - 21ms/step\n",
            "Epoch: 2/100\n",
            "32/32 - 0s - loss: 0.7065 - accuracy: 0.9783 - val_loss: 0.6869 - val_accuracy: 0.9970 - 188ms/epoch - 6ms/step\n",
            "Epoch: 3/100\n",
            "32/32 - 0s - loss: 0.6523 - accuracy: 0.9784 - val_loss: 0.6625 - val_accuracy: 0.9934 - 180ms/epoch - 6ms/step\n",
            "Epoch: 4/100\n",
            "32/32 - 0s - loss: 0.6178 - accuracy: 0.9607 - val_loss: 0.6366 - val_accuracy: 0.9934 - 198ms/epoch - 6ms/step\n",
            "Epoch: 5/100\n",
            "32/32 - 0s - loss: 0.5804 - accuracy: 0.9308 - val_loss: 0.6062 - val_accuracy: 0.9934 - 184ms/epoch - 6ms/step\n",
            "Epoch: 6/100\n",
            "32/32 - 0s - loss: 0.5420 - accuracy: 0.9431 - val_loss: 0.5733 - val_accuracy: 0.9934 - 190ms/epoch - 6ms/step\n",
            "Epoch: 7/100\n",
            "32/32 - 0s - loss: 0.5049 - accuracy: 0.9659 - val_loss: 0.5410 - val_accuracy: 0.9934 - 188ms/epoch - 6ms/step\n",
            "Epoch: 8/100\n",
            "32/32 - 0s - loss: 0.4731 - accuracy: 0.9797 - val_loss: 0.5113 - val_accuracy: 0.9923 - 204ms/epoch - 6ms/step\n",
            "Epoch: 9/100\n",
            "32/32 - 0s - loss: 0.4442 - accuracy: 0.9804 - val_loss: 0.4845 - val_accuracy: 0.9923 - 199ms/epoch - 6ms/step\n",
            "Epoch: 10/100\n",
            "32/32 - 0s - loss: 0.4205 - accuracy: 0.9810 - val_loss: 0.4608 - val_accuracy: 0.9923 - 193ms/epoch - 6ms/step\n",
            "Epoch: 11/100\n",
            "32/32 - 0s - loss: 0.4004 - accuracy: 0.9814 - val_loss: 0.4404 - val_accuracy: 0.9923 - 194ms/epoch - 6ms/step\n",
            "Epoch: 12/100\n",
            "32/32 - 0s - loss: 0.3829 - accuracy: 0.9830 - val_loss: 0.4223 - val_accuracy: 0.9923 - 180ms/epoch - 6ms/step\n",
            "Epoch: 13/100\n",
            "32/32 - 0s - loss: 0.3678 - accuracy: 0.9831 - val_loss: 0.4063 - val_accuracy: 0.9923 - 188ms/epoch - 6ms/step\n",
            "Epoch: 14/100\n",
            "32/32 - 0s - loss: 0.3550 - accuracy: 0.9837 - val_loss: 0.3920 - val_accuracy: 0.9934 - 193ms/epoch - 6ms/step\n",
            "Epoch: 15/100\n",
            "32/32 - 0s - loss: 0.3428 - accuracy: 0.9835 - val_loss: 0.3794 - val_accuracy: 0.9973 - 199ms/epoch - 6ms/step\n",
            "Epoch: 16/100\n",
            "32/32 - 0s - loss: 0.3342 - accuracy: 0.9841 - val_loss: 0.3682 - val_accuracy: 0.9973 - 202ms/epoch - 6ms/step\n",
            "Epoch: 17/100\n",
            "32/32 - 0s - loss: 0.3256 - accuracy: 0.9839 - val_loss: 0.3582 - val_accuracy: 0.9973 - 190ms/epoch - 6ms/step\n",
            "Epoch: 18/100\n",
            "32/32 - 0s - loss: 0.3171 - accuracy: 0.9845 - val_loss: 0.3492 - val_accuracy: 0.9973 - 204ms/epoch - 6ms/step\n",
            "Epoch: 19/100\n",
            "32/32 - 0s - loss: 0.3102 - accuracy: 0.9843 - val_loss: 0.3409 - val_accuracy: 0.9973 - 192ms/epoch - 6ms/step\n",
            "Epoch: 20/100\n",
            "32/32 - 0s - loss: 0.3044 - accuracy: 0.9851 - val_loss: 0.3330 - val_accuracy: 0.9973 - 185ms/epoch - 6ms/step\n",
            "Epoch: 21/100\n",
            "32/32 - 0s - loss: 0.3004 - accuracy: 0.9844 - val_loss: 0.3264 - val_accuracy: 0.9973 - 198ms/epoch - 6ms/step\n",
            "Epoch: 22/100\n",
            "32/32 - 0s - loss: 0.2939 - accuracy: 0.9844 - val_loss: 0.3199 - val_accuracy: 0.9973 - 198ms/epoch - 6ms/step\n",
            "Epoch: 23/100\n",
            "32/32 - 0s - loss: 0.2897 - accuracy: 0.9839 - val_loss: 0.3139 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 24/100\n",
            "32/32 - 0s - loss: 0.2861 - accuracy: 0.9845 - val_loss: 0.3083 - val_accuracy: 0.9973 - 179ms/epoch - 6ms/step\n",
            "Epoch: 25/100\n",
            "32/32 - 0s - loss: 0.2814 - accuracy: 0.9844 - val_loss: 0.3031 - val_accuracy: 0.9978 - 193ms/epoch - 6ms/step\n",
            "Epoch: 26/100\n",
            "32/32 - 0s - loss: 0.2771 - accuracy: 0.9844 - val_loss: 0.2983 - val_accuracy: 0.9978 - 185ms/epoch - 6ms/step\n",
            "Epoch: 27/100\n",
            "32/32 - 0s - loss: 0.2746 - accuracy: 0.9844 - val_loss: 0.2940 - val_accuracy: 0.9978 - 182ms/epoch - 6ms/step\n",
            "Epoch: 28/100\n",
            "32/32 - 0s - loss: 0.2727 - accuracy: 0.9839 - val_loss: 0.2901 - val_accuracy: 0.9978 - 189ms/epoch - 6ms/step\n",
            "Epoch: 29/100\n",
            "32/32 - 0s - loss: 0.2688 - accuracy: 0.9842 - val_loss: 0.2866 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 30/100\n",
            "32/32 - 0s - loss: 0.2673 - accuracy: 0.9841 - val_loss: 0.2832 - val_accuracy: 0.9978 - 211ms/epoch - 7ms/step\n",
            "Epoch: 31/100\n",
            "32/32 - 0s - loss: 0.2645 - accuracy: 0.9844 - val_loss: 0.2800 - val_accuracy: 0.9978 - 173ms/epoch - 5ms/step\n",
            "Epoch: 32/100\n",
            "32/32 - 0s - loss: 0.2628 - accuracy: 0.9846 - val_loss: 0.2771 - val_accuracy: 0.9978 - 186ms/epoch - 6ms/step\n",
            "Epoch: 33/100\n",
            "32/32 - 0s - loss: 0.2596 - accuracy: 0.9842 - val_loss: 0.2742 - val_accuracy: 0.9978 - 179ms/epoch - 6ms/step\n",
            "Epoch: 34/100\n",
            "32/32 - 0s - loss: 0.2582 - accuracy: 0.9841 - val_loss: 0.2716 - val_accuracy: 0.9978 - 188ms/epoch - 6ms/step\n",
            "Epoch: 35/100\n",
            "32/32 - 0s - loss: 0.2557 - accuracy: 0.9841 - val_loss: 0.2690 - val_accuracy: 0.9978 - 180ms/epoch - 6ms/step\n",
            "Epoch: 36/100\n",
            "32/32 - 0s - loss: 0.2551 - accuracy: 0.9842 - val_loss: 0.2666 - val_accuracy: 0.9978 - 194ms/epoch - 6ms/step\n",
            "Epoch: 37/100\n",
            "32/32 - 0s - loss: 0.2518 - accuracy: 0.9844 - val_loss: 0.2642 - val_accuracy: 0.9978 - 203ms/epoch - 6ms/step\n",
            "Epoch: 38/100\n",
            "32/32 - 0s - loss: 0.2519 - accuracy: 0.9835 - val_loss: 0.2620 - val_accuracy: 0.9978 - 195ms/epoch - 6ms/step\n",
            "Epoch: 39/100\n",
            "32/32 - 0s - loss: 0.2490 - accuracy: 0.9846 - val_loss: 0.2597 - val_accuracy: 0.9978 - 209ms/epoch - 7ms/step\n",
            "Epoch: 40/100\n",
            "32/32 - 0s - loss: 0.2463 - accuracy: 0.9846 - val_loss: 0.2573 - val_accuracy: 0.9978 - 191ms/epoch - 6ms/step\n",
            "Epoch: 41/100\n",
            "32/32 - 0s - loss: 0.2452 - accuracy: 0.9841 - val_loss: 0.2550 - val_accuracy: 0.9978 - 177ms/epoch - 6ms/step\n",
            "Epoch: 42/100\n",
            "32/32 - 0s - loss: 0.2424 - accuracy: 0.9840 - val_loss: 0.2529 - val_accuracy: 0.9978 - 190ms/epoch - 6ms/step\n",
            "Epoch: 43/100\n",
            "32/32 - 0s - loss: 0.2412 - accuracy: 0.9844 - val_loss: 0.2507 - val_accuracy: 0.9978 - 194ms/epoch - 6ms/step\n",
            "Epoch: 44/100\n",
            "32/32 - 0s - loss: 0.2400 - accuracy: 0.9839 - val_loss: 0.2487 - val_accuracy: 0.9978 - 188ms/epoch - 6ms/step\n",
            "Epoch: 45/100\n",
            "32/32 - 0s - loss: 0.2373 - accuracy: 0.9839 - val_loss: 0.2466 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 46/100\n",
            "32/32 - 0s - loss: 0.2378 - accuracy: 0.9837 - val_loss: 0.2445 - val_accuracy: 0.9978 - 190ms/epoch - 6ms/step\n",
            "Epoch: 47/100\n",
            "32/32 - 0s - loss: 0.2349 - accuracy: 0.9842 - val_loss: 0.2426 - val_accuracy: 0.9978 - 185ms/epoch - 6ms/step\n",
            "Epoch: 48/100\n",
            "32/32 - 0s - loss: 0.2333 - accuracy: 0.9838 - val_loss: 0.2406 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 49/100\n",
            "32/32 - 0s - loss: 0.2312 - accuracy: 0.9838 - val_loss: 0.2385 - val_accuracy: 0.9978 - 180ms/epoch - 6ms/step\n",
            "Epoch: 50/100\n",
            "32/32 - 0s - loss: 0.2305 - accuracy: 0.9837 - val_loss: 0.2364 - val_accuracy: 0.9978 - 185ms/epoch - 6ms/step\n",
            "Epoch: 51/100\n",
            "32/32 - 0s - loss: 0.2290 - accuracy: 0.9837 - val_loss: 0.2346 - val_accuracy: 0.9978 - 197ms/epoch - 6ms/step\n",
            "Epoch: 52/100\n",
            "32/32 - 0s - loss: 0.2270 - accuracy: 0.9838 - val_loss: 0.2323 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 53/100\n",
            "32/32 - 0s - loss: 0.2243 - accuracy: 0.9835 - val_loss: 0.2303 - val_accuracy: 0.9978 - 193ms/epoch - 6ms/step\n",
            "Epoch: 54/100\n",
            "32/32 - 0s - loss: 0.2235 - accuracy: 0.9836 - val_loss: 0.2283 - val_accuracy: 0.9978 - 187ms/epoch - 6ms/step\n",
            "Epoch: 55/100\n",
            "32/32 - 0s - loss: 0.2225 - accuracy: 0.9832 - val_loss: 0.2265 - val_accuracy: 0.9978 - 174ms/epoch - 5ms/step\n",
            "Epoch: 56/100\n",
            "32/32 - 0s - loss: 0.2207 - accuracy: 0.9839 - val_loss: 0.2244 - val_accuracy: 0.9978 - 171ms/epoch - 5ms/step\n",
            "Epoch: 57/100\n",
            "32/32 - 0s - loss: 0.2195 - accuracy: 0.9832 - val_loss: 0.2224 - val_accuracy: 0.9978 - 193ms/epoch - 6ms/step\n",
            "Epoch: 58/100\n",
            "32/32 - 0s - loss: 0.2176 - accuracy: 0.9840 - val_loss: 0.2205 - val_accuracy: 0.9978 - 187ms/epoch - 6ms/step\n",
            "Epoch: 59/100\n",
            "32/32 - 0s - loss: 0.2165 - accuracy: 0.9837 - val_loss: 0.2187 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 60/100\n",
            "32/32 - 0s - loss: 0.2160 - accuracy: 0.9827 - val_loss: 0.2173 - val_accuracy: 0.9978 - 191ms/epoch - 6ms/step\n",
            "Epoch: 61/100\n",
            "32/32 - 0s - loss: 0.2134 - accuracy: 0.9833 - val_loss: 0.2153 - val_accuracy: 0.9973 - 176ms/epoch - 5ms/step\n",
            "Epoch: 62/100\n",
            "32/32 - 0s - loss: 0.2137 - accuracy: 0.9827 - val_loss: 0.2138 - val_accuracy: 0.9973 - 196ms/epoch - 6ms/step\n",
            "Epoch: 63/100\n",
            "32/32 - 0s - loss: 0.2118 - accuracy: 0.9839 - val_loss: 0.2122 - val_accuracy: 0.9978 - 184ms/epoch - 6ms/step\n",
            "Epoch: 64/100\n",
            "32/32 - 0s - loss: 0.2109 - accuracy: 0.9834 - val_loss: 0.2106 - val_accuracy: 0.9978 - 191ms/epoch - 6ms/step\n",
            "Epoch: 65/100\n",
            "32/32 - 0s - loss: 0.2100 - accuracy: 0.9833 - val_loss: 0.2090 - val_accuracy: 0.9973 - 201ms/epoch - 6ms/step\n",
            "Epoch: 66/100\n",
            "32/32 - 0s - loss: 0.2083 - accuracy: 0.9832 - val_loss: 0.2076 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 67/100\n",
            "32/32 - 0s - loss: 0.2065 - accuracy: 0.9827 - val_loss: 0.2060 - val_accuracy: 0.9973 - 196ms/epoch - 6ms/step\n",
            "Epoch: 68/100\n",
            "32/32 - 0s - loss: 0.2058 - accuracy: 0.9835 - val_loss: 0.2046 - val_accuracy: 0.9973 - 180ms/epoch - 6ms/step\n",
            "Epoch: 69/100\n",
            "32/32 - 0s - loss: 0.2054 - accuracy: 0.9832 - val_loss: 0.2032 - val_accuracy: 0.9973 - 192ms/epoch - 6ms/step\n",
            "Epoch: 70/100\n",
            "32/32 - 0s - loss: 0.2032 - accuracy: 0.9826 - val_loss: 0.2020 - val_accuracy: 0.9978 - 175ms/epoch - 5ms/step\n",
            "Epoch: 71/100\n",
            "32/32 - 0s - loss: 0.2019 - accuracy: 0.9830 - val_loss: 0.2005 - val_accuracy: 0.9973 - 242ms/epoch - 8ms/step\n",
            "Epoch: 72/100\n",
            "32/32 - 0s - loss: 0.2016 - accuracy: 0.9831 - val_loss: 0.1994 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 73/100\n",
            "32/32 - 0s - loss: 0.2003 - accuracy: 0.9829 - val_loss: 0.1981 - val_accuracy: 0.9973 - 191ms/epoch - 6ms/step\n",
            "Epoch: 74/100\n",
            "32/32 - 0s - loss: 0.2002 - accuracy: 0.9833 - val_loss: 0.1970 - val_accuracy: 0.9973 - 202ms/epoch - 6ms/step\n",
            "Epoch: 75/100\n",
            "32/32 - 0s - loss: 0.2001 - accuracy: 0.9831 - val_loss: 0.1961 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 76/100\n",
            "32/32 - 0s - loss: 0.1976 - accuracy: 0.9832 - val_loss: 0.1949 - val_accuracy: 0.9973 - 195ms/epoch - 6ms/step\n",
            "Epoch: 77/100\n",
            "32/32 - 0s - loss: 0.1975 - accuracy: 0.9830 - val_loss: 0.1940 - val_accuracy: 0.9978 - 199ms/epoch - 6ms/step\n",
            "Epoch: 78/100\n",
            "32/32 - 0s - loss: 0.1950 - accuracy: 0.9837 - val_loss: 0.1926 - val_accuracy: 0.9973 - 178ms/epoch - 6ms/step\n",
            "Epoch: 79/100\n",
            "32/32 - 0s - loss: 0.1958 - accuracy: 0.9832 - val_loss: 0.1915 - val_accuracy: 0.9973 - 191ms/epoch - 6ms/step\n",
            "Epoch: 80/100\n",
            "32/32 - 0s - loss: 0.1946 - accuracy: 0.9832 - val_loss: 0.1904 - val_accuracy: 0.9973 - 198ms/epoch - 6ms/step\n",
            "Epoch: 81/100\n",
            "32/32 - 0s - loss: 0.1949 - accuracy: 0.9823 - val_loss: 0.1896 - val_accuracy: 0.9973 - 192ms/epoch - 6ms/step\n",
            "Epoch: 82/100\n",
            "32/32 - 0s - loss: 0.1941 - accuracy: 0.9827 - val_loss: 0.1886 - val_accuracy: 0.9973 - 184ms/epoch - 6ms/step\n",
            "Epoch: 83/100\n",
            "32/32 - 0s - loss: 0.1931 - accuracy: 0.9834 - val_loss: 0.1879 - val_accuracy: 0.9973 - 185ms/epoch - 6ms/step\n",
            "Epoch: 84/100\n",
            "32/32 - 0s - loss: 0.1933 - accuracy: 0.9820 - val_loss: 0.1870 - val_accuracy: 0.9973 - 182ms/epoch - 6ms/step\n",
            "Epoch: 85/100\n",
            "32/32 - 0s - loss: 0.1914 - accuracy: 0.9827 - val_loss: 0.1858 - val_accuracy: 0.9973 - 204ms/epoch - 6ms/step\n",
            "Epoch: 86/100\n",
            "32/32 - 0s - loss: 0.1900 - accuracy: 0.9831 - val_loss: 0.1847 - val_accuracy: 0.9973 - 195ms/epoch - 6ms/step\n",
            "Epoch: 87/100\n",
            "32/32 - 0s - loss: 0.1906 - accuracy: 0.9828 - val_loss: 0.1837 - val_accuracy: 0.9973 - 197ms/epoch - 6ms/step\n",
            "Epoch: 88/100\n",
            "32/32 - 0s - loss: 0.1893 - accuracy: 0.9831 - val_loss: 0.1828 - val_accuracy: 0.9973 - 208ms/epoch - 7ms/step\n",
            "Epoch: 89/100\n",
            "32/32 - 0s - loss: 0.1885 - accuracy: 0.9826 - val_loss: 0.1816 - val_accuracy: 0.9973 - 191ms/epoch - 6ms/step\n",
            "Epoch: 90/100\n",
            "32/32 - 0s - loss: 0.1879 - accuracy: 0.9826 - val_loss: 0.1810 - val_accuracy: 0.9973 - 193ms/epoch - 6ms/step\n",
            "Epoch: 91/100\n",
            "32/32 - 0s - loss: 0.1871 - accuracy: 0.9832 - val_loss: 0.1801 - val_accuracy: 0.9973 - 196ms/epoch - 6ms/step\n",
            "Epoch: 92/100\n",
            "32/32 - 0s - loss: 0.1871 - accuracy: 0.9831 - val_loss: 0.1797 - val_accuracy: 0.9973 - 181ms/epoch - 6ms/step\n",
            "Epoch: 93/100\n",
            "32/32 - 0s - loss: 0.1864 - accuracy: 0.9828 - val_loss: 0.1785 - val_accuracy: 0.9973 - 189ms/epoch - 6ms/step\n",
            "Epoch: 94/100\n",
            "32/32 - 0s - loss: 0.1857 - accuracy: 0.9833 - val_loss: 0.1778 - val_accuracy: 0.9973 - 193ms/epoch - 6ms/step\n",
            "Epoch: 95/100\n",
            "32/32 - 0s - loss: 0.1851 - accuracy: 0.9828 - val_loss: 0.1769 - val_accuracy: 0.9973 - 178ms/epoch - 6ms/step\n",
            "Epoch: 96/100\n",
            "32/32 - 0s - loss: 0.1833 - accuracy: 0.9833 - val_loss: 0.1761 - val_accuracy: 0.9973 - 183ms/epoch - 6ms/step\n",
            "Epoch: 97/100\n",
            "32/32 - 0s - loss: 0.1832 - accuracy: 0.9835 - val_loss: 0.1751 - val_accuracy: 0.9973 - 185ms/epoch - 6ms/step\n",
            "Epoch: 98/100\n",
            "32/32 - 0s - loss: 0.1827 - accuracy: 0.9832 - val_loss: 0.1741 - val_accuracy: 0.9973 - 187ms/epoch - 6ms/step\n",
            "Epoch: 99/100\n",
            "32/32 - 0s - loss: 0.1814 - accuracy: 0.9831 - val_loss: 0.1733 - val_accuracy: 0.9973 - 192ms/epoch - 6ms/step\n",
            "Epoch: 100/100\n",
            "32/32 - 0s - loss: 0.1826 - accuracy: 0.9831 - val_loss: 0.1726 - val_accuracy: 0.9973 - 186ms/epoch - 6ms/step\n",
            "Model: \"DOS2019-LUCID\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv0 (Conv2D)              (None, 8, 1, 64)          2176      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " activation (Activation)     (None, 8, 1, 64)          0         \n",
            "                                                                 \n",
            " mp0 (MaxPooling2D)          (None, 1, 1, 64)          0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 64)                0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,241\n",
            "Trainable params: 2,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "Epoch: 1/100\n",
            "16/16 - 1s - loss: 1.1035 - accuracy: 0.6856 - val_loss: 0.9658 - val_accuracy: 0.9978 - 665ms/epoch - 42ms/step\n",
            "Epoch: 2/100\n",
            "16/16 - 0s - loss: 0.8654 - accuracy: 0.9092 - val_loss: 0.7883 - val_accuracy: 0.9934 - 161ms/epoch - 10ms/step\n",
            "Epoch: 3/100\n",
            "16/16 - 0s - loss: 0.7278 - accuracy: 0.9173 - val_loss: 0.7047 - val_accuracy: 0.9934 - 165ms/epoch - 10ms/step\n",
            "Epoch: 4/100\n",
            "16/16 - 0s - loss: 0.6756 - accuracy: 0.9185 - val_loss: 0.6838 - val_accuracy: 0.9934 - 146ms/epoch - 9ms/step\n",
            "Epoch: 5/100\n",
            "16/16 - 0s - loss: 0.6529 - accuracy: 0.9126 - val_loss: 0.6693 - val_accuracy: 0.9934 - 154ms/epoch - 10ms/step\n",
            "Epoch: 6/100\n",
            "16/16 - 0s - loss: 0.6348 - accuracy: 0.9114 - val_loss: 0.6565 - val_accuracy: 0.9934 - 156ms/epoch - 10ms/step\n",
            "Epoch: 7/100\n",
            "16/16 - 0s - loss: 0.6176 - accuracy: 0.9111 - val_loss: 0.6434 - val_accuracy: 0.9934 - 156ms/epoch - 10ms/step\n",
            "Epoch: 8/100\n",
            "16/16 - 0s - loss: 0.5995 - accuracy: 0.9111 - val_loss: 0.6289 - val_accuracy: 0.9923 - 152ms/epoch - 10ms/step\n",
            "Epoch: 9/100\n",
            "16/16 - 0s - loss: 0.5818 - accuracy: 0.9111 - val_loss: 0.6142 - val_accuracy: 0.9923 - 146ms/epoch - 9ms/step\n",
            "Epoch: 10/100\n",
            "16/16 - 0s - loss: 0.5630 - accuracy: 0.9108 - val_loss: 0.5984 - val_accuracy: 0.9923 - 147ms/epoch - 9ms/step\n",
            "Epoch: 11/100\n",
            "16/16 - 0s - loss: 0.5440 - accuracy: 0.9108 - val_loss: 0.5825 - val_accuracy: 0.9923 - 157ms/epoch - 10ms/step\n",
            "Epoch: 12/100\n",
            "16/16 - 0s - loss: 0.5264 - accuracy: 0.9110 - val_loss: 0.5666 - val_accuracy: 0.9923 - 156ms/epoch - 10ms/step\n",
            "Epoch: 13/100\n",
            "16/16 - 0s - loss: 0.5089 - accuracy: 0.9111 - val_loss: 0.5511 - val_accuracy: 0.9923 - 149ms/epoch - 9ms/step\n",
            "Epoch: 14/100\n",
            "16/16 - 0s - loss: 0.4929 - accuracy: 0.9109 - val_loss: 0.5359 - val_accuracy: 0.9923 - 149ms/epoch - 9ms/step\n",
            "Epoch: 15/100\n",
            "16/16 - 0s - loss: 0.4778 - accuracy: 0.9112 - val_loss: 0.5215 - val_accuracy: 0.9923 - 156ms/epoch - 10ms/step\n",
            "Epoch: 16/100\n",
            "16/16 - 0s - loss: 0.4635 - accuracy: 0.9304 - val_loss: 0.5075 - val_accuracy: 0.9923 - 151ms/epoch - 9ms/step\n",
            "Epoch: 17/100\n",
            "16/16 - 0s - loss: 0.4504 - accuracy: 0.9759 - val_loss: 0.4943 - val_accuracy: 0.9923 - 149ms/epoch - 9ms/step\n",
            "Epoch: 18/100\n",
            "16/16 - 0s - loss: 0.4384 - accuracy: 0.9788 - val_loss: 0.4823 - val_accuracy: 0.9923 - 168ms/epoch - 11ms/step\n",
            "Epoch: 19/100\n",
            "16/16 - 0s - loss: 0.4282 - accuracy: 0.9785 - val_loss: 0.4704 - val_accuracy: 0.9920 - 146ms/epoch - 9ms/step\n",
            "Epoch: 20/100\n",
            "16/16 - 0s - loss: 0.4171 - accuracy: 0.9787 - val_loss: 0.4599 - val_accuracy: 0.9920 - 160ms/epoch - 10ms/step\n",
            "Epoch: 21/100\n",
            "16/16 - 0s - loss: 0.4083 - accuracy: 0.9790 - val_loss: 0.4499 - val_accuracy: 0.9920 - 154ms/epoch - 10ms/step\n",
            "Epoch: 22/100\n",
            "16/16 - 0s - loss: 0.3983 - accuracy: 0.9791 - val_loss: 0.4405 - val_accuracy: 0.9920 - 145ms/epoch - 9ms/step\n",
            "Epoch: 23/100\n",
            "16/16 - 0s - loss: 0.3902 - accuracy: 0.9786 - val_loss: 0.4319 - val_accuracy: 0.9920 - 144ms/epoch - 9ms/step\n",
            "Epoch: 24/100\n",
            "16/16 - 0s - loss: 0.3829 - accuracy: 0.9786 - val_loss: 0.4236 - val_accuracy: 0.9920 - 143ms/epoch - 9ms/step\n",
            "Epoch: 25/100\n",
            "16/16 - 0s - loss: 0.3752 - accuracy: 0.9801 - val_loss: 0.4158 - val_accuracy: 0.9920 - 143ms/epoch - 9ms/step\n",
            "Epoch: 26/100\n",
            "16/16 - 0s - loss: 0.3686 - accuracy: 0.9801 - val_loss: 0.4084 - val_accuracy: 0.9920 - 155ms/epoch - 10ms/step\n",
            "Epoch: 27/100\n",
            "16/16 - 0s - loss: 0.3623 - accuracy: 0.9803 - val_loss: 0.4011 - val_accuracy: 0.9920 - 144ms/epoch - 9ms/step\n",
            "Epoch: 28/100\n",
            "16/16 - 0s - loss: 0.3556 - accuracy: 0.9816 - val_loss: 0.3944 - val_accuracy: 0.9920 - 142ms/epoch - 9ms/step\n",
            "Epoch: 29/100\n",
            "16/16 - 0s - loss: 0.3507 - accuracy: 0.9814 - val_loss: 0.3882 - val_accuracy: 0.9920 - 156ms/epoch - 10ms/step\n",
            "Epoch: 30/100\n",
            "16/16 - 0s - loss: 0.3451 - accuracy: 0.9828 - val_loss: 0.3820 - val_accuracy: 0.9920 - 142ms/epoch - 9ms/step\n",
            "Epoch: 31/100\n",
            "16/16 - 0s - loss: 0.3407 - accuracy: 0.9831 - val_loss: 0.3765 - val_accuracy: 0.9920 - 149ms/epoch - 9ms/step\n",
            "Epoch: 32/100\n",
            "16/16 - 0s - loss: 0.3365 - accuracy: 0.9845 - val_loss: 0.3709 - val_accuracy: 0.9920 - 143ms/epoch - 9ms/step\n",
            "Epoch: 33/100\n",
            "16/16 - 0s - loss: 0.3307 - accuracy: 0.9839 - val_loss: 0.3656 - val_accuracy: 0.9920 - 142ms/epoch - 9ms/step\n",
            "Epoch: 34/100\n",
            "16/16 - 0s - loss: 0.3268 - accuracy: 0.9842 - val_loss: 0.3606 - val_accuracy: 0.9934 - 139ms/epoch - 9ms/step\n",
            "Epoch: 35/100\n",
            "16/16 - 0s - loss: 0.3221 - accuracy: 0.9848 - val_loss: 0.3559 - val_accuracy: 0.9973 - 153ms/epoch - 10ms/step\n",
            "Epoch: 36/100\n",
            "16/16 - 0s - loss: 0.3200 - accuracy: 0.9845 - val_loss: 0.3512 - val_accuracy: 0.9973 - 142ms/epoch - 9ms/step\n",
            "Epoch: 37/100\n",
            "16/16 - 0s - loss: 0.3149 - accuracy: 0.9847 - val_loss: 0.3467 - val_accuracy: 0.9973 - 164ms/epoch - 10ms/step\n",
            "Epoch: 38/100\n",
            "16/16 - 0s - loss: 0.3121 - accuracy: 0.9842 - val_loss: 0.3424 - val_accuracy: 0.9973 - 154ms/epoch - 10ms/step\n",
            "Epoch: 39/100\n",
            "16/16 - 0s - loss: 0.3097 - accuracy: 0.9843 - val_loss: 0.3383 - val_accuracy: 0.9973 - 151ms/epoch - 9ms/step\n",
            "Epoch: 40/100\n",
            "16/16 - 0s - loss: 0.3061 - accuracy: 0.9839 - val_loss: 0.3344 - val_accuracy: 0.9973 - 160ms/epoch - 10ms/step\n",
            "Epoch: 41/100\n",
            "16/16 - 0s - loss: 0.3018 - accuracy: 0.9842 - val_loss: 0.3307 - val_accuracy: 0.9973 - 152ms/epoch - 10ms/step\n",
            "Epoch: 42/100\n",
            "16/16 - 0s - loss: 0.2991 - accuracy: 0.9842 - val_loss: 0.3271 - val_accuracy: 0.9973 - 155ms/epoch - 10ms/step\n",
            "Epoch: 43/100\n",
            "16/16 - 0s - loss: 0.2960 - accuracy: 0.9839 - val_loss: 0.3235 - val_accuracy: 0.9973 - 157ms/epoch - 10ms/step\n",
            "Epoch: 44/100\n",
            "16/16 - 0s - loss: 0.2935 - accuracy: 0.9838 - val_loss: 0.3202 - val_accuracy: 0.9973 - 157ms/epoch - 10ms/step\n",
            "Epoch: 45/100\n",
            "16/16 - 0s - loss: 0.2921 - accuracy: 0.9839 - val_loss: 0.3169 - val_accuracy: 0.9973 - 150ms/epoch - 9ms/step\n",
            "Epoch: 46/100\n",
            "16/16 - 0s - loss: 0.2886 - accuracy: 0.9840 - val_loss: 0.3139 - val_accuracy: 0.9973 - 154ms/epoch - 10ms/step\n",
            "Epoch: 47/100\n",
            "16/16 - 0s - loss: 0.2873 - accuracy: 0.9847 - val_loss: 0.3107 - val_accuracy: 0.9973 - 153ms/epoch - 10ms/step\n",
            "Epoch: 48/100\n",
            "16/16 - 0s - loss: 0.2846 - accuracy: 0.9841 - val_loss: 0.3078 - val_accuracy: 0.9973 - 150ms/epoch - 9ms/step\n",
            "Epoch: 49/100\n",
            "16/16 - 0s - loss: 0.2831 - accuracy: 0.9841 - val_loss: 0.3050 - val_accuracy: 0.9973 - 150ms/epoch - 9ms/step\n",
            "Epoch: 50/100\n",
            "16/16 - 0s - loss: 0.2812 - accuracy: 0.9839 - val_loss: 0.3024 - val_accuracy: 0.9973 - 149ms/epoch - 9ms/step\n",
            "Epoch: 51/100\n",
            "16/16 - 0s - loss: 0.2794 - accuracy: 0.9841 - val_loss: 0.2998 - val_accuracy: 0.9973 - 144ms/epoch - 9ms/step\n",
            "Epoch: 52/100\n",
            "16/16 - 0s - loss: 0.2772 - accuracy: 0.9843 - val_loss: 0.2974 - val_accuracy: 0.9973 - 153ms/epoch - 10ms/step\n",
            "Epoch: 53/100\n",
            "16/16 - 0s - loss: 0.2759 - accuracy: 0.9843 - val_loss: 0.2949 - val_accuracy: 0.9973 - 148ms/epoch - 9ms/step\n",
            "Epoch: 54/100\n",
            "16/16 - 0s - loss: 0.2737 - accuracy: 0.9844 - val_loss: 0.2926 - val_accuracy: 0.9973 - 151ms/epoch - 9ms/step\n",
            "Epoch: 55/100\n",
            "16/16 - 0s - loss: 0.2737 - accuracy: 0.9845 - val_loss: 0.2904 - val_accuracy: 0.9973 - 151ms/epoch - 9ms/step\n",
            "Epoch: 56/100\n",
            "16/16 - 0s - loss: 0.2716 - accuracy: 0.9848 - val_loss: 0.2884 - val_accuracy: 0.9973 - 144ms/epoch - 9ms/step\n",
            "Epoch: 57/100\n",
            "16/16 - 0s - loss: 0.2709 - accuracy: 0.9848 - val_loss: 0.2864 - val_accuracy: 0.9973 - 155ms/epoch - 10ms/step\n",
            "Epoch: 58/100\n",
            "16/16 - 0s - loss: 0.2683 - accuracy: 0.9851 - val_loss: 0.2844 - val_accuracy: 0.9978 - 149ms/epoch - 9ms/step\n",
            "Epoch: 59/100\n",
            "16/16 - 0s - loss: 0.2672 - accuracy: 0.9851 - val_loss: 0.2826 - val_accuracy: 0.9978 - 163ms/epoch - 10ms/step\n",
            "Epoch: 60/100\n",
            "16/16 - 0s - loss: 0.2661 - accuracy: 0.9836 - val_loss: 0.2808 - val_accuracy: 0.9978 - 144ms/epoch - 9ms/step\n",
            "Epoch: 61/100\n",
            "16/16 - 0s - loss: 0.2657 - accuracy: 0.9845 - val_loss: 0.2791 - val_accuracy: 0.9978 - 152ms/epoch - 10ms/step\n",
            "Epoch: 62/100\n",
            "16/16 - 0s - loss: 0.2642 - accuracy: 0.9848 - val_loss: 0.2774 - val_accuracy: 0.9978 - 150ms/epoch - 9ms/step\n",
            "Epoch: 63/100\n",
            "16/16 - 0s - loss: 0.2638 - accuracy: 0.9849 - val_loss: 0.2758 - val_accuracy: 0.9978 - 160ms/epoch - 10ms/step\n",
            "Epoch: 64/100\n",
            "16/16 - 0s - loss: 0.2626 - accuracy: 0.9846 - val_loss: 0.2742 - val_accuracy: 0.9978 - 155ms/epoch - 10ms/step\n",
            "Epoch: 65/100\n",
            "16/16 - 0s - loss: 0.2617 - accuracy: 0.9843 - val_loss: 0.2727 - val_accuracy: 0.9978 - 145ms/epoch - 9ms/step\n",
            "Epoch: 66/100\n",
            "16/16 - 0s - loss: 0.2608 - accuracy: 0.9846 - val_loss: 0.2713 - val_accuracy: 0.9978 - 157ms/epoch - 10ms/step\n",
            "Epoch: 67/100\n",
            "16/16 - 0s - loss: 0.2590 - accuracy: 0.9845 - val_loss: 0.2698 - val_accuracy: 0.9978 - 155ms/epoch - 10ms/step\n",
            "Epoch: 68/100\n",
            "16/16 - 0s - loss: 0.2592 - accuracy: 0.9844 - val_loss: 0.2683 - val_accuracy: 0.9978 - 143ms/epoch - 9ms/step\n",
            "Epoch: 69/100\n",
            "16/16 - 0s - loss: 0.2567 - accuracy: 0.9835 - val_loss: 0.2670 - val_accuracy: 0.9978 - 152ms/epoch - 9ms/step\n",
            "Epoch: 70/100\n",
            "16/16 - 0s - loss: 0.2561 - accuracy: 0.9839 - val_loss: 0.2657 - val_accuracy: 0.9978 - 148ms/epoch - 9ms/step\n",
            "Epoch: 71/100\n",
            "16/16 - 0s - loss: 0.2538 - accuracy: 0.9838 - val_loss: 0.2643 - val_accuracy: 0.9978 - 146ms/epoch - 9ms/step\n",
            "Epoch: 72/100\n",
            "16/16 - 0s - loss: 0.2533 - accuracy: 0.9842 - val_loss: 0.2629 - val_accuracy: 0.9978 - 154ms/epoch - 10ms/step\n",
            "Epoch: 73/100\n",
            "16/16 - 0s - loss: 0.2527 - accuracy: 0.9843 - val_loss: 0.2616 - val_accuracy: 0.9978 - 148ms/epoch - 9ms/step\n",
            "Epoch: 74/100\n",
            "16/16 - 0s - loss: 0.2511 - accuracy: 0.9846 - val_loss: 0.2603 - val_accuracy: 0.9978 - 141ms/epoch - 9ms/step\n",
            "Epoch: 75/100\n",
            "16/16 - 0s - loss: 0.2520 - accuracy: 0.9836 - val_loss: 0.2589 - val_accuracy: 0.9978 - 145ms/epoch - 9ms/step\n",
            "Epoch: 76/100\n",
            "16/16 - 0s - loss: 0.2485 - accuracy: 0.9842 - val_loss: 0.2576 - val_accuracy: 0.9978 - 145ms/epoch - 9ms/step\n",
            "Epoch: 77/100\n",
            "16/16 - 0s - loss: 0.2496 - accuracy: 0.9840 - val_loss: 0.2565 - val_accuracy: 0.9978 - 150ms/epoch - 9ms/step\n",
            "Epoch: 78/100\n",
            "16/16 - 0s - loss: 0.2476 - accuracy: 0.9838 - val_loss: 0.2552 - val_accuracy: 0.9978 - 153ms/epoch - 10ms/step\n",
            "Epoch: 79/100\n",
            "16/16 - 0s - loss: 0.2468 - accuracy: 0.9836 - val_loss: 0.2541 - val_accuracy: 0.9978 - 141ms/epoch - 9ms/step\n",
            "Epoch: 80/100\n",
            "16/16 - 0s - loss: 0.2456 - accuracy: 0.9842 - val_loss: 0.2529 - val_accuracy: 0.9978 - 164ms/epoch - 10ms/step\n",
            "Epoch: 81/100\n",
            "16/16 - 0s - loss: 0.2447 - accuracy: 0.9844 - val_loss: 0.2517 - val_accuracy: 0.9978 - 164ms/epoch - 10ms/step\n",
            "Epoch: 82/100\n",
            "16/16 - 0s - loss: 0.2437 - accuracy: 0.9841 - val_loss: 0.2505 - val_accuracy: 0.9978 - 148ms/epoch - 9ms/step\n",
            "Epoch: 83/100\n",
            "16/16 - 0s - loss: 0.2423 - accuracy: 0.9839 - val_loss: 0.2492 - val_accuracy: 0.9978 - 151ms/epoch - 9ms/step\n",
            "Epoch: 84/100\n",
            "16/16 - 0s - loss: 0.2426 - accuracy: 0.9846 - val_loss: 0.2479 - val_accuracy: 0.9978 - 143ms/epoch - 9ms/step\n",
            "Epoch: 85/100\n",
            "16/16 - 0s - loss: 0.2418 - accuracy: 0.9836 - val_loss: 0.2468 - val_accuracy: 0.9978 - 141ms/epoch - 9ms/step\n",
            "Epoch: 86/100\n",
            "16/16 - 0s - loss: 0.2407 - accuracy: 0.9837 - val_loss: 0.2456 - val_accuracy: 0.9978 - 162ms/epoch - 10ms/step\n",
            "Epoch: 87/100\n",
            "16/16 - 0s - loss: 0.2401 - accuracy: 0.9839 - val_loss: 0.2445 - val_accuracy: 0.9978 - 172ms/epoch - 11ms/step\n",
            "Epoch: 88/100\n",
            "16/16 - 0s - loss: 0.2398 - accuracy: 0.9837 - val_loss: 0.2434 - val_accuracy: 0.9978 - 159ms/epoch - 10ms/step\n",
            "Epoch: 89/100\n",
            "16/16 - 0s - loss: 0.2371 - accuracy: 0.9840 - val_loss: 0.2424 - val_accuracy: 0.9978 - 168ms/epoch - 11ms/step\n",
            "Epoch: 90/100\n",
            "16/16 - 0s - loss: 0.2379 - accuracy: 0.9839 - val_loss: 0.2412 - val_accuracy: 0.9978 - 165ms/epoch - 10ms/step\n",
            "Epoch: 91/100\n",
            "16/16 - 0s - loss: 0.2367 - accuracy: 0.9839 - val_loss: 0.2401 - val_accuracy: 0.9978 - 156ms/epoch - 10ms/step\n",
            "Epoch: 92/100\n",
            "16/16 - 0s - loss: 0.2351 - accuracy: 0.9839 - val_loss: 0.2390 - val_accuracy: 0.9978 - 163ms/epoch - 10ms/step\n",
            "Epoch: 93/100\n",
            "16/16 - 0s - loss: 0.2332 - accuracy: 0.9829 - val_loss: 0.2378 - val_accuracy: 0.9978 - 151ms/epoch - 9ms/step\n",
            "Epoch: 94/100\n",
            "16/16 - 0s - loss: 0.2334 - accuracy: 0.9835 - val_loss: 0.2367 - val_accuracy: 0.9978 - 158ms/epoch - 10ms/step\n",
            "Epoch: 95/100\n",
            "16/16 - 0s - loss: 0.2329 - accuracy: 0.9837 - val_loss: 0.2356 - val_accuracy: 0.9978 - 153ms/epoch - 10ms/step\n",
            "Epoch: 96/100\n",
            "16/16 - 0s - loss: 0.2324 - accuracy: 0.9840 - val_loss: 0.2345 - val_accuracy: 0.9978 - 154ms/epoch - 10ms/step\n",
            "Epoch: 97/100\n",
            "16/16 - 0s - loss: 0.2317 - accuracy: 0.9835 - val_loss: 0.2334 - val_accuracy: 0.9978 - 154ms/epoch - 10ms/step\n",
            "Epoch: 98/100\n",
            "16/16 - 0s - loss: 0.2314 - accuracy: 0.9841 - val_loss: 0.2323 - val_accuracy: 0.9978 - 152ms/epoch - 10ms/step\n",
            "Epoch: 99/100\n",
            "16/16 - 0s - loss: 0.2300 - accuracy: 0.9840 - val_loss: 0.2313 - val_accuracy: 0.9978 - 148ms/epoch - 9ms/step\n",
            "Epoch: 100/100\n",
            "16/16 - 0s - loss: 0.2280 - accuracy: 0.9834 - val_loss: 0.2304 - val_accuracy: 0.9978 - 152ms/epoch - 9ms/step\n"
          ]
        }
      ],
      "source": [
        "#Train on new datasets\n",
        "!python lucid_cnn.py --train ./CIC2019_PCAP/  --epochs 100 --regularization l1 --dropout 0.5"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict on CICDDoS2019 dataset\n",
        "!python lucid_cnn.py --predict ./CIC2019_PCAP/ --model ./CIC2019_PCAP/10t-10n-DOS2019-LUCID.h5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GxQpyY17J5pE",
        "outputId": "9fde2d0e-845e-4c5c-94c1-b40b8b6a62bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-06-14 08:57:33.540124: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Model         TIME(sec) PACKETS SAMPLES DDOS% ACC    ERR    F1     PPV    TPR    FPR    TNR    FNR    Data Source\n",
            "DOS2019-LUCID     0.057 0015791 0004038 0.506 0.9968 0.1112 0.9968 0.9985 0.9951 0.0015 0.9985 0.0049 10t-10n-DOS2019-dataset-test.hdf5\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# manually input the confusion matrix value & plot\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "array = [[0.9951, 0.0049], \n",
        "        [0.0015,0.9985]]\n",
        "\n",
        "plt.figure(figsize = (4,4))\n",
        "lucid_map = sns.heatmap(array, annot=True, cmap='Blues', fmt='g')\n",
        "\n",
        "# Config map display\n",
        "lucid_map.set_xlabel('\\nPredicted Values')\n",
        "lucid_map.set_ylabel('Actual Values ');\n",
        "\n",
        "## Ticket labels - List must be in alphabetical order\n",
        "lucid_map.xaxis.set_ticklabels(['Positive','Negative'])\n",
        "lucid_map.yaxis.set_ticklabels(['Positive','Negative'])\n",
        "\n",
        "## Display the visualization of the Confusion Matrix.\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyPuaEiXlAmN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        },
        "outputId": "5b26f415-ca87-4c26-8d1a-71f2002805f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 288x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /proc/cpuinfo\n",
        "!cat /proc/meminfo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNwOQjO-llKS",
        "outputId": "fc95138c-a6da-4528-81c2-34e7abd758f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "processor\t: 0\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 0\n",
            "initial apicid\t: 0\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "processor\t: 1\n",
            "vendor_id\t: GenuineIntel\n",
            "cpu family\t: 6\n",
            "model\t\t: 79\n",
            "model name\t: Intel(R) Xeon(R) CPU @ 2.20GHz\n",
            "stepping\t: 0\n",
            "microcode\t: 0x1\n",
            "cpu MHz\t\t: 2199.998\n",
            "cache size\t: 56320 KB\n",
            "physical id\t: 0\n",
            "siblings\t: 2\n",
            "core id\t\t: 0\n",
            "cpu cores\t: 1\n",
            "apicid\t\t: 1\n",
            "initial apicid\t: 1\n",
            "fpu\t\t: yes\n",
            "fpu_exception\t: yes\n",
            "cpuid level\t: 13\n",
            "wp\t\t: yes\n",
            "flags\t\t: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap xsaveopt arat md_clear arch_capabilities\n",
            "bugs\t\t: cpu_meltdown spectre_v1 spectre_v2 spec_store_bypass l1tf mds swapgs taa\n",
            "bogomips\t: 4399.99\n",
            "clflush size\t: 64\n",
            "cache_alignment\t: 64\n",
            "address sizes\t: 46 bits physical, 48 bits virtual\n",
            "power management:\n",
            "\n",
            "MemTotal:       13298580 kB\n",
            "MemFree:        10498360 kB\n",
            "MemAvailable:   11973788 kB\n",
            "Buffers:          141584 kB\n",
            "Cached:          1454116 kB\n",
            "SwapCached:            0 kB\n",
            "Active:          2116300 kB\n",
            "Inactive:         460496 kB\n",
            "Active(anon):     914916 kB\n",
            "Inactive(anon):      448 kB\n",
            "Active(file):    1201384 kB\n",
            "Inactive(file):   460048 kB\n",
            "Unevictable:           0 kB\n",
            "Mlocked:               0 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:              1004 kB\n",
            "Writeback:            32 kB\n",
            "AnonPages:        981396 kB\n",
            "Mapped:           256216 kB\n",
            "Shmem:              1208 kB\n",
            "KReclaimable:     110896 kB\n",
            "Slab:             156448 kB\n",
            "SReclaimable:     110896 kB\n",
            "SUnreclaim:        45552 kB\n",
            "KernelStack:        6064 kB\n",
            "PageTables:        12644 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6649288 kB\n",
            "Committed_AS:    4209692 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:        8568 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1504 kB\n",
            "AnonHugePages:         0 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      146240 kB\n",
            "DirectMap2M:     7190528 kB\n",
            "DirectMap1G:     8388608 kB\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "lucid.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}